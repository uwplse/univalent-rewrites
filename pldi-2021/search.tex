\section{Case Studies: \textsc{Pumpkin P}i Four Ways}
\label{sec:search}

This section explains four case studies for \toolname, building from the simple to complex:

\begin{enumerate}
\item We use \toolname to support multiple variants of a repair benchmark from a user study of Coq proof engineers (Section~\ref{sec:replica}).
\item We use \toolname to update functions and proofs about lists to functions and proofs about dependently typed vectors of
a particular length (Section~\ref{sec:dep}).
\item We use \toolname to port functions and proofs about unary numbers to functions and proofs about binary numbers (Section~\ref{sec:bin}).
\item We report on the experiences of an industrial proof engineer who is using \toolname to integrate Coq with a company proof engineering workflow and write proofs about an implementation of the TLS handshake protocol (Section~\ref{sec:industry}).
\end{enumerate}
For each case study, we explain the configuration used, walk through an example, and describe lessons learned.
In all, we found the following:

\begin{enumerate}
\item \toolname was flexible. Each search procedure for automatic configuration
was simple to write, and handled an entire class of equivalences corresponding to real use cases.
Manual configuration was possible but challenging for an interesting use case.
\item \toolname had good enough workflow integration to support real use cases.
The tactic decompiler showed promising early results with clear paths to improvements.
Some workflows were unanticipated and informed design changes in \toolname.
\end{enumerate}

\subsection{\textsc{Replica} Benchmark Variants}
\label{sec:replica}

\begin{figure*}
\begin{minipage}{0.48\textwidth}
   \lstinputlisting[firstline=1, lastline=8]{replica.tex}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
   \lstinputlisting[firstline=10, lastline=17]{replica.tex}
\end{minipage}
\vspace{-0.4cm}
\caption{A simple language (left) and the same language with two swapped constructors (right).}
\label{fig:replica}
\end{figure*}

The constructor swapping example from Section~\ref{sec:overview} was inspired by two benchmarks 
from the \textsc{Replica} user study of proof engineers~\cite{replica}.
An isolated change corresponding to part of one of the benchmarks is shown in Figure~\ref{fig:replica}.
The proof engineer had a simple language represented by an inductive type \lstinline{Term},
as well as some definitions and proofs about the language.
The proof engineer swapped two constructors in the term language.

We used \toolname to automatically configure the proof term transformation to swap these constructors,
then repair all of the functions and proofs.
We also succeeded at more difficult variants of this,
like swapping constructors with the same type, renaming all of the constructors,
permuting more than two constructors,
or permuting and renaming constructors at the same time.
In all cases, with a bit of human guidance, \toolname repaired the functions and proofs.

\subsubsection{Configuration}

The automatic configuration this used handles permuting and renaming constructors of inductive types.
This is one of the simplest configurations.
The only nontrivial part is that \lstinline{DepConstr} over the updated type permutes back the constructors, in our example:

\begin{lstlisting}[backgroundcolor=\color{cyan!30}]
dep_constr_0 (H : Z) : Term := (@\codediff{Int}@) H.(@\vspace{-0.04cm}@)
dep_constr_1 (H H0 : Term) : Term := (@\codediff{Eq}@) H H0.
\end{lstlisting}
so that they align with the original constructors.
The eliminator similarly permutes cases:

\begin{lstlisting}[backgroundcolor=\color{cyan!30}]
dep_elim P f0 f1 f2 f3 f4 f5 f6 t : P t := Term_rect P f0 (@\codediff{f2}@) (@\codediff{f1}@) f3 f4 f5 f6 t.
\end{lstlisting}
where \lstinline{Term_rect} is the eliminator over the old version of \lstinline{Term}.

\subsubsection{Example}

We used \toolname to automatically update the functions and proofs in \href{https://github.com/uwplse/pumpkin-pi/blob/master/plugin/coq/Swap.v}{\lstinline{Swap.v}} from the \textsc{Replica} benchmark.
This included functions about \lstinline{Term}, as well as a large record \lstinline{EpsilonLogic} that encoded the semantics of the language,
plus a proof about that record:

\begin{lstlisting}
Theorem eval_eq_true_or_false : $\forall$ (L : EpsilonLogic) env (t1 t2 : Term),(@\vspace{-0.04cm}@)
  L.eval env (Eq t1 t2) = L.vTrue $\vee$ L.eval env (Eq t1 t2) = L.vFalse.
\end{lstlisting}
\toolname automatically updated all of these. For the inductive proof, it produced
a new inductive proof that used the induction principle with the swapped constructors.
It also discovered all other 23 type-correct permutations of constructors, all available for selection to 
prove the appropriate equivalence and use to update functions and proofs.
It presented the desired transformation as the first option in the list, so that all we had to do
was pass the argument \lstinline{mapping 0} to \lstinline{Repair module} for it to handle this change.
It was also able to handle the more advanced variants of this change.

\subsubsection{Lessons}

Implementing the search procedure was simple. %and took about three days.
The biggest challenge was implementing an interactive interface to choose between mappings when there are multiple possible mappings,
or to allow the proof engineer to write a single custom mapping function and derive the rest of the configuration from that.
This was only a single snapshot of the benchmark, which included
other changes like adding constructors to inductive types.
One ongoing challenge is defining and discovering useful configurations to support adding new constructors.

The isolated change was simple enough that \toolname would not have been necessary
for updating the proofs for the initial change---keeping the tactics the same would have also worked.
%This is a property of the particular proofs that we had access to;
As we saw in Sections~\ref{sec:overview} and~\ref{sec:key1}, even for simple changes, this is not always true.
More advanced variants of this benchmark that involved also renaming constructors did necessitate \toolname,
and \toolname worked well for those. % TODO what happens with the tactic decompiler for these?
The entire \lstinline{Swap.v} file, which includes swapping constructors of every function in the \lstinline{list} module and
its dependencies, four variants of the \textsc{Replica} benchmark,
and testing a large and ambiguous permutation of a 30 constructor \lstinline{Enum},
took \toolname less than 90 seconds total. % TODO specs, reproduction
Each variant of the \textsc{Replica} benchmark took \toolname less than 5 seconds. % TODO specs, reproduction

\subsection{Vectors from Lists}
\label{sec:dep}

The proof term transformation in \toolname is based on the proof term transformation from
\textsc{Devoid}, an earlier version of \toolname.
\textsc{Devoid} supported proof reuse across \textit{algebraic ornaments}, which describe relations
between two inductive types, where one type is exactly the other type indexed by a fold~\cite{mcbride}.
The standard example of this is the relation between a list and a
length-indexed vector, like we saw in Figure~\ref{fig:listtovect} in Section~\ref{sec:key1}.

With \toolname, we were able to not only support algebraic ornaments as in \textsc{Devoid},
but also automate effort that had previously been manual.
Several proof engineers including the industrial proof engineer, a Reddit user,
and someone on the \lstinline{coq-club} message board contacted us expressing interest in using this functionality.
%, though we do not yet know if the latter two ended up using \toolname. % TODO honestly ask

\subsubsection{Configuration}

We used two automatic configurations to ease development with dependent types using algebraic ornaments.
The first instantiates the proof term transformation to the tranformation from \textsc{Devoid}:
it ports functions and proofs from the input type to the ornamented type at \textit{some} index,
like $\Sigma$\lstinline{(n : nat).vector T n}.
The second provides the missing link to get proofs at a \textit{particular} index, like \lstinline{vector T n};
\textsc{Devoid} had left this to the proof engineer.

For lists and vectors, the search procedure for the first configuration used the equivalence:

\begin{lstlisting}
list T $\simeq$ $\Sigma$(n : nat).vector T n
\end{lstlisting}
The configuration it found for \lstinline{list} was simple: identity for \lstinline{Eta},
the list constructors for \lstinline{DepConstr}, the list eliminator for \lstinline{DepElim},
and definitional \lstinline{Iota}.
For \lstinline{vector}, it set \lstinline{Eta} to $\eta$-expand $\Sigma$ types:

\begin{lstlisting}[backgroundcolor=\color{cyan!30}]
eta (T : Type) (s : $\Sigma$(n : nat).vector T n) : $\Sigma$(n : nat).vector T n := $\exists$ ($\pi_l$ s) ($\pi_r$ s).
\end{lstlisting}
it set \lstinline{DepConstr} to pack constructors: % TODO shrink this now that some of it is in the other section

\begin{lstlisting}[backgroundcolor=\color{cyan!30}]
dep_constr_0 (T : Type) : $\Sigma$(n : nat).vector T n :=(@\vspace{-0.04cm}@)
  $\exists$ 0 (Vector.nil A).(@\vspace{-0.04cm}@)
dep_constr_1 (T : Type) (t : T) (s : $\Sigma$(n : nat).vector T n) : $\Sigma$(n : nat).vector T n :=(@\vspace{-0.04cm}@)
  $\exists$ (S ($\pi_l$ s)) (Vector.cons ($\pi_l$ s) t ($\pi_r$ s)).
\end{lstlisting}
it set \lstinline{DepElim} to eliminate its projections:

\begin{lstlisting}[backgroundcolor=\color{cyan!30}]
dep_elim (T : Type) (P : $\Sigma$(n : nat).vector T n $\rightarrow$ Type) (pnil : P (dep_constr_0 T))(@\vspace{-0.04cm}@)
    (pcons : $\forall$ t s, P (eta T s) $\rightarrow$ P (dep_constr_1 T t s)) (s : $\Sigma$(n : nat).vector T n)(@\vspace{-0.04cm}@)
    : P (eta T s) :=(@\vspace{-0.04cm}@)
  vector_rect T(@\vspace{-0.04cm}@)
    (fun (n : nat) (v : vector T n) => P ($\exists$ n v))(@\vspace{-0.04cm}@)
    pnil(@\vspace{-0.04cm}@)
    (fun (t : T) (n : nat) (v : vector T n) => pcons t ($\exists$ n v))(@\vspace{-0.04cm}@)
    ($\pi_l$ s) ($\pi_r$ s).
\end{lstlisting}
and it used definitional \lstinline{Iota}.
%This configuration was enough to capture all of the search and lifting functionality from \textsc{Devoid}.

To get from lists to vectors \textit{at a particular length}, we used one additional automatic configuration.
This configuration corresponds to the equivalence between sigma types at a particular projection
and the same type escaping the sigma type, in our example:

\begin{lstlisting}
$\Sigma$(s : $\Sigma$(m : nat).vector T m).$\pi_l$ s = n $\simeq$ vector T n
\end{lstlisting}
By composition with the initial equivalence, this transports proofs
across the equivalence we want:

\begin{lstlisting}
$\Sigma$(l : list T).length l = n $\simeq$ vector T n
\end{lstlisting}
since these equivalences are equal up to transport along the first equivalence.

The second configuration carries equality proofs over the indices.
For example, it views \lstinline{vector T n} as implicitly representing $\Sigma$\lstinline{(v : vector T m).n = m} for some \lstinline{m}.
This is seen in \lstinline{eta}, here: 
% TODO both should take the same number of arguments, even if eta_A takes fewer. also does this belong in iota?

\begin{lstlisting}[backgroundcolor=\color{cyan!30}]
eta (T : Type) (n m : nat) (H : n = m) (v : vector T m) : vector T n :=(@\vspace{-0.04cm}@)
  eq_rect m (vector T) v n H.
\end{lstlisting}
which is the identity function generalized over any equal index.
Since there are no changes in inductive types, \lstinline{DepElim} and \lstinline{DepConstr} are trivial,
and \lstinline{Iota} does not change.

%\begin{lstlisting}
%(@\codeauto{dep_elim}@) (T : Type) (P : $\forall$ (n : nat), vector T n $\rightarrow$ Type) (p : $\forall$ n v, P n (eta T n n v)) (v : vector T n) : %P (eta T n n v) :=
%  p.
%\end{lstlisting}

\subsubsection{Example}

The expanded example from the \textsc{Devoid} paper is in \href{https://github.com/uwplse/pumpkin-pi/blob/master/plugin/coq/examples/Example.v}{\lstinline{Example.v}}.
The \textsc{Devoid} example ported a list \lstinline{zip} function,
a \lstinline{zip_with} function, and a proof \lstinline{zip_with_is_zip} relating the two
functions from lists to vectors of some length.
It then manually ported those proofs to proofs over vectors at a particular length.
The updated \toolname example automates this last step.

%The workflow for this was a bit different than it was with \textsc{Devoid}.
To handle this, we used a custom eliminator \toolname generated to combine the list functions
with length invariants, and to combine the list proofs with the proofs about those length invariants.
This gave us a proof of this lemma (with \lstinline{zip_with} and \lstinline{zip} operating over lists at given lengths):

\begin{lstlisting}
Lemma zip_with_is_zip : $\forall$ A B n(@\vspace{-0.04cm}@)
  (v1: $\Sigma$(l1: (@\codediff{list A}@)).(@\codediff{length}@) l1 = n) (v2: $\Sigma$(l2: (@\codediff{list B}@)).(@\codediff{length}@) l2 = n),(@\vspace{-0.04cm}@)
    zip_with pair n v1 v2 = zip n v1 v2.
\end{lstlisting}
We then ran \lstinline{Repair module} using the first
configuration, which proved this lemma:

\begin{lstlisting}[backgroundcolor=\color{cyan!30}]
Lemma zip_with_is_zip : $\forall$ A B n(@\vspace{-0.04cm}@)
  (v1:$\Sigma$(l1:(@\codediff{$\Sigma$(m: nat).vector A m}@)).(@\codediff{$\pi_l$}@) l1 = n) (v2:$\Sigma$(l2:(@\codediff{$\Sigma$(m: nat).vector B m}@)).(@\codediff{$\pi_l$}@) l2 = n),(@\vspace{-0.04cm}@)
    zip_with pair v1 v2 = zip v1 v2.
\end{lstlisting}
with functions \lstinline{zip_with} and \lstinline{zip} updated as well.
We composed this with \lstinline{Repair module} on the second configuration,
which proved the final lemma:

\begin{lstlisting}[backgroundcolor=\color{cyan!30}]
Lemma zip_with_is_zip : $\forall$ A B n(@\vspace{-0.04cm}@)
  (v1: (@\codediff{vector A n}@)) (v2: (@\codediff{vector B n}@)),(@\vspace{-0.04cm}@)
    zip_with pair v1 v2 = zip v1 v2.
\end{lstlisting}
with functions \lstinline{zip_with} and \lstinline{zip} operating directly over vectors.

\subsubsection{Lessons}

%\paragraph{Configuration \& Flexibility}
Implementing the search procedure for the first configuration was straightforward.
%It also simplified the transformation from \textsc{Devoid} %by removing the need for special rules for handling eliminators and constructors
%by adding and removing arguments, and for special handling of non-primitive projections,
%as that work is done in the configuration.
%by moving all of the interesting work into the search procedure for the configuration.
Implementing the search procedure for the second configuration was less straightforward.
It is still an open challenge to define complete unification heuristics to port any arbitrary proof
along the second configuration in either direction,
though this does not impact the case study.

%\paragraph{Workflow Integration}
The result was a simplified workflow from \textsc{Devoid} that automated steps 
that had previously been manual.
%\toolname implemented some additional automation to generate eliminators that help separate out this additional information
%for repair. 
The tactic decompiler suggested tactics that helped us write tactic proofs ourselves that had been
prohibitively difficult for us to write by hand.
However, the suggested tactics did need massaging, as the decompiler struggled with motive inference
for induction with dependent types.
Additional effort is needed to further improve tactic integration with dependent types.
% TODO how long did compiling the file take?

\subsection{Unary and Binary Numbers}
\label{sec:bin}

All of the case studies so far have dealt with pairs of types with the same inductive structure.
Some of the oldest problems in the transport literature deal with \textit{changing} inductive
structure.
We have realized a classic example~\cite{magaud2000changing}  of this with \toolname using a manual configuration:
updating unary to binary natural numbers.

In Coq, a binary number \lstinline{N} (see Section~\ref{sec:key2}, Figure~\ref{fig:nattobin}) is either zero or a positive binary number. A positive binary number
is either 1 (\lstinline{xH}), or the result of shifting left and adding 1 (\lstinline{xI})
or nothing (\lstinline{xO}).
This allows for a fast addition function, found in the Coq standard library.
In the style of \citet{magaud2000changing}, we use \toolname to derive a slow binary
addition function that does not refer to \lstinline{nat}.
From that, we port proofs over unary addition to binary addition,
removing all references to \lstinline{nat}, and show that they hold over fast binary addition too.

\subsubsection{Configuration}
We configured this manually using the \lstinline{Configure} command,
which takes the configuration as an argument.
The result is in \href{https://github.com/uwplse/pumpkin-pi/blob/master/plugin/coq/nonorn.v}{\lstinline{nonorn.v}}.
The configuration for \lstinline{nat} was straightforward.
For \lstinline{N}, we used functions from the Coq standard library that
behaved like the \lstinline{nat} constructors:

\begin{lstlisting}
dep_constr_0 : N := 0%N.(@\vspace{-0.04cm}@)
dep_constr_1 : N $\rightarrow$ N := N.succ.
\end{lstlisting}
and an eliminator from the Coq standard library that behaves like the \lstinline{nat} eliminator:

\begin{lstlisting}
dep_elim P (pO : P dep_constr_0) (pS : $\forall$n, P n $\rightarrow$ P (dep_constr_1 n)) (n : N) : P n :=(@\vspace{-0.04cm}@)
  N.peano_rect P pO pS n.
\end{lstlisting}
\lstinline{Iota} was almost written for us.
The standard library had a lemma that reduced the successor case:

\begin{lstlisting}
N.peano_rect_succ :(@\vspace{-0.04cm}@)
  $\forall$ P pO pS n, N.peano_rect P pO pS (N.succ n) = pS n (N.peano_rect P pO pS n).
\end{lstlisting}
\lstinline{Iota} over the successor case was a simple rewrite by this lemma:

\begin{lstlisting}
Lemma iota_1 :(@\vspace{-0.04cm}@)
  $\forall$ P pO pS n (Q : P (dep_constr_1 n) $\rightarrow$ Type),(@\vspace{-0.04cm}@)
     Q (pS n (dep_elim P pO pS n)) $\rightarrow$(@\vspace{-0.04cm}@)
     Q (dep_elim P pO pS (dep_constr_1 n)).(@\vspace{-0.04cm}@)
Proof.(@\vspace{-0.04cm}@)
  intros. unfold dep_elim, dep_constr_1. rewrite N.peano_rect_succ. auto.(@\vspace{-0.04cm}@)
Defined.
\end{lstlisting}

The need for a nontrivial \lstinline{Iota} comes from the fact that \lstinline{N} has a different
inductive structure from \lstinline{nat}, and is noted as far back as \citet{magaud2000changing}.
This corresponds to a broader pattern---it captures the essence of the change in inductive structure.
\toolname's configurable proof term transformation captures that intuition.

\subsubsection{Example}

We ported unary addition from \lstinline{nat} to \lstinline{N} fully automatically:

\begin{lstlisting}
Repair nat N in add as slow_add.
\end{lstlisting}
The result (tellingly named) has the same slow behavior as the \lstinline{add} function over \lstinline{nat}.
However, it no longer refers to \lstinline{nat} in any way.
Like \citet{magaud2000changing}, we found it easy to manually prove that
this has the same behavior as fast binary addition:

\begin{lstlisting}
Lemma add_fast_add: $\forall$ (n m : Bin.nat), slow_add n m = N.add n m.(@\vspace{-0.04cm}@)
Proof.(@\vspace{-0.04cm}@)
  induction n using N.peano_rect; intros m; auto. unfold slow_add.(@\vspace{-0.04cm}@)
  rewrite N.peano_rect_succ. (* $\leftarrow$ iota_1 *)(@\vspace{-0.04cm}@)
  unfold slow_add in IHn. rewrite IHn. rewrite N.add_succ_l. reflexivity.(@\vspace{-0.04cm}@)
Qed.
\end{lstlisting}

We then used \toolname again to transform a proof:
\begin{lstlisting}
add_n_Sm : $\forall$ (n m : nat), S (add n m) = add n (S m).
\end{lstlisting}
from \lstinline{add} to \lstinline{slow_add}:

\begin{lstlisting}[backgroundcolor=\color{cyan!30}]
slow_add_n_Sm : $\forall$ (n m : N), N.succ (slow_add n m) = slow_add n (N.succ m).
\end{lstlisting}
This was not quite as push-button.
It involved a manual expansion step, turning implicit casts in the inductive case
into explicit applications of \lstinline{Iota} over \A.
These applications were formulaic, but tricky to write.
Once we had that, though, we could run the same \lstinline{Repair} command
to get \lstinline{slow_add_n_Sm}.
Showing that the same theorem held over fast binary addition was then
straightforward:

\begin{lstlisting}
Lemma add_n_Sm : $\forall$ n m, Bin.succ (N.add n m) = N.add n (Bin.succ m).
Proof.
  intros. repeat rewrite $\leftarrow$ add_fast_add. apply slow_plus_n_Sm.
Qed.
\end{lstlisting}

\subsubsection{Lessons}

\lstinline{Iota} was the key to supporting this case.
It was enough to implement this transformation that had previously been its own tool
just by writing a configuration with \lstinline{Iota}. 

The file took under a second for us to compile using \toolname.
We did not need tactic suggestions for this workflow,
but we did note that supporting custom eliminators like \lstinline{N.peano_rect} would be a simple way
to improve the decompiler.
The most difficult part was manually expanding proofs about \lstinline{nat}
to apply \lstinline{Iota},
as there is not yet a way to supply custom unification heuristics for manual configuration.
We discuss ideas for this in Sections~\ref{sec:related} and~\ref{sec:discussion}.

\subsection{Industrial Use}
\label{sec:industry}

An industrial proof engineer at Galois has been using \toolname in proving
correct an implementation of the TLS handshake protocol.
While this is ongoing work, thus far,
\toolname has helped \company integrate Coq with their existing verification workflow.

Before contact, \company had been using a custom solver-aided verification language to prove correct C programs.
They had found that at times, those constraint solvers got stuck, and they could not
progress on proofs about those programs.
They had built a compiler that translates their solver-aided language into Coq's specification language Gallina,
that way their proof engineers could finish stuck proofs interactively using Coq.
However, they had found that the generated Gallina programs and specifications were sometimes too difficult to work with.

A proof engineer at \company has used \toolname to work with those automatically generated programs and specifications
with the following workflow:

\begin{enumerate}
\item use \toolname to update the automatically generated functions and specifications into more
human-readable functions and specifications, then
\item write Coq proofs about the more human-readable functions and specifications, then finally
\item use \toolname again to update those proofs about human-readable functions and specifications back to
proofs about the original automatically generated functions and specifications.
\end{enumerate}
This workflow has allowed for industrial integration with Coq and has helped the proof engineer write functions and proofs
that would have otherwise been difficult.

% TODO will add better proofs here once Val sends them

\subsubsection{Configuration}

\begin{figure*}
\begin{minipage}{0.33\textwidth}
   \lstinputlisting[firstline=1, lastline=10]{records.tex}
\end{minipage}
\hfill
\begin{minipage}{0.66\textwidth}
   \lstinputlisting[firstline=12, lastline=21]{records.tex}
\end{minipage}
\vspace{-0.5cm}
\caption{Two unnamed tuples (left) and corresponding named records (right).}
\label{fig:records}
\end{figure*}

Some example proofs that the proof engineer used \toolname for
can be found in a branch of the proof engineer's repository.\footnote{\url{https://github.com/Ptival/saw-core-coq/tree/dump-wip}}
The proof engineer used \toolname to port anonymous tuples produced by \company' compiler
to named records, as in the example in Figure~\ref{fig:records}.

We implemented a search procedure for the proof engineer to automatically configure the proof term transformation to an equivalence
between nested tuples and named records.
The search procedure triggered automatically when the proof engineer called the \lstinline{Repair} command.
%It set \A to be the record type and \B to be the tuple.
The configuration it found for the record type was straightforward: identity for \lstinline{Eta},
the record constructor for \lstinline{DepConstr}, the record eliminator for \lstinline{DepElim}, and definitional \lstinline{Iota}.
For the tuple, it set \lstinline{Eta} to expand and project, for example for \lstinline{Handshake}:
\begin{lstlisting}[backgroundcolor=\color{cyan!30}]
eta (H : Handshake) : Handshake := (fst H, snd H).
\end{lstlisting}
it set \lstinline{DepConstr} to apply the pair constructor:

\begin{lstlisting}[backgroundcolor=\color{cyan!30}]
dep_constr_0 (handshakeType : seq 32 bool) (messageNumber : seq 32 bool) :=(@\vspace{-0.04cm}@)
  (handshakeType, messageNumber).
\end{lstlisting}
and it set \lstinline{DepElim} to eliminate over the pair:

\begin{lstlisting}[backgroundcolor=\color{cyan!30}]
dep_elim P (f: $\forall$ h m, P (dep_constr_0 h m)) (H : Handshake) : P (eta H) :=(@\vspace{-0.04cm}@)
  prod_rect(@\vspace{-0.04cm}@)
    (fun (p : seq 32 bool * seq 32 bool) => P (eta p))(@\vspace{-0.04cm}@)
    (fun (h : seq 32 bool) (m : seq 32 bool) => f h m)(@\vspace{-0.04cm}@)
    H.
\end{lstlisting}
For \lstinline{Connection}, it found similar \lstinline{Eta}, \lstinline{DepConstr}, and \lstinline{DepElim},
except that \lstinline{Eta} included all nine projections, \lstinline{DepConstr} \textit{recursively} applied the
pair constructor, and \lstinline{DepElim} \textit{recursively} eliminated the pair.
This induced an equivalence between the nested tuple and record,
which \toolname generated and proved automatically.

\subsubsection{Example}
Using this configuration, the proof engineer automatically ported this compiler-generated function:

\begin{lstlisting}
cork (c : Connection) : Connection :=(@\vspace{-0.04cm}@)
  (fst c, (bvAdd _ (fst (snd c)) (bvNat _ 1), snd (snd c))).
\end{lstlisting}
to the corresponding function over records:

\begin{lstlisting}[backgroundcolor=\color{cyan!30}]
cork (c : (@\texttt{Record}@).Connection) : (@\texttt{Record}@).Connection := {|(@\vspace{-0.04cm}@)
  clientAuthFlag := clientAuthFlag c; (@\hspace{0.43cm}@) corked := bvAdd _ (corked c) (bvNat _ 1);(@\vspace{-0.04cm}@)
  corkedIO := corkedIO c; (@\hspace{2.36cm}@) handshake := handshake c;(@\vspace{-0.04cm}@)
  isCachingEnabled := isCachingEnabled c; keyExchangeEPH := keyExchangeEPH c;(@\vspace{-0.04cm}@)
  mode := mode c; (@\hspace{3.64cm}@) resumeFromCache := resumeFromCache c;(@\vspace{-0.04cm}@)
  serverCanSendOCSP := serverCanSendOCSP c(@\vspace{-0.04cm}@)
|}.
\end{lstlisting}
The proof engineer then wrote proofs that record, for example:

\begin{lstlisting}
Lemma corkLemma :(@\vspace{-0.04cm}@)
  $\forall$ (c : (@\texttt{Record}@).Connection), corked c = bvNat 2 0 $\rightarrow$ corked (cork c) = bvNat 2 1.(@\vspace{-0.04cm}@)
Proof.(@\vspace{-0.04cm}@)
  intros []. simpl. intros H. subst. reflexivity.(@\vspace{-0.04cm}@)
Defined.
\end{lstlisting} % TODO better proof
and then used \toolname to automatically port these back to proofs about the original function:

\begin{lstlisting}[backgroundcolor=\color{cyan!30}]
Lemma corkLemma :(@\vspace{-0.04cm}@)
  $\forall$ (c : Connection), fst (snd c) = bvNat 2 0 $\rightarrow$ fst (snd (cork c)) = bvNat 2 1.
\end{lstlisting} % TODO decompiler

\subsubsection{Lessons}

%but also to help write dependently typed functions.
So far, the proof engineer has used at least three automatic configurations.
Two had existed already, while the one for tuples and records was
added in response to the proof engineer's needs.
This search procedure was easy for us to implement. %using techniques from
%the repair and reuse tools \textsc{Pumpkin Patch} and \textsc{Devoid}.
Flexibility could be further improved by exposing an interface to allow proof engineers to
write search procedures themselves.

The proof engineer was able to use \toolname to integrate Coq into an existing proof engineering
workflow using solver-aided tools at \company.
The workflow for using \toolname itself was a bit nonstandard,
and there was little need for tactic proofs about the compiler-generated functions and specifications.
In the initial days, we worked closely with the proof engineer;
later, the proof engineer worked independently and reached out occasionally by email or phone.
\toolname was usable enough for this to work, but we found two challenges with workflow integration:
the proof engineer sometimes could not distinguish between user errors and bugs in our code,
and the proof engineer typically waited only about ten seconds at most for \toolname to return.
Both observations informed improvements to \toolname, like better error messages, caching of transformed subterms,
and the ability to tell \toolname not to $\delta$-reduce certain terms.
% TODO how long did compiling the file take?

% TODO what does the tactic decompiler do for this? It's broken. Why?

%\subsubsection{Algebraic}

%It is straightforward to fit the search algorithm from DEVOID into this framework, and in fact
%we can loosen the restriction that the language has primitive projections.
%Let $A$ be $A$ from DEVOID, let $B_{ind}$ be $B$ from DEVOID, let $I_B$ be $I_B$ from DEVOID,
%and let \lstinline{index} be \lstinline{index} from DEVOID.
%Let $B$ wrap $B_{ind}$ packed into a sigma type:

%\begin{lstlisting}
%B := $\lambda$ ($\vec{t}$ : $\vec{T}$) . ($\Sigma$ (i : I$_B$ $\vec{t}$) . B$_{ind}$ (index i $\vec{t}$))
%\end{lstlisting}
%Let $\vec{T_{B_j}}$ be the arguments of constructor type $C_{B_j}$ (type of constructor of $B_{\mathrm{ind}}$).
%Define \lstinline{DepConstr(j, B)} recursively using the following derivation (based on and same fall-through convention as the DEVOID paper %for now,
%and I'd prefer to move this away from a derivation but not sure how to do so and maintain formality): % TODO check

%\begin{mathpar}
%\mprset{flushleft}
%\small
%\hfill\fbox{$\Gamma$ $\vdash$ $(T_A, T_B)$ $\Downarrow_{C}$ $t$}\\%

%\inferrule[Dep-Constr-Conclusion]
%  { \Gamma \vdash \vec{t_{B_j}} : \vec{T_{B_j}} \\ \Gamma \vdash Constr(j, B)\ \vec{t_{B_j}} : B_{\mathrm{ind}} \vec{i_B}  }
%  { \Gamma \vdash (A\ \vec{i_A},\ B_{\mathrm{ind}}\ \vec{i_B}) \Downarrow_{p_{c}} \exists\ (\vec{i_B}[\mathrm{off}\ A\ B]) (Constr(j, B)\ \vec{t_{B_j}}) }

%\inferrule[Dep-Constr-Index] % new hypothesis for index
%  { \mathrm{new}\ n_B\ b_B \\ \Gamma,\ n_B : t_B \vdash (\Pi (n_A : t_A) . b_A,\ b_B) \Downarrow_{i_{c}} t }
%  {  \Gamma \vdash (\Pi (n_A : t_A) . b_A,\ \Pi (n_B : t_B) . b_B) \Downarrow_{C} t}

%\inferrule[Dep-Constr-IH] % inductive hypothesis
%  { \Gamma,\ n_B : B\ \vec{i_B} \vdash (b_A [n_B / n_A], b_B [\pi_l\ n_B / \vec{i_B}[\mathrm{off}\ A\ B]]) \Downarrow_{C} t }
%  { \Gamma \vdash (\Pi (n_A : A\ \vec{i_A}) . b_A, \Pi (n_B : B\ \vec{i_B}) \Downarrow_{C} \lambda (n_B : B\ \vec{i_B}) . t }

%\inferrule[Dep-Constr-Prod] % otherwise, unchanged (when we get rid of the gross fall-through thing, needs not new, and needs to check t_A and t_B not IHs)
%  { \Gamma,\ n_B : t_B \vdash (b_A [n_B / n_A], b_B) \Downarrow_{C} t }
%  { \Gamma \vdash (\Pi (n_A : t_A) . b_A, \Pi (n_B, t_B) . b_B) \Downarrow_{C} \lambda (n_B : t_B) . t }\\

%\inferrule[Dep-Constr]
%{ \Gamma \vdash Constr(j, A) : C_{A_j} \\ \Gamma \vdash (C_{A_j}, C_{B_j}) \Downarrow_{C} t }
%{ \Gamma \vdash (Constr(j, A), Constr(j, B_{\mathrm{ind}}) \Downarrow_{C} t }
%\end{mathpar}
%and \lstinline{DepElim(b, p)} similarly:

%\begin{mathpar}
%TODO
%\end{mathpar}

%Then:

%\begin{lstlisting}
%DepConstr(j, A) : C$_{A_{j}}$ := Constr(j, A)
%DepConstr(j, B) : C$_{A_{j}}$[B / A] := DepConstr(j, B)

%DepElim(a, p){f$_{1}$, $\ldots$, f$_{n}$} : p a := Elim(a, p){f$_{1}$, $\ldots$, f$_{n}$}
%DepElim(b, p){f$_{1}$, $\ldots$, f$_{n}$} : p b := DepElim(b, p)

%Eta(A) := $\lambda$(a : A).a
%Eta(B) := $\lambda$(b : B).$\exists$ ($\pi_l$ b) ($\pi_r$ b)
%\end{lstlisting}

% TODO investigate below projection thing, and write in when you finish
%For now assume we have some \lstinline{pack} function to pack into an existential;
%this is just for convenience.
%The indexer is just the first projection of this lifted across the eliminator rule, AFAIK---note this isn't exactly $\Pi_{l}$ like we use
%in the tool, but is really an eliminated $\Pi_{l}$? I will need to check on this, it's the only weird part.
%Also assume some \lstinline{index_args} function to add the new index to the appropriate arguments---I'll
%elaborate on this later but it's also something search needs to find and it's determined in terms of the \lstinline{indexer} that search finds.
%Also now, we no longer assume primitive projections.

%\subsubsection{Unpack sigma}

%This one is kind of weird but it gets us user-friendly types. I'll explain later.

%\begin{lstlisting}
%DepConstr(j, A) := (* TODO pack into existential, deal with equality *)
%DepConstr(j, B) : C$_{B_{j}}$ := Constr(j, B)

%DepElim(a, p){f$_{1}$, $\ldots$, f$_{n}$} : p a := (* TODO *)
%DepElim(b, p){f$_{1}$, $\ldots$, f$_{n}$} : p b := Elim(b, p){f$_{1}$, $\ldots$, f$_{n}$}

%Eta(A) := $\lambda$(a : A).$\exists$ ($\exists$ ($\pi_l$ ($\pi_l$ a)) ($\pi_r$ ($\pi_l$ a))) ($\pi_r$ a)
%Eta(B) := $\lambda$(b : B).b
%\end{lstlisting}

%\subsubsection{Records and tuples}

%This one should be easier. We'll play a similar trick with $B$ and $B_{ind}$ like we do for algebraic,
%and give things similar names.
%Then:

%\begin{lstlisting}
%DepConstr(j, A) : C$_{A_{j}}$ := Constr(j, A)
%DepConstr(j, B) : C$_{A_{j}}$[B / A] := $\lambda$ ($\vec{t_{A_j}}$ : $\vec{T_{A_j}}$) . (* TODO recursively pack into pair *)

%DepElim(a, p){f$_{1}$, $\ldots$, f$_{n}$} : p a := Elim(a, p){f$_{1}$, $\ldots$, f$_{n}$}
%DepElim(b, p){f$_{1}$, $\ldots$, f$_{n}$} : p b := (* TODO recursively eliminate product *)

%Eta(A) := $\lambda$(a : A).a
%Eta(B) := (* TODO recursive eta *)
%\end{lstlisting}
