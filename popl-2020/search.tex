\section{Case Studies \& Configurations}
\label{sec:search}

As we saw in Figure~\ref{fig:listswap} in Section~\ref{sec:overview},
there are two ways to use \toolname:

\begin{enumerate}
\item with automatic configuration, using search procedures to discover configurations for entire classes of equivalences, and
\item with manual configuration, providing the configuration directly.
\end{enumerate}
These two modes of configuration have made it possible for \toolname to support a wide
varity of applications.
This section explans four different case studies in using \toolname:

\begin{enumerate}
\item Refactoring automatically generated proofs for an industrial user (automatic, Section~\ref{sec:industry})
\item Generating dependently typed vector functions and proofs from functions and proofs about lists and their lengths (automatic, Section~\ref{sec:dep})
\item Supporting a benchmark from a user study of Coq proof engineers (automatic, Section~\ref{sec:replica})
\item Porting functions and proofs about unary numbers to functions and proofs about binary numbers (manual, Section~\ref{sec:bin})
\end{enumerate}
For each case study, we explain the configuration used, walk through an example, and describe takeaways.
In all, we found the following:

\begin{enumerate}
\item \toolname handled examples of both proof refactoring and repair. In practice, the line between these was sometimes
blurred, as was the line with proof reuse.
\item \toolname was configurable to different classes of changes. Each search procedure for automatic configuration
took between two days and two weeks to write, and handled an entire class of equivalences corresponding to a real use case.
Manual configuration was possible for an interesting use case, but remained challenging.
\item \toolname had good enough workflow integration to support real users.
The tactic decompiler was far from perfect, but showed promising early results with clear paths to improvements.
Some user workflows were unanticipated and informed changes in the design of \toolname.
\end{enumerate}
% TODO is it a contribution in itself to have real observation of users in a 3P proof refactoring/repair tool?

%\begin{enumerate} TODO
%\item LOC for each equiv/structure
%\item Choosing the swap equivalence
%\item Fun eliminators
%\end{enumerate}

\subsection{Industrial Use}
\label{sec:industry}

An industrial user at \company\footnote{Name withheld for double-blind review.} has been using \toolname in proving
correct an implementation of the TLS handshake protocol.
While this is ongoing work, thus far,
\toolname has helped \company integrate Coq with their existing verification workflow.

Before contact, \company had been using a custom solver-aided verification language to prove correct C programs.
They had found that at times, those constraint solvers got stuck, and they could not
progress on proofs about those programs.
They had built a compiler that translates their solver-aided language into Coq's specification language Gallina,
that way their proof engineers could finish stuck proofs interactively using Coq.
However, they had found that the generated Gallina programs and specifications were sometimes too difficult to work with.

A proof engineer at \company has used \toolname to work with those automatically generated programs and specifications
with the following workflow:

\begin{enumerate}
\item First, the proof engineer uses \toolname to refactor the automatically generated programs and specifications into more
human-readable programs and specifications.
\item Next, the proof engineer writes Coq proofs about the more human-readable programs and specifications.
\item Finally, the proof engineer uses \toolname again to refactor those proofs about human-readable programs and specifications back to
proofs about the original automatically generated programs and specifications.
\end{enumerate}
This workflow has allowed for industrial integration with Coq and has helped the user write functions and proofs
that would have otherwise been difficult.

% TODO will add better proofs here once Val sends them

\subsubsection{Configuration}

\begin{figure}
\begin{minipage}{0.25\textwidth}
   \lstinputlisting[firstline=1, lastline=4]{records.tex}
\end{minipage}
\hfill
\begin{minipage}{0.74\textwidth}
   \lstinputlisting[firstline=6, lastline=9]{records.tex}
\end{minipage}
\caption{Two unnamed tuples (left) and corresponding named records (right).}
\label{fig:records}
\end{figure}

Minimal examples corresponding to the proofs that the proof engineer used \toolname for
can be found in \lstinline{minimal_records.v}.
The proof engineer used \toolname to port anonymous tuples produced by \company's compiler
to named records, as shown in the example in Figure~\ref{fig:records}.

We implemented a search procedure for the proof engineer to automatically configure the proof term transformation to an equivalence
between nested tuples and named records.
The search procedure triggered automatically when the proof engineer called the \lstinline{Repair} or \lstinline{Repair module} command
with the tuple and record as arguments.
It set \A to be the record type and \B to be the tuple.
There were no \lstinline{RewEta} since there were no inductive hypotheses.
For the record \A, it set \lstinline{IdEta} to identity, \lstinline{DepConstr} to the single
constructor corresponding to the record constructor, and \lstinline{DepElim} to the standard record eliminator.
For the tuple \B, on the other hand, it expanded identity to deal with non-primitive projections,
in our example:
\begin{lstlisting}
Definition id_eta_B (H : Tuple.input) : Tuple.input :=
  (fst H, (fst (snd H), snd (snd H))).
\end{lstlisting}
it constructed a nested tuple by recursively applying the pair constructor:

\begin{lstlisting}
Definition dep_constr_0_B (firstBool : bool) (numberI : nat) (secondBool : bool) : Tuple.input :=
  (firstBool, (numberI, secondBool)).
\end{lstlisting}
and it recursively eliminated over the pair:

\begin{lstlisting}
Definition dep_elim_B (P : Tuple.input -> Type) (f : $\forall$ f n s, P (dep_constr_0_B f n s)) (i : Tuple.input) : P (id_eta_B i) :=
  prod_rect
    (fun p : bool * (nat * bool) => P (id_eta_b p))
    (fun (a : bool) (b : nat * bool) =>
      f a (fst b) (snd b))
    i.
\end{lstlisting}
This induced an equivalence between the nested tuple and record,
which \toolname generated and proved automatically.

\subsubsection{Example}
Using this configuration, the proof engineer automatically ported this compiler-generated function:

\begin{lstlisting}
Definition op (r : bool * (nat * bool)) : nat * bool :=
  (fst (snd r), andb (firstBool r) (secondBool r)).
\end{lstlisting}
to this function:

\begin{lstlisting}
Definition op (r : Record.input) : Record.output :=
  {|
     numberO := numberI r;
     andBools := andb (fistBool r) (secondBool r)
  |}.
\end{lstlisting}
The proof engineer then wrote this proof about the new specification: % TODO did val write this or did I?

\begin{lstlisting}
Theorem and_spec_true_true :
  forall (r : Record.input),
    firstBool r = true ->
    secondBool r = true ->
    andBools (Record.op r) = true.
Proof.
  destruct r as [f n s].
  unfold Record.op.
  simpl in *.
  apply andb_true_intro.
  intuition.
Qed.
\end{lstlisting}
and then used \toolname to automatically port this proof back to a proof of the original function: % TODO induction & preprocess though

\begin{lstlisting}
Theorem and_spec_true_true :
  forall (r : Tuple.input),
    fst r = true ->
    snd (snd r) = true ->
    andb (Tuple.op r) = true.
\end{lstlisting}
The proof engineer had no need for the decompiler here because the proof engineer modified
the proof over records directly. % TODO it is being silly; can we get it working?

\subsubsection{Takeaways}

\paragraph{Refactoring \& Repair}:
The proof engineer used \toolname for both refactoring and repair.
Refactorings like the changes from tuples to records were the most common use cases.
The proof engineer has also used \toolname to repair some non-dependently-typed
functions and proofs to instead use dependent types.

\paragraph{Configuration \& Flexibility}:
The proof engineer used at least three automatic configurations and no manual configurations.
One of these configurations was backed by a search procedured that we implemented
specifically to support this user: the search procedure to
configure the equivalence between nested tuples and records.
This search procedure took about two weeks for us to implement using techniques from
the repair and reuse tools \textsc{Pumpkin Patch} and \textsc{Devoid}.
Flexibility could be further improved by exposing an interface to users to allow them to
write these search procedures themselves.

\paragraph{Workflow Integration}:
The industrial proof engineer was able to use \toolname to integrate Coq with existing proof engineering
workflows using solver-aided tools at \company.
The workflow for using \toolname itself, however, was a bit nonstandard,
and there was little need for tactic proofs about the compiler-generated functions and specifications.
In the initial days, we worked closely with the proof engineer;
later, the proof engineer worked independently and reached out occasional by email.
\toolname was usable enough for this to work.
However, we found two challenges with workflow integration:
the proof engineer sometimes could not distinguish between user errors and bugs in our code,
and the proof engineer typically waited only about ten seconds at most for \toolname
to port a function or proof.
Both of these observations informed significant changes to \toolname, like better error messages
and caching of transformed subterms.

\subsection{Vectors from Lists and their Lengths}
\label{sec:dep}

\begin{figure}
\begin{minipage}{0.40\textwidth}
   \lstinputlisting[firstline=1, lastline=4]{listtovect.tex}
\end{minipage}
\hfill
\begin{minipage}{0.58\textwidth}
   \lstinputlisting[firstline=6, lastline=9]{listtovect.tex}
\end{minipage}
\caption{A vector (right) is a list (left) ornamented by its length.}
\label{fig:listtovect}
\end{figure}

The proof term transformation in \toolname is based on the proof term transformation from
the \textsc{Devoid} proof reuse tool.
\textsc{Devoid} is a proof reuse tool for \textit{algebraic ornaments}~\cite{mcbride}. Algebraic ornaments describe relations
between two inductive types, where one inductive type is exactly the old inductive type indexed by a fold
over the original type.
The running example of this in the \textsc{Devoid} paper is the relation between a list and a
length-indexed vector (Figure~\ref{fig:listtovect}).

We configured the generalized algorithm in \toolname to support algebraic ornaments like those found in \textsc{Devoid},
and passed all of the regression tests from \textsc{Devoid}.
We found that this simplified the algorithm from \textsc{Devoid}.
In addition, we added one more configuration to automate effort that had been manual in \textsc{Devoid}.
Several proof engineers including our industrial proof engineer, a Reddit user,
and someone on the \lstinline{coq-club} message board contacted us expressing interest in using this functionality,
though we do not yet know if the latter two ended up using \toolname. % TODO honestly ask

\subsubsection{Configuration}

We used two configurations to easy development with dependent types using algebraic ornaments.
The first fits the proof term transformation from \textsc{Devoid} into the \toolname framework:
it sets \A to the unornamented type and \B to the ornamented type at \textit{some} index---\textsc{Devoid}
calls the resulting type \textit{packed}.
In the case of \lstinline{list} and \lstinline{vector}, that configuration
transports proofs across this equivalence:

\begin{lstlisting}
list T $\simeq$ $\Sigma$ (n : nat) . vector T n
\end{lstlisting}
The second configuration goes beyond the functionality from \textsc{Devoid}, and provides
the missing link to get proofs about \B at a \textit{particular} index:

\begin{lstlisting}
{ l : list T & length l = n } $\simeq$ vector T n
\end{lstlisting}
producing something \textit{unpacked},
a step that \textsc{Devoid} had left to the user.

The search procedure for the first configuration is based heavily on the search procedure from \textsc{Devoid}.
Since \textsc{Devoid} supports ornaments which by definition represent types with the same inductive structure,
\lstinline{RewEta} for both \A and \B was always reflexivity.
For the inductive type \A, \lstinline{IdEta}, \lstinline{DepConstr}, and \lstinline{DepElim}
correspond to the identity function, the constructors of \A, and the eliminator for \A, respectively.
For \B, \lstinline{IdEta} expands the input to avoid reliance on primitive projections,
in our example:

\begin{lstlisting}
Definition id_eta_B (T : Type) (s : $\Sigma$ (n : nat) . vector T n) : $\Sigma$ (n : nat) . vector T n  :=
  $\exists$ ($\pi_l$ s) ($\pi_r$ s).
\end{lstlisting}
This implements \textit{repacking} from \textsc{Devoid}.
Like in \textsc{Devoid},
\lstinline{DepConstr} packs constructors of \B:

\begin{lstlisting}
Definition dep_constr_B_0 (T : Type) : $\Sigma$ (n : nat) . vector T n :=
  $\exists$ 0 (Vector.nil A).

Definition dep_constr_B_1 (T : Type) (t : T) (s : $\Sigma$ (n : nat) . vector T n) : $\Sigma$ (n : nat) . vector T n :=
  $\exists$ (S ($\pi_l$ s)) (Vector.cons ($\pi_l$ s) t ($\pi_r$ s)).
\end{lstlisting}
and \lstinline{DepElim} eliminates its projections:

\begin{lstlisting}
Definition dep_elim_B_0 (T : Type) (P : $\Sigma$ (n : nat) . vector T n -> Type) (pnil : P (dep_constr_B_0 T)) (pcons : $\forall$ t s, P (id_eta_B T s) -> P (dep_constr_B_1 T t s)) (s : $\Sigma$ (n : nat) . vector T n) : P (id_eta_B T s) :=
  vector_rect
    T
    (fun (n : nat) (v : vector T n) => P ($\exists$ n v))
    pnil
    (fun (t : T) (n : nat) (v : vector T n) => pcons t ($\exists$ n v))
    ($\pi_l$ s)
    ($\pi_r$ s).
\end{lstlisting}
Together this configuration is enough to capture all of the functionality from \textsc{Devoid} for both search and lifting,
save for the optional proof that these induce an equivalence, which \toolname borrows from \textsc{Devoid}.

To get from lists to vectors \textit{at a particular length}, \toolname implements one additional configuration.
This configuration corresponds to the equivalence between packed types at a particular projection
and unpacked types, in our example:

\begin{lstlisting}
{ s : $\Sigma$ (n : nat) . vector T n & $\pi_l$ s = n } $\simeq$ vector T n
\end{lstlisting}
By composition with the initial equivalence, this lets us transport proofs
across the equivalence we eventually want:

\begin{lstlisting}
{ l : list T & length l = n } $\simeq$ vector T n
\end{lstlisting}
since the left-hand sides of these two equivalences are equal up to transport along the first equivalence.

The second configuration is similar to the first configuration, except that it also carries equality proofs over the indices.
That is, it views the type \B like \lstinline{vector T n} as implicitly representing \{\lstinline{ v : vector T n' & n' = n }\} for some \lstinline{n'}.
This is seen, for example, in the identity rule for \B, here: 
% TODO both should take the same number of arguments, even if id_eta_A takes fewer. also does this belong in rew_eta?

\begin{lstlisting}
Definition id_eta_B (T : Type) (n n' : nat) (v : vector T n') (H : n = n') : vector T n :=
  eq_rect n' (vector T) v n H.
\end{lstlisting}
which is the identity function generalized over any equal index.

\subsubsection{Example}

The expanded example from the \textsc{Devoid} paper is in \lstinline{Example.v}.
The \textsc{Devoid} example ported a list \lstinline{zip} function,
a \lstinline{zip_with} function, and a proof \lstinline{zip_with_is_zip} relating the two
functions from lists to packed vectors.
It then manually ports those proofs to proofs over unpacked vectors at a particular index.
The updated \toolname example automates this last step.

The workflow for this was a bit different than it was with \textsc{Devoid}.
First, we used a custom eliminator \toolname generated to combine the list functions
with the length invariants from \textsc{Devoid}, and to combine the list proofs
with the proofs about those length invariants from \textsc{Devoid}.
This gave us a proof of this lemma:

\begin{lstlisting}
Lemma zip_with_is_zip :
  forall A B n (v1 : { l1 : list A & length l1 = n }) (v2 : { l2 : list B & length l2 = n }),
    zip_with pair v1 v2 = zip v1 v2.
\end{lstlisting}
with functions \lstinline{zip_with} and \lstinline{zip} operating over lists at given lengths.
We then ran \lstinline{Repair module} to transport those functions and proofs using the first
configuration, which proved this lemma:

\begin{lstlisting}
Lemma zip_with_is_zip :
  forall A B n (v1 : { l1 : $\Sigma$(n : nat).vector T n & $\pi_l$ l1 = n }) (v2 : { l2 : $\Sigma$(n : nat).vector T n & $\pi_l$ l2 = n }),
    zip_with pair v1 v2 = zip v1 v2.
\end{lstlisting}
with functions \lstinline{zip_with} and \lstinline{zip} operating over vectors at given projections.
We composed this with \lstinline{Repair module} on the second configuration,
which proved this lemma:

\begin{lstlisting}
Lemma zip_with_is_zip :
  forall A B n (v1 : vector A n) (v2 : vector B n),
    zip_with pair v1 v2 = zip v1 v2.
\end{lstlisting}
with functions \lstinline{zip_with} and \lstinline{zip} operating directly over vectors.

\subsubsection{Takeaways}

\paragraph{Refactoring \& Repair}
can view as repair bc started from list proofs

\paragraph{Configuration \& Flexibility}
fewer rules!
removes reliance on special rules for eliminators and stuff.
moves gunk out of lifting algorithm and into configuration, like for changing number of arguments.
opposite direction was super hard and ran into equality problem, suggesting 
we need \lstinline{RewEta} here.

\paragraph{Workflow Integration}
While this produced proof terms with the correct type, the decompiler struggled
to generate useful tactic scripts for dependently typed functions.
tbh though I don't even know how to do that well myself and I found \toolname could write
functions and proofs I wouldn't be able to write by hand so -shrug-
%We discuss ideas for handling this in Section~\ref{sec:decompiler}.
% TODO actually do this
custom eliminators.
termination

\subsection{REPLICA Benchmarks}
\label{sec:replica}

Section~\ref{sec:overview} showed an example of swapping constructors.
The configuration this used handles arbitrary swapping and renaming of constructors of inductive types.
In addition to using this for list proofs, we also used it to replay part of a change found in
a REPLICA benchmark.

In that change, the proof engineer had a simple language:

\begin{lstlisting}
Inductive Term : Set :=
  | Var : Identifier -> Term
  | Int : Z -> Term
  | Eq : Term -> Term -> Term
  | Plus : Term -> Term -> Term
  | Times : Term -> Term -> Term
  | Minus : Term -> Term -> Term
  | Choose : Identifier -> Term -> Term.
\end{lstlisting}
as well as some definitions and proofs about the language.
As part of the change, the user moved the \lstinline{Int} constructor down:

\begin{lstlisting}
Inductive Term : Set :=
  | Var : Identifier -> Term
  | Eq : Term -> Term -> Term
  | Int : Z -> Term
  | Plus : Term -> Term -> Term
  | Times : Term -> Term -> Term
  | Minus : Term -> Term -> Term
  | Choose : Identifier -> Term -> Term.
\end{lstlisting}

Using the search procedure for the swap configuration, we were able to use \toolname
to automatically configure the program transformation to move this constructor,
then transform all of the functions and proofs about the language.
We also went beyond the proof engineer's change and tried swapping two constructors with the same type,
or renaming all of the constructors.
In all cases, with a bit of human guidance, \toolname was able to repair the functions and proofs.

Note that, in the original benchmark, for just the first swap change,
the original tactics would have also worked for the proof. 
As we saw in Section~\ref{sec:overview}, this is not always true.
REPLICA also saw that users move cases in match statements to match moved constructors
in inductive types, and the transported terms \toolname generated did this automatically
(with induction principles though).

\subsubsection{Configuration}
The configuration this used handles swapping and renaming constructors of inductive types.
When a user swaps or renames constructors, the new type is equivalent to the old type exactly.
The configuration just maps the old constructors to the swapped new constructors, and maps appropriate
cases of the old eliminator to appropriate cases of the new eliminator.
Equality does not change.

More formally, let $A$ and $B$ be inductive types:

\begin{lstlisting}
$A$ := $\mathrm{Ind} (\mathit{Ty}_A : \Pi (\vec{i_A} : \vec{\mathrm{X}_A}) . \mathrm{s}_A)\{\mathrm{C}_{A_1}, \ldots, \mathrm{C}_{A_n}\}$
$B$ := $\mathrm{Ind} (\mathit{Ty}_B : \Pi (\vec{i_B} : \vec{\mathrm{X}_B}) . \mathrm{s}_B)\{\mathrm{C}_{B_1}, \ldots, \mathrm{C}_{B_n}\}$
\end{lstlisting}		
Assume there is some invertible swap map $m$ such that for any index $j$,
\lstinline{C}$_{B_{m(j)}}$ is exactly \lstinline{C}$_{A_j}[B / A]$.
Then:

\begin{lstlisting}
DepConstr(j, A) : C$_{A_{j}}$ := Constr(j, A) 
DepConstr(j, B) : C$_{A_{j}}$[B / A] := Constr(m(j), B)

DepElim(a, p){f$_{1}$, $\ldots$, f$_{n}$} : p a := Elim(a, p){f$_{1}$, $\ldots$, f$_{n}$}
DepElim(b, p){f$_{1}$, $\ldots$, f$_{n}$} : p b := Elim(b, p){f$_{m(1)}$, $\ldots$, f$_{m(n)}$}

IdEta(A) := $\lambda$ ($\vec{t}$ : $\vec{T}$) (a : A $\vec{t}$).a
IdEta(B) := $\lambda$ ($\vec{t}$ : $\vec{T}$) (b : B $\vec{t}$).b

RewEta(A) := eq_refl A
RewEta(B) := eq_refl B
\end{lstlisting}

\subsubsection{Example}

Example (honestly let's just show with particular type and explain more generally, and leave out formalities until later if we need them).

\subsubsection{Takeaways}

\paragraph{Refactoring \& Repair}

\paragraph{Configuration \& Flexibility}

\paragraph{Workflow Integration}

\subsection{Unary and Binary Numbers}
\label{sec:bin}

All of the configurations we have seen so far have not included \lstinline{Rew-Eta}.
This is because they are all ornaments---all changes preserve the same inductive structure.
We can also repair proofs to use a type with a different inductive structure.
We do not yet have any search procedures for this, but since \toolname lets you supply
custom configurations, you can use a custom configuration for this.

As an example, we ported functions and proofs between Coq's natural numbers:

\begin{lstlisting}
Inductive nat :=
| O : nat
| S : nat -> nat.
\end{lstlisting}
and a simple version of binary numbers, which comes from another source:

\begin{lstlisting}
Inductive binnat :=
| zero : binnat
| consOdd : binnat -> binnat
| consEven : binnat -> binnat.
\end{lstlisting}
This is one of the oldest known transport problems.
We are able to supply the equivalence and configuration manually,
then port functions and proofs.
For example, we port the addition function automatically,
then we can automatically port a proof of the lemma \lstinline{plus_n_Sm}:

\begin{lstlisting}
Lemma plus_n_Sm:
  forall (n m : nat), Nat.S (Nat.add n m) = Nat.add n (Nat.S m).
\end{lstlisting}
to a proof over binary numbers:

\begin{lstlisting}
Lemma plus_n_Sm:
  forall (n m : binnat), Bin.S (Bin.add n m) = Bin.add n (Bin.S m).
Proof.
  intros n m.
  induction (bin_natty n) as [|n0 n1 IHn0].
  - reflexivity.
  - apply (binnat_plus_n_Sm_expanded_rewrites2_inductive m n0 IHn0).
Defined.
\end{lstlisting}
where the new proof and new addition function do not at any point convert
the binary number back to natural numbers.

Notably, to port \lstinline{plus_n_Sm}, we had to deal with the equality problem.
\lstinline{RewEta} is what let us do this.
There is still one caveat, though: we had to explicitly write out applications of \lstinline{RewEta}
in our proof over unary natural numbers, since we do not yet have a way to configure \toolname with custom
matching procedures, and unification was not sufficient for matching with \lstinline{RewEta}.
We hope to make it possible for the user to also add custom matching rules,
and we think some ideas from Section~\ref{sec:discussion} could help with this.
Another thing worth noting is that we do not automatically get the efficient version of \lstinline{add} this way,
though it's worth determining whether there is a way to configure \toolname to do so.

\subsubsection{Configuration}
We supplied this configuration manually.
Our definitions were as follows (eventually should just show the interesting parts):

\begin{lstlisting}
DepConstr(1, nat) := O 
DepConstr(2, nat) := S
DepConstr(1, binnat) := zero
DepConstr(2, binnat) :=
  binnat_rec
    (fun _ : binnat => binnat)
    (consOdd zero)
    (fun b0 _ : binnat => consEven b0)
    (fun _ IHb : binnat => consOdd IHb)

DepElim(n, p){f$_{1}$, f$_{2}$} : p n := nat_rect p f$_{1}$ f$_{2}$ n
DepElim(b, p){f$_{1}$, f$_{2}$} : p b := natty_rect p f$_{1}$ f$_{2}$ b (bin_natty n)

IdEta(nat) := fun (n : nat) => n
IdEta(binnat) := fun (b : binnat) => b

RewEta(2, nat) := fun (P : nat -> Type) (PO : P O) (PS : forall n : nat, P n -> P (nat_S n))
  (n : nat) (Q : P (nat_S n) -> Type) (H : Q (PS n (nat_rect P PO PS n))) =>
eq_rect (PS n (nat_rect P PO PS n)) (fun H0 : P (nat_S n) => Q H0) H
  (nat_rect P PO PS (nat_S n)) eq_refl
RewEta(2, binnat) := fun (P : binnat -> Type) (PO : P z) (PS : forall b : binnat, P b -> P (S b))
  (n : binnat) (Q : P (e.suc_binnat n) -> Type)
  (H : Q (PS n (binnat_nat_rect P PO PS n))) =>
eq_rect (PS n (binnat_nat_rect P PO PS n)) (fun H0 : P (e.suc_binnat n) => Q H0)
  H (binnat_nat_rect P PO PS (e.suc_binnat n)) (eq_sym (refold_elim_S P PO PS n)).
\end{lstlisting}
We need to introduce \lstinline{natty} and credit it to the Conor McBride paper,
and note why we go through it.
Interestingly, we used transport across algebraic ornaments in order to determine
\lstinline{DepElim}.
We need to note how \lstinline{bin_natty} is generated that way.
Also we need multiple \lstinline{RewEta} one for each constructor ugh.

\subsubsection{Example}

\subsubsection{Takeaways}

\paragraph{Refactoring \& Repair}

\paragraph{Configuration \& Flexibility}

\paragraph{Workflow Integration}


% TODO what does the tactic decompiler do for this? It's broken. Why?

%\subsubsection{Algebraic}

%It is straightforward to fit the search algorithm from DEVOID into this framework, and in fact
%we can loosen the restriction that the language has primitive projections.
%Let $A$ be $A$ from DEVOID, let $B_{ind}$ be $B$ from DEVOID, let $I_B$ be $I_B$ from DEVOID,
%and let \lstinline{index} be \lstinline{index} from DEVOID.
%Let $B$ wrap $B_{ind}$ packed into a sigma type:

%\begin{lstlisting}
%B := $\lambda$ ($\vec{t}$ : $\vec{T}$) . ($\Sigma$ (i : I$_B$ $\vec{t}$) . B$_{ind}$ (index i $\vec{t}$))
%\end{lstlisting}
%Let $\vec{T_{B_j}}$ be the arguments of constructor type $C_{B_j}$ (type of constructor of $B_{\mathrm{ind}}$).
%Define \lstinline{DepConstr(j, B)} recursively using the following derivation (based on and same fall-through convention as the DEVOID paper %for now,
%and I'd prefer to move this away from a derivation but not sure how to do so and maintain formality): % TODO check

%\begin{mathpar}
%\mprset{flushleft}
%\small
%\hfill\fbox{$\Gamma$ $\vdash$ $(T_A, T_B)$ $\Downarrow_{C}$ $t$}\\%

%\inferrule[Dep-Constr-Conclusion]
%  { \Gamma \vdash \vec{t_{B_j}} : \vec{T_{B_j}} \\ \Gamma \vdash Constr(j, B)\ \vec{t_{B_j}} : B_{\mathrm{ind}} \vec{i_B}  }
%  { \Gamma \vdash (A\ \vec{i_A},\ B_{\mathrm{ind}}\ \vec{i_B}) \Downarrow_{p_{c}} \exists\ (\vec{i_B}[\mathrm{off}\ A\ B]) (Constr(j, B)\ \vec{t_{B_j}}) }

%\inferrule[Dep-Constr-Index] % new hypothesis for index
%  { \mathrm{new}\ n_B\ b_B \\ \Gamma,\ n_B : t_B \vdash (\Pi (n_A : t_A) . b_A,\ b_B) \Downarrow_{i_{c}} t }
%  {  \Gamma \vdash (\Pi (n_A : t_A) . b_A,\ \Pi (n_B : t_B) . b_B) \Downarrow_{C} t}

%\inferrule[Dep-Constr-IH] % inductive hypothesis
%  { \Gamma,\ n_B : B\ \vec{i_B} \vdash (b_A [n_B / n_A], b_B [\pi_l\ n_B / \vec{i_B}[\mathrm{off}\ A\ B]]) \Downarrow_{C} t }
%  { \Gamma \vdash (\Pi (n_A : A\ \vec{i_A}) . b_A, \Pi (n_B : B\ \vec{i_B}) \Downarrow_{C} \lambda (n_B : B\ \vec{i_B}) . t }

%\inferrule[Dep-Constr-Prod] % otherwise, unchanged (when we get rid of the gross fall-through thing, needs not new, and needs to check t_A and t_B not IHs)
%  { \Gamma,\ n_B : t_B \vdash (b_A [n_B / n_A], b_B) \Downarrow_{C} t }
%  { \Gamma \vdash (\Pi (n_A : t_A) . b_A, \Pi (n_B, t_B) . b_B) \Downarrow_{C} \lambda (n_B : t_B) . t }\\

%\inferrule[Dep-Constr]
%{ \Gamma \vdash Constr(j, A) : C_{A_j} \\ \Gamma \vdash (C_{A_j}, C_{B_j}) \Downarrow_{C} t }
%{ \Gamma \vdash (Constr(j, A), Constr(j, B_{\mathrm{ind}}) \Downarrow_{C} t }
%\end{mathpar}
%and \lstinline{DepElim(b, p)} similarly:

%\begin{mathpar}
%TODO
%\end{mathpar}

%Then:

%\begin{lstlisting}
%DepConstr(j, A) : C$_{A_{j}}$ := Constr(j, A)
%DepConstr(j, B) : C$_{A_{j}}$[B / A] := DepConstr(j, B)

%DepElim(a, p){f$_{1}$, $\ldots$, f$_{n}$} : p a := Elim(a, p){f$_{1}$, $\ldots$, f$_{n}$}
%DepElim(b, p){f$_{1}$, $\ldots$, f$_{n}$} : p b := DepElim(b, p)

%IdEta(A) := $\lambda$(a : A).a
%IdEta(B) := $\lambda$(b : B).$\exists$ ($\pi_l$ b) ($\pi_r$ b)
%\end{lstlisting}

% TODO investigate below projection thing, and write in when you finish
%For now assume we have some \lstinline{pack} function to pack into an existential;
%this is just for convenience.
%The indexer is just the first projection of this lifted across the eliminator rule, AFAIK---note this isn't exactly $\Pi_{l}$ like we use
%in the tool, but is really an eliminated $\Pi_{l}$? I will need to check on this, it's the only weird part.
%Also assume some \lstinline{index_args} function to add the new index to the appropriate arguments---I'll
%elaborate on this later but it's also something search needs to find and it's determined in terms of the \lstinline{indexer} that search finds.
%Also now, we no longer assume primitive projections.

%\subsubsection{Unpack sigma}

%This one is kind of weird but it gets us user-friendly types. I'll explain later.

%\begin{lstlisting}
%DepConstr(j, A) := (* TODO pack into existential, deal with equality *)
%DepConstr(j, B) : C$_{B_{j}}$ := Constr(j, B)

%DepElim(a, p){f$_{1}$, $\ldots$, f$_{n}$} : p a := (* TODO *)
%DepElim(b, p){f$_{1}$, $\ldots$, f$_{n}$} : p b := Elim(b, p){f$_{1}$, $\ldots$, f$_{n}$}

%IdEta(A) := $\lambda$(a : A).$\exists$ ($\exists$ ($\pi_l$ ($\pi_l$ a)) ($\pi_r$ ($\pi_l$ a))) ($\pi_r$ a)
%IdEta(B) := $\lambda$(b : B).b
%\end{lstlisting}

%\subsubsection{Records and tuples}

%This one should be easier. We'll play a similar trick with $B$ and $B_{ind}$ like we do for algebraic,
%and give things similar names.
%Then:

%\begin{lstlisting}
%DepConstr(j, A) : C$_{A_{j}}$ := Constr(j, A)
%DepConstr(j, B) : C$_{A_{j}}$[B / A] := $\lambda$ ($\vec{t_{A_j}}$ : $\vec{T_{A_j}}$) . (* TODO recursively pack into pair *)

%DepElim(a, p){f$_{1}$, $\ldots$, f$_{n}$} : p a := Elim(a, p){f$_{1}$, $\ldots$, f$_{n}$}
%DepElim(b, p){f$_{1}$, $\ldots$, f$_{n}$} : p b := (* TODO recursively eliminate product *)

%IdEta(A) := $\lambda$(a : A).a
%IdEta(B) := (* TODO recursive eta *)
%\end{lstlisting}
