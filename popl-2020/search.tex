\section{Case Studies: \textsc{Carrot} Four Ways}
\label{sec:search}

This section explans four different case studies in using \toolname:

\begin{enumerate}
\item We report on the experiences of an industrial proof engineer who is using \toolname to integrate Coq with a company proof engineering workflow and write proofs about an implementation of the TLS handshake protocol (automatic, Section~\ref{sec:industry}).
\item We use \toolname to update functions and proofs about lists to functions and proofs about dependently typed vectors of
a particular length (automatic, Section~\ref{sec:dep}).
\item We use \toolname to support multiple variants of a repair benchmark from a user study of Coq proof engineers (automatic, Section~\ref{sec:replica}).
\item We use \toolname to port functions and proofs about unary numbers to functions and proofs about binary numbers (manual, Section~\ref{sec:bin}).
\end{enumerate}
For each case study, we explain the configuration used, walk through an example, and describe lessons learned.
In all, we found the following:

\begin{enumerate}
\item \toolname was flexible and configurable to different classes of changes. Each search procedure for automatic configuration
took between two days and two weeks to write, and handled an entire class of equivalences corresponding to a real use case.
Manual configuration was possible for an interesting use case, but remained challenging.
\item \toolname had good enough workflow integration to support real users.
The tactic decompiler was far from perfect, but showed promising early results with clear paths to improvements.
Some user workflows were unanticipated and informed changes in the design of \toolname.
\end{enumerate}
% TODO is it a contribution in itself to have real observation of users in a 3P proof refactoring/repair tool?

%\begin{enumerate} TODO
%\item LOC for each equiv/structure
%\item Choosing the swap equivalence
%\item Fun eliminators
%\end{enumerate}

\subsection{Industrial Use}
\label{sec:industry}

An industrial proof engineer at \company\footnote{Name withheld for double-blind review.} has been using \toolname in proving
correct an implementation of the TLS handshake protocol.
While this is ongoing work, thus far,
\toolname has helped \company integrate Coq with their existing verification workflow.

Before contact, \company had been using a custom solver-aided verification language to prove correct C programs.
They had found that at times, those constraint solvers got stuck, and they could not
progress on proofs about those programs.
They had built a compiler that translates their solver-aided language into Coq's specification language Gallina,
that way their proof engineers could finish stuck proofs interactively using Coq.
However, they had found that the generated Gallina programs and specifications were sometimes too difficult to work with.

A proof engineer at \company has used \toolname to work with those automatically generated programs and specifications
with the following workflow:

\begin{enumerate}
\item First, the proof engineer uses \toolname to update the automatically generated programs and specifications into more
human-readable programs and specifications.
\item Next, the proof engineer writes Coq proofs about the more human-readable programs and specifications.
\item Finally, the proof engineer uses \toolname again to update those proofs about human-readable programs and specifications back to
proofs about the original automatically generated programs and specifications.
\end{enumerate}
This workflow has allowed for industrial integration with Coq and has helped the proof engineer write functions and proofs
that would have otherwise been difficult.

% TODO will add better proofs here once Val sends them

\subsubsection{Configuration}

\begin{figure}
\begin{minipage}{0.25\textwidth}
   \lstinputlisting[firstline=1, lastline=4]{records.tex}
\end{minipage}
\hfill
\begin{minipage}{0.74\textwidth}
   \lstinputlisting[firstline=6, lastline=9]{records.tex}
\end{minipage}
\caption{Two unnamed tuples (left) and corresponding named records (right).}
\label{fig:records}
\end{figure}

Minimal examples corresponding to the proofs that the proof engineer used \toolname for
can be found in \lstinline{minimal_records.v}.
The proof engineer used \toolname to port anonymous tuples produced by \company's compiler
to named records, as shown in the example in Figure~\ref{fig:records}.

We implemented a search procedure for the proof engineer to automatically configure the proof term transformation to an equivalence
between nested tuples and named records.
The search procedure triggered automatically when the proof engineer called the \lstinline{Repair} command.
%It set \A to be the record type and \B to be the tuple.
The configuration it found for the record type was straightforward: identity for \lstinline{Eta},
the record constructor for \lstinline{DepConstr}, the record eliminator for \lstinline{DepElim}, and reflexive \lstinline{Iota}.
For the tuple, it set \lstinline{Eta} to expand and project, in our example:
\begin{lstlisting}
(@\codeauto{eta}@) (H : input) : input := (fst H, (fst (snd H), snd (snd H))).
\end{lstlisting}
it set \lstinline{DepConstr} to recursively apply the pair constructor:

\begin{lstlisting}
(@\codeauto{dep_constr_0}@) (firstBool : bool) (numberI : nat) (secondBool : bool) : input :=
  (firstBool, (numberI, secondBool)).
\end{lstlisting}
and it set \lstinline{DepElim} to recursively eliminate over the pair:

\begin{lstlisting}
(@\codeauto{dep_elim}@) (P: input $\rightarrow$ Type) (f: $\forall$ f n s, P(dep_constr_0 f n s)) (i: input): P(eta i) :=
  prod_rect
    (fun (p : bool * (nat * bool)) => P (eta p))
    (fun (a : bool) (b : nat * bool) => f a (fst b) (snd b))
    i.
\end{lstlisting}
This induced an equivalence between the nested tuple and record,
which \toolname generated and proved automatically.

\subsubsection{Example}
Using this configuration, the proof engineer automatically ported this compiler-generated function:

\begin{lstlisting}
op (r : bool * (nat * bool)) : nat * bool :=
  (fst (snd r), andb (firstBool r) (secondBool r)).
\end{lstlisting}
to this function:

\begin{lstlisting}
(@\codeauto{op}@) (r : (@\texttt{Record}@).input) : (@\texttt{Record}@).output :=
  {|
     numberO := numberI r;
     andBools := andb (fistBool r) (secondBool r)
  |}.
\end{lstlisting}
The proof engineer then wrote this proof about the new specification: % TODO did val write this or did I?

\begin{lstlisting}
Theorem and_spec_true_true : $\forall$ (r : (@\texttt{Record}@).input),
  firstBool r = true $\rightarrow$ secondBool r = true $\rightarrow$ andBools ((@\texttt{Record}@).op r) = true.
Proof.
  destruct r as [f n s].
  unfold (@\texttt{Record}@).op.
  simpl in *.
  apply andb_true_intro.
  intuition.
Qed.
\end{lstlisting}
and then used \toolname to automatically port this proof back to a proof of the original function
(with a bit of massaging to clean up the tactic output): % TODO induction & preprocess though

\begin{lstlisting}
Theorem (@\codeauto{and_spec_true_true}@) : $\forall$ (r : input),
  fst r = true $\rightarrow$ snd (snd r) = true $\rightarrow$ andb (op r) = true.
Proof.
  destruct r as [a b].
  apply andb_true_intro.
  intuition.
Qed.
\end{lstlisting}

\subsubsection{Lessons}

The proof engineer used \toolname most commonly to update tuples to records,
but also to help write dependently typed functions.
In total, the proof engineer used at least three automatic configurations and no manual configurations.
Two of these automatic configurations existed already, while the one for tuples and records was
added in response to the proof engineers' needs.
This search procedure took about two weeks for us to implement using techniques from
the repair and reuse tools \textsc{Pumpkin Patch} and \textsc{Devoid}.
Flexibility could be further improved by exposing an interface to users to allow them to
write search procedures themselves.

The proof engineer was able to use \toolname to integrate Coq into an existing proof engineering
workflow using solver-aided tools at \company.
The workflow for using \toolname itself, however, was a bit nonstandard,
and there was little need for tactic proofs about the compiler-generated functions and specifications.
In the initial days, we worked closely with the proof engineer;
later, the proof engineer worked independently and reached out occasionally by email.
\toolname was usable enough for this to work, but we found two challenges with workflow integration:
the proof engineer sometimes could not distinguish between user errors and bugs in our code,
and the proof engineer typically waited only about 10 seconds at most for \toolname to return.
Both of these observations informed improvements to \toolname, like better error messages, caching of transformed subterms,
and the ability to set terms as opaque to \toolname.
% TODO how long did compiling the file take?

\subsection{Vectors from Lists}
\label{sec:dep}

The proof term transformation in \toolname is based on the proof term transformation from
the \textsc{Devoid} proof reuse tool.
\textsc{Devoid} is a proof reuse tool for \textit{algebraic ornaments}, which describe relations
between two inductive types, where one type is exactly the other type indexed by a fold~\cite{mcbride}.
The running example of this in the \textsc{Devoid} paper is the relation between a list and a
length-indexed vector, like we saw in Figure~\ref{fig:listtovect} in Section~\ref{sec:key1}.

We configured the generalized algorithm in \toolname to support algebraic ornaments like those found in \textsc{Devoid},
and passed all of the regression tests from \textsc{Devoid}.
%We found that this simplified the algorithm from \textsc{Devoid}.
In addition, we added one more configuration to automate effort that had been manual in \textsc{Devoid}.
Several proof engineers including the industrial proof engineer, a Reddit user,
and someone on the \lstinline{coq-club} message board contacted us expressing interest in using this functionality.
%, though we do not yet know if the latter two ended up using \toolname. % TODO honestly ask

\subsubsection{Configuration}

We used two configurations to ease development with dependent types using algebraic ornaments.
The first fits the proof term transformation from \textsc{Devoid} into the \toolname framework:
it ports functions and proofs from the input type to the ornamented type at \textit{some} index,
like $\Sigma$\lstinline{(n : nat).vector T n}.
The second configuration goes beyond the functionality from \textsc{Devoid}, and provides
the missing link to get proofs aat a \textit{particular} index, like \lstinline{vector T n}.
\textsc{Devoid} had left this step to the user.

The search procedure for the first configuration is based heavily on the search procedure from \textsc{Devoid}.
In the case of lists and vectors, it used the equivalence:

\begin{lstlisting}
list T $\simeq$ $\Sigma$(n : nat).vector T n
\end{lstlisting}
The configuration for \lstinline{list} was simple: identity for \lstinline{Eta},
the list constructors for \lstinline{DepConstr}, the list eliminator for \lstinline{DepElim},
and trivial \lstinline{Iota}.
For \lstinline{vector}, it set \lstinline{Eta} to expand the input:

\begin{lstlisting}
(@\codeauto{eta}@) (T : Type) (s : $\Sigma$(n : nat).vector T n) : $\Sigma$(n : nat).vector T n := $\exists$ ($\pi_l$ s) ($\pi_r$ s).
\end{lstlisting}
it set \lstinline{DepConstr} to pack constructors: % TODO shrink this now that some of it is in the other section

\begin{lstlisting}
(@\codeauto{dep_constr_0}@) (T : Type) : $\Sigma$(n : nat).vector T n :=
  $\exists$ 0 (Vector.nil A).
(@\codeauto{dep_constr_1}@) (T : Type) (t : T) (s : $\Sigma$(n : nat).vector T n) : $\Sigma$(n : nat).vector T n :=
  $\exists$ (S ($\pi_l$ s)) (Vector.cons ($\pi_l$ s) t ($\pi_r$ s)).
\end{lstlisting}
and it set \lstinline{DepElim} to eliminate its projections:

\begin{lstlisting}
(@\codeauto{dep_elim}@) (T : Type) (P : $\Sigma$(n : nat).vector T n $\rightarrow$ Type) (pnil : P (dep_constr_0 T))
    (pcons : $\forall$ t s, P (eta T s) $\rightarrow$ P (dep_constr_1 T t s)) (s : $\Sigma$(n : nat).vector T n)
    : P (eta T s) :=
  vector_rect T
    (fun (n : nat) (v : vector T n) => P ($\exists$ n v))
    pnil
    (fun (t : T) (n : nat) (v : vector T n) => pcons t ($\exists$ n v))
    ($\pi_l$ s) ($\pi_r$ s).
\end{lstlisting}
This configuration was enough to capture all of the search and lifting functionality from \textsc{Devoid}. %for both search and lifting,
%save for the optional proof that these induce an equivalence, which \toolname borrows from \textsc{Devoid}.

To get from lists to vectors \textit{at a particular length}, we implemented one additional configuration.
This configuration corresponds to the equivalence between sigma types at a particular projection
and the same type escaping the sigma type, in our example:

\begin{lstlisting}
$\Sigma$(s : $\Sigma$(m : nat).vector T m).$\pi_l$ s = n $\simeq$ vector T n
\end{lstlisting}
By composition with the initial equivalence, this lets us transport proofs
across the equivalence we eventually want:

\begin{lstlisting}
$\Sigma$(l : list T).length l = n $\simeq$ vector T n
\end{lstlisting}
since these equivalences are equal up to transport along the first equivalence.

The second configuration carries equality proofs over the indices.
For example, it views \lstinline{vector T n} as implicitly representing $\Sigma$\lstinline{(v : vector T m).n = m} for some \lstinline{m}.
This is seen in \lstinline{eta}, here: 
% TODO both should take the same number of arguments, even if eta_A takes fewer. also does this belong in iota?

\begin{lstlisting}
(@\codeauto{eta}@) (T : Type) (n m : nat) (H : n = m) (v : vector T m) : vector T n :=
  eq_rect m (vector T) v n H.
\end{lstlisting}
which is the identity function generalized over any equal index.
Since there are no changes in inductive types, \lstinline{DepElim} and \lstinline{DepConstr} are trivial,
and \lstinline{Iota} does not change.

%\begin{lstlisting}
%(@\codeauto{dep_elim}@) (T : Type) (P : $\forall$ (n : nat), vector T n $\rightarrow$ Type) (p : $\forall$ n v, P n (eta T n n v)) (v : vector T n) : %P (eta T n n v) :=
%  p.
%\end{lstlisting}

\subsubsection{Example}

The expanded example from the \textsc{Devoid} paper is in \lstinline{Example.v}.
The \textsc{Devoid} example ported a list \lstinline{zip} function,
a \lstinline{zip_with} function, and a proof \lstinline{zip_with_is_zip} relating the two
functions from lists to vectors of some length.
It then manually ported those proofs to proofs over vectors at a particular length.
The updated \toolname example automates this last step.

The workflow for this was a bit different than it was with \textsc{Devoid}.
First, we used a custom eliminator \toolname generated to combine the list functions
with the length invariants from \textsc{Devoid}, and to combine the list proofs
with the proofs about those length invariants from \textsc{Devoid}.
This gave us a proof of this lemma:

\begin{lstlisting}
Lemma zip_with_is_zip : $\forall$ A B n
  (v1: $\Sigma$(l1: (@\codediff{list A}@)).(@\codediff{length}@) l1 = n) (v2: $\Sigma$(l2: (@\codediff{list B}@)).(@\codediff{length}@) l2 = n),
    zip_with pair n v1 v2 = zip n v1 v2.
\end{lstlisting}
with functions \lstinline{zip_with} and \lstinline{zip} operating over lists at given lengths.
We then ran \lstinline{Repair module} using the first
configuration, which proved this lemma:

\begin{lstlisting}
Lemma (@\codeauto{zip_with_is_zip}@) : $\forall$ A B n
  (v1:$\Sigma$(l1:(@\codediff{$\Sigma$(m: nat).vector A m}@)).(@\codediff{$\pi_l$}@) l1 = n) (v2:$\Sigma$(l2:(@\codediff{$\Sigma$(m: nat).vector B m}@)).(@\codediff{$\pi_l$}@) l2 = n),
    zip_with pair v1 v2 = zip v1 v2.
\end{lstlisting}
with functions \lstinline{zip_with} and \lstinline{zip} updated as well.
We composed this with \lstinline{Repair module} on the second configuration,
which proved this lemma:

\begin{lstlisting}
Lemma (@\codeauto{zip_with_is_zip}@) : $\forall$ A B n
  (v1: (@\codediff{vector A n}@)) (v2: (@\codediff{vector B n}@)),
    zip_with pair v1 v2 = zip v1 v2.
\end{lstlisting}
with functions \lstinline{zip_with} and \lstinline{zip} operating directly over vectors.

\subsubsection{Lessons}

%\paragraph{Configuration \& Flexibility}
Implementing the configuration from \textsc{Devoid} in this framework was straightforward and
took a couple of days.
It also simplified the transformation from \textsc{Devoid} %by removing the need for special rules for handling eliminators and constructors
%by adding and removing arguments, and for special handling of non-primitive projections,
%as that work is done in the configuration.
significantly by moving all of the interesting into the search procedure for the configuration.
Implementing the second configuration was less straightforward,
especially when it came to understanding the behaviors of equalities.
There are still some open challenges in correctly supporting \lstinline{Iota} to port any arbitrary
proof backward along the second configuration in the opposite direction.

%\paragraph{Workflow Integration}
The result was a simplifed workflow from \textsc{Devoid} and automated steps 
that had previously been manual.
%\toolname implemented some additional automation to generate eliminators that help separate out this additional information
%for repair. 
The tactic decompiler successfully generated tactic scripts to write dependently typed
functions that carry proofs about length invariants,
but struggled at generating tactic scripts for proofs relating those invariants due to poor motive inference with the \lstinline{induction} tactic.
This is to some degree unsurprising, since these proofs
were very difficult for us to write by hand.
Still, additional effort is needed to improve tactic integration with dependent types.
% TODO how long did compiling the file take?

\subsection{\textsc{Replica} Benchmark}
\label{sec:replica}

\begin{figure}
\begin{minipage}{0.48\textwidth}
   \lstinputlisting[firstline=1, lastline=8]{replica.tex}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
   \lstinputlisting[firstline=10, lastline=17]{replica.tex}
\end{minipage}
\caption{A simple language (left) and the same language with two swapped constructors (right).}
\label{fig:replica}
\end{figure}

The constructor swapping example from Section~\ref{sec:overview} was inspired by two benchmarks 
from the \textsc{Replica} user study of proof engineers~\cite{replica}.
An isolated change corresponding to part of the benchmark is shown in Figure~\ref{fig:replica}.
The proof engineer had a simple language represented by an inductive type \lstinline{Term},
as well as some definitions and proofs about the language.
The proof engineer swapped two constructors in the term language.

We were able to use \toolname to automatically configure the program transformation to move this constructor,
then transform all of the functions and proofs about the language.
We also succeeded at more difficult variants of this,
like swapping two constructors with the same type, or renaming all of the constructors,
or swapping and renaming at the same time.
In all cases, with just a bit of human guidance, \toolname was able to repair the functions and proofs.

\subsubsection{Configuration}

The configuration this used handles swapping and renaming constructors of inductive types.
This is one of the simplest configurations.
The only nontrivial part is that \lstinline{DepConstr} over the updated type swaps back the constructors, in our example:

\begin{lstlisting}
(@\codeauto{dep_constr_1}@) (H : Z) : Term := Int H.
(@\codeauto{dep_constr_2}@) (H H0 : Term) : Term := Eq H H0.
\end{lstlisting}
so that they align with the constructors of the original type.
The eliminator similarly swaps cases:

\begin{lstlisting}
(@\codeauto{dep_elim}@) P f0 f1 f2 f3 f4 f5 f6 t : P t := Term_rect P f0 (@\codediff{f2}@) (@\codediff{f1}@) f3 f4 f5 f6 t.
\end{lstlisting}
where \lstinline{Term_rect} is the eliminator over the old version of \lstinline{Term}.

%The search procedure for this configuration is flexible.
%It can handle swapping and renaming of constructors at the same time,
%and it can support multiple swapped constructors at the same time.
%When there are many possible mappings between the constructors of the old
%and new version, \toolname prompts the proof engineer to choose a configuration from a ranked list.
%It allows the user to provide a custom mapping between constructors if the desired mapping
%does not show up high enough on the ranked list.
%In all cases, it proves the equivalence from the swap map automatically.
%The transformation then updates functions, swapping and renaming constructors and cases.

\subsubsection{Example}

We used \toolname to automatically update the functions and proofs in \lstinline{Swap.v} from the \textsc{Replica} benchmark.
This included functions about \lstinline{Term}, as well as a large record \lstinline{EpsilonLogic} that encoded the semantics of the language,
plus a proof about that record:

\begin{lstlisting}
Theorem eval_eq_true_or_false :
  $\forall$ (L : EpsilonLogic) env (t1 t2 : Term),
    L.eval env (Eq t1 t2) = L.vTrue $\vee$ L.eval env (Eq t1 t2) = L.vFalse.
\end{lstlisting}
\toolname automatically updated all of these. For the inductive proof, it produced
a new inductive proof that used the induction principle with the swapped constructors.
It also discovered all other 23 type-correct permutations of constructors, all available for selection to 
prove the appropriate equivalence and use to update functions and proofs.
It presented the desired transformation as the first option in the list, so that all we had to do
was pass the argument \lstinline{mapping 0} to \lstinline{Repair module} for it to handle this change.
It was also able to handle more advance changes like renaming constructors, swapping multiple constructors at once,
and doing both at the same time.

\subsubsection{Lessons}

Implementing the search procedure was simple and took about three days total.
The biggest challenge was implementing an interactive interface to choose between mappings when there are multiple possible mappings,
or to allow the user to write a single custom mapping function and derive the rest of the configuration from there.
This was only a single snapshot of the benchmark, which included
other changes like adding constructors to inductive types.
One ongoing challenge is defining and discovering useful configurations to support adding new constructors.

The isolated change was simple enough that \toolname would not have been necessary
for updating the proofs for the initial change---keeping the tactics the same would have also worked.
%This is a property of the particular proofs that we had access to;
As we saw in Section~\ref{sec:overview}, even for simple changes, this is not always true.
More advanced variants of this benchmark that involved also renaming constructors did necessitate \toolname,
and \toolname worked well for those. % TODO what happens with the tactic decompiler for these?
The entire \lstinline{Swap.v} file, which includes swapping constructors of every function in the \lstinline{list} module and
its dependencies, four variants of the \textsc{Replica} benchmark,
and testing a large and ambiguous swap of a type \lstinline{Enum} with 30 constructors of identical types,
took \toolname less than 90 seconds total. % TODO specs, reproduction
Each variant of the \textsc{Replica} benchmark took \toolname less than 5 seconds. % TODO specs, reproduction

\subsection{Unary and Binary Numbers}
\label{sec:bin}

All of the case studies so far have dealt with pairs of types with the same inductive structure.
Some of the oldest problems in the transport literature deal with \textit{changing} inductive
structure.
We have realized a classic example~\cite{magaud2000changing}  of this with \toolname using a manual configuration:
updating unary to binary natural numbers (Section~\ref{sec:key2}, Figure~\ref{fig:nattobin}).

In Coq, a binary number \lstinline{N} is either zero or a positive binary number. A positive binary number
is either 1 (\lstinline{xH}), the result of shifting left and adding 1 (\lstinline{xI}),
or the result of shifting left and not adding anything (\lstinline{xO}).
This allows for a fast addition function, also found in the Coq standard library.
In the style of \citet{magaud2000changing}, we use \toolname to derive a slow binary
addition function that does not refer to \lstinline{nat}.
From that, we are able to port our proofs over unary addition to binary addition,
removing all references to \lstinline{nat}, and show that they hold over fast binary addition too.

\subsubsection{Configuration}
We supplied this configuration manually using the \lstinline{Configure} command,
which takes the configuration parts as arguments.
The result is in \lstinline{nonorn.v}.
The configuration for \lstinline{nat} was straightforward.
For \lstinline{N}, we used functions from the Coq standard library that
behaved like the \lstinline{nat} constructors:

\begin{lstlisting}
dep_constr_0 : N := 0%N.
dep_constr_1 : N $\rightarrow$ N := N.succ.
\end{lstlisting}
Similarly, the Coq standard library included precisely the eliminator we wanted, one that behaves
like the eliminator over unary natural numbers:

\begin{lstlisting}
dep_elim (P: N $\rightarrow$ Type) (pO: P dep_constr_0) (pS: $\forall$n, P n $\rightarrow$ P (dep_constr_1 n)) (n: N) 
    : P n :=
  N.peano_rect P pO pS n.
\end{lstlisting}
\lstinline{Iota} was almost written for us.
The standard library had a lemma that reduced the successor case:

\begin{lstlisting}
N.peano_rect_succ :
  $\forall$ P pO pS n, N.peano_rect P pO pS (N.succ n) = pS n (N.peano_rect P pO pS n).
\end{lstlisting}
\lstinline{Iota} over the successor case was a simple rewrite by this lemma:

\begin{lstlisting}
Lemma iota_1 :
  $\forall$ P pO pS n (Q : P (dep_constr_1 n) $\rightarrow$ Type),
     Q (pS n (dep_elim P pO pS n)) $\rightarrow$
     Q (dep_elim P pO pS (dep_constr_1 n)).
Proof.
  intros. unfold dep_elim, dep_constr_1. rewrite N.peano_rect_succ. auto.
Defined.
\end{lstlisting}

The need for a nontrivial \lstinline{Iota} comes from the fact that \lstinline{N} has a different
inductive structure from \lstinline{nat}.
The need for it is noted as far back as \citet{magaud2000changing}.
This corresponds to a broader pattern---it captures the essence of the change in inductive structure.
\toolname's configurable proof term transformation captures that intuition.

\subsubsection{Example}

We ported unary addition from \lstinline{nat} to \lstinline{N} fully automatically:

\begin{lstlisting}
Repair nat N in add as (@\codeauto{slow_add}@).
\end{lstlisting}
The result (tellingly named) has the same slow behavior as the \lstinline{add} function over \lstinline{nat}.
However, it no longer refers to \lstinline{nat} in any way.
Like \citet{magaud2000changing}, we found it easy to manually prove that
this has the same behavior as fast binary addition:

\begin{lstlisting}
Lemma add_fast_add:
  $\forall$ (n m : Bin.nat), slow_add n m = N.add n m.
Proof.
  induction n using N.peano_rect; intros m; auto. unfold slow_add.
  rewrite N.peano_rect_succ. (* $\leftarrow$ iota_1 *)
  unfold slow_add in IHn. rewrite IHn.
  rewrite N.add_succ_l. reflexivity.
Qed.
\end{lstlisting}

We then used \toolname again to transform a proof:
\begin{lstlisting}
add_n_Sm : $\forall$ (n m : nat), S (add n m) = add n (S m).
\end{lstlisting}
from \lstinline{add} to \lstinline{slow_add}:

\begin{lstlisting}
(@\codeauto{slow_add_n_Sm}@) : $\forall$ (n m : N), N.succ (slow_add n m) = slow_add n (N.succ m).
\end{lstlisting}
This was not quite as push-button.
It involved a manual expansion step, turning implicit casts in the inductive case
into explicit applications of \lstinline{Iota} over \A.
These applications were formulaic, but tricky to write.
Once we had that, though, we could run the same \lstinline{Repair} command
to get \lstinline{slow_add_n_Sm}.
Showing that the same theorem held over fast binary addition was then
straightforward:

\begin{lstlisting}
Lemma add_n_Sm :
  $\forall$ n m, Bin.succ (N.add n m) = N.add n (Bin.succ m).
Proof.
  intros. repeat rewrite $\leftarrow$ add_fast_add. apply slow_plus_n_Sm.
Qed.
\end{lstlisting}

\subsubsection{Lessons}

\lstinline{Iota} was the key to supporting this case.
It was enough to implement this transformation that had previously been its own tool
just by writing a configuration with \lstinline{Iota}. 

The entire file took under a second for us to compile using \toolname.
We did not need tactic integration, since we
ultimately wanted compatibility with a faster version of the function.
We did note that supporting custom eliminators like \lstinline{N.peano_rect} would be a simple way
to improve the tactic decompiler.
The most difficult part was manually expanding proofs about \lstinline{nat}
to explicitly apply \lstinline{Iota}.
This is because there is not yet any way for manual configuration to supply custom matching functions,
and unification was not enough.
We discuss some ideas for this in Section~\ref{sec:discussion}. % TODO actually dos

% TODO what does the tactic decompiler do for this? It's broken. Why?

%\subsubsection{Algebraic}

%It is straightforward to fit the search algorithm from DEVOID into this framework, and in fact
%we can loosen the restriction that the language has primitive projections.
%Let $A$ be $A$ from DEVOID, let $B_{ind}$ be $B$ from DEVOID, let $I_B$ be $I_B$ from DEVOID,
%and let \lstinline{index} be \lstinline{index} from DEVOID.
%Let $B$ wrap $B_{ind}$ packed into a sigma type:

%\begin{lstlisting}
%B := $\lambda$ ($\vec{t}$ : $\vec{T}$) . ($\Sigma$ (i : I$_B$ $\vec{t}$) . B$_{ind}$ (index i $\vec{t}$))
%\end{lstlisting}
%Let $\vec{T_{B_j}}$ be the arguments of constructor type $C_{B_j}$ (type of constructor of $B_{\mathrm{ind}}$).
%Define \lstinline{DepConstr(j, B)} recursively using the following derivation (based on and same fall-through convention as the DEVOID paper %for now,
%and I'd prefer to move this away from a derivation but not sure how to do so and maintain formality): % TODO check

%\begin{mathpar}
%\mprset{flushleft}
%\small
%\hfill\fbox{$\Gamma$ $\vdash$ $(T_A, T_B)$ $\Downarrow_{C}$ $t$}\\%

%\inferrule[Dep-Constr-Conclusion]
%  { \Gamma \vdash \vec{t_{B_j}} : \vec{T_{B_j}} \\ \Gamma \vdash Constr(j, B)\ \vec{t_{B_j}} : B_{\mathrm{ind}} \vec{i_B}  }
%  { \Gamma \vdash (A\ \vec{i_A},\ B_{\mathrm{ind}}\ \vec{i_B}) \Downarrow_{p_{c}} \exists\ (\vec{i_B}[\mathrm{off}\ A\ B]) (Constr(j, B)\ \vec{t_{B_j}}) }

%\inferrule[Dep-Constr-Index] % new hypothesis for index
%  { \mathrm{new}\ n_B\ b_B \\ \Gamma,\ n_B : t_B \vdash (\Pi (n_A : t_A) . b_A,\ b_B) \Downarrow_{i_{c}} t }
%  {  \Gamma \vdash (\Pi (n_A : t_A) . b_A,\ \Pi (n_B : t_B) . b_B) \Downarrow_{C} t}

%\inferrule[Dep-Constr-IH] % inductive hypothesis
%  { \Gamma,\ n_B : B\ \vec{i_B} \vdash (b_A [n_B / n_A], b_B [\pi_l\ n_B / \vec{i_B}[\mathrm{off}\ A\ B]]) \Downarrow_{C} t }
%  { \Gamma \vdash (\Pi (n_A : A\ \vec{i_A}) . b_A, \Pi (n_B : B\ \vec{i_B}) \Downarrow_{C} \lambda (n_B : B\ \vec{i_B}) . t }

%\inferrule[Dep-Constr-Prod] % otherwise, unchanged (when we get rid of the gross fall-through thing, needs not new, and needs to check t_A and t_B not IHs)
%  { \Gamma,\ n_B : t_B \vdash (b_A [n_B / n_A], b_B) \Downarrow_{C} t }
%  { \Gamma \vdash (\Pi (n_A : t_A) . b_A, \Pi (n_B, t_B) . b_B) \Downarrow_{C} \lambda (n_B : t_B) . t }\\

%\inferrule[Dep-Constr]
%{ \Gamma \vdash Constr(j, A) : C_{A_j} \\ \Gamma \vdash (C_{A_j}, C_{B_j}) \Downarrow_{C} t }
%{ \Gamma \vdash (Constr(j, A), Constr(j, B_{\mathrm{ind}}) \Downarrow_{C} t }
%\end{mathpar}
%and \lstinline{DepElim(b, p)} similarly:

%\begin{mathpar}
%TODO
%\end{mathpar}

%Then:

%\begin{lstlisting}
%DepConstr(j, A) : C$_{A_{j}}$ := Constr(j, A)
%DepConstr(j, B) : C$_{A_{j}}$[B / A] := DepConstr(j, B)

%DepElim(a, p){f$_{1}$, $\ldots$, f$_{n}$} : p a := Elim(a, p){f$_{1}$, $\ldots$, f$_{n}$}
%DepElim(b, p){f$_{1}$, $\ldots$, f$_{n}$} : p b := DepElim(b, p)

%Eta(A) := $\lambda$(a : A).a
%Eta(B) := $\lambda$(b : B).$\exists$ ($\pi_l$ b) ($\pi_r$ b)
%\end{lstlisting}

% TODO investigate below projection thing, and write in when you finish
%For now assume we have some \lstinline{pack} function to pack into an existential;
%this is just for convenience.
%The indexer is just the first projection of this lifted across the eliminator rule, AFAIK---note this isn't exactly $\Pi_{l}$ like we use
%in the tool, but is really an eliminated $\Pi_{l}$? I will need to check on this, it's the only weird part.
%Also assume some \lstinline{index_args} function to add the new index to the appropriate arguments---I'll
%elaborate on this later but it's also something search needs to find and it's determined in terms of the \lstinline{indexer} that search finds.
%Also now, we no longer assume primitive projections.

%\subsubsection{Unpack sigma}

%This one is kind of weird but it gets us user-friendly types. I'll explain later.

%\begin{lstlisting}
%DepConstr(j, A) := (* TODO pack into existential, deal with equality *)
%DepConstr(j, B) : C$_{B_{j}}$ := Constr(j, B)

%DepElim(a, p){f$_{1}$, $\ldots$, f$_{n}$} : p a := (* TODO *)
%DepElim(b, p){f$_{1}$, $\ldots$, f$_{n}$} : p b := Elim(b, p){f$_{1}$, $\ldots$, f$_{n}$}

%Eta(A) := $\lambda$(a : A).$\exists$ ($\exists$ ($\pi_l$ ($\pi_l$ a)) ($\pi_r$ ($\pi_l$ a))) ($\pi_r$ a)
%Eta(B) := $\lambda$(b : B).b
%\end{lstlisting}

%\subsubsection{Records and tuples}

%This one should be easier. We'll play a similar trick with $B$ and $B_{ind}$ like we do for algebraic,
%and give things similar names.
%Then:

%\begin{lstlisting}
%DepConstr(j, A) : C$_{A_{j}}$ := Constr(j, A)
%DepConstr(j, B) : C$_{A_{j}}$[B / A] := $\lambda$ ($\vec{t_{A_j}}$ : $\vec{T_{A_j}}$) . (* TODO recursively pack into pair *)

%DepElim(a, p){f$_{1}$, $\ldots$, f$_{n}$} : p a := Elim(a, p){f$_{1}$, $\ldots$, f$_{n}$}
%DepElim(b, p){f$_{1}$, $\ldots$, f$_{n}$} : p b := (* TODO recursively eliminate product *)

%Eta(A) := $\lambda$(a : A).a
%Eta(B) := (* TODO recursive eta *)
%\end{lstlisting}
