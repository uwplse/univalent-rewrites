\section{Case Studies \& Configurations}
\label{sec:search}

As we saw in Figure~\ref{fig:listswap} in Section~\ref{sec:overview},
there are two ways to use \toolname:

\begin{enumerate}
\item with automatic configuration, using search procedures to discover configurations for entire classes of equivalences, and
\item with manual configuration, providing the configuration directly.
\end{enumerate}
These two modes of configuration have made it possible for \toolname to support a wide
varity of applications.
This section explans four different case studies in using \toolname:

\begin{enumerate}
\item Refactoring automatically generated proofs for an industrial user (automatic, Section~\ref{sec:industry})
\item Generating dependently typed vector functions and proofs from functions and proofs about lists and their lengths (automatic, Section~\ref{sec:dep})
\item Supporting a benchmark from a user study of Coq proof engineers (automatic, Section~\ref{sec:replica})
\item Porting functions and proofs about unary numbers to functions and proofs about binary numbers (manual, Section~\ref{sec:bin})
\end{enumerate}
For each case study, we explain the configuration used, walk through an example, and describe takeaways.
In all, we found the following:

\begin{enumerate}
\item \toolname handled examples of both proof refactoring and repair. In practice, the line between these was sometimes
blurred, as was the line with proof reuse.
\item \toolname was configurable to different classes of changes. Each search procedure for automatic configuration
took between two days and two weeks to write, and handled an entire class of equivalences corresponding to a real use case.
Manual configuration was possible for an interesting use case, but remained challenging.
\item \toolname had good enough workflow integration to support real users.
The tactic decompiler was far from perfect, but showed promising early results with clear paths to improvements.
Some user workflows were unanticipated and informed changes in the design of \toolname.
\end{enumerate}
% TODO is it a contribution in itself to have real observation of users in a 3P proof refactoring/repair tool?

%\begin{enumerate} TODO
%\item LOC for each equiv/structure
%\item Choosing the swap equivalence
%\item Fun eliminators
%\end{enumerate}

\subsection{Industrial Use}
\label{sec:industry}

An industrial user at \company\footnote{Name withheld for double-blind review.} has been using \toolname in proving
correct an implementation of the TLS handshake protocol.
While this is ongoing work, thus far,
\toolname has helped \company integrate Coq with their existing verification workflow.

Before contact, \company had been using a custom solver-aided verification language to prove correct C programs.
They had found that at times, those constraint solvers got stuck, and they could not
progress on proofs about those programs.
They had built a compiler that translates their solver-aided language into Coq's specification language Gallina,
that way their proof engineers could finish stuck proofs interactively using Coq.
However, they had found that the generated Gallina programs and specifications were sometimes too difficult to work with.

A proof engineer at \company has used \toolname to work with those automatically generated programs and specifications
with the following workflow:

\begin{enumerate}
\item First, the proof engineer uses \toolname to refactor the automatically generated programs and specifications into more
human-readable programs and specifications.
\item Next, the proof engineer writes Coq proofs about the more human-readable programs and specifications.
\item Finally, the proof engineer uses \toolname again to refactor those proofs about human-readable programs and specifications back to
proofs about the original automatically generated programs and specifications.
\end{enumerate}
This workflow has allowed for industrial integration with Coq and has helped the user write functions and proofs
that would have otherwise been difficult.

% TODO will add better proofs here once Val sends them

\paragraph{Configuration}

\begin{figure}
\begin{minipage}{0.25\textwidth}
   \lstinputlisting[firstline=1, lastline=4]{records.tex}
\end{minipage}
\hfill
\begin{minipage}{0.74\textwidth}
   \lstinputlisting[firstline=6, lastline=9]{records.tex}
\end{minipage}
\caption{Two unnamed tuples (left) and corresponding named records (right).}
\label{fig:records}
\end{figure}

Minimal examples corresponding to the proofs that the proof engineer used \toolname for
can be found in \lstinline{minimal_records.v}.
The proof engineer used \toolname to port anonymous tuples produced by \company's compiler
to named records, as shown in the example in Figure~\ref{fig:records}.

We implemented a search procedure for the proof engineer to automatically configure the proof term transformation to an equivalence
between nested tuples and named records.
The search procedure triggered automatically when the proof engineer called the \lstinline{Repair} or \lstinline{Repair module} command
with the tuple and record as arguments.
It set \A to be the record type and \B to be the tuple.
There were no \lstinline{RewEta} since there were no inductive hypotheses.
For the record \A, it set \lstinline{IdEta} to identity, \lstinline{DepConstr} to the single
constructor corresponding to the record constructor, and \lstinline{DepElim} to the standard record eliminator.
For the tuple \B, on the other hand, it expanded identity to deal with non-primitive projections,
in our example:
\begin{lstlisting}
Definition id_eta_b (H : Tuple.input) : Tuple.input := (fst H, (fst (snd H), snd (snd H))).
\end{lstlisting}
it constructed a nested tuple by recursively applying the pair constructor:

\begin{lstlisting}
Definition dep_constr_0_B (firstBool : bool) (numberI : nat) (secondBool : bool) : Tuple.input :=
  (firstBool, (numberI, secondBool)).
\end{lstlisting}
and it recursively eliminated over the pair:

\begin{lstlisting}
Definition dep_elim_B (P : Tuple.input -> Type) (f : $\forall$ firstBool numberI secondBool, P (dep_constr_0_B firstBool numberI secondBool)) (i : Tuple.input) :=
  prod_rect
    (fun p : bool * (nat * bool) => P (id_eta_b p))
    (fun (a : bool) (b : nat * bool) =>
      f a (fst b) (snd b))
    i.
\end{lstlisting}
This induced an equivalence between the nested tuple and record,
which \toolname generated and proved automatically.

\paragraph{Example}
Using this configuration, the proof engineer automatically ported this compiler-generated function:

\begin{lstlisting}
Definition op (r : bool * (nat * bool)) : nat * bool :=
  (fst (snd r), andb (firstBool r) (secondBool r)).
\end{lstlisting}
to this function:

\begin{lstlisting}
Definition op (r : Record.input) : Record.output :=
  {|
     numberO := numberI r;
     andBools := andb (fistBool r) (secondBool r)
  |}.
\end{lstlisting}
The proof engineer then wrote this proof about the new specification: % TODO did val write this or did I?

\begin{lstlisting}
Theorem and_spec_true_true :
  forall (r : Record.input),
    firstBool r = true ->
    secondBool r = true ->
    andBools (Record.op r) = true.
Proof.
  destruct r as [f n s].
  unfold Record.op.
  simpl in *.
  apply andb_true_intro.
  intuition.
Qed.
\end{lstlisting}
and then used \toolname to automatically port this proof back to a proof of the original function: % TODO induction & preprocess though

\begin{lstlisting}
Theorem and_spec_true_true :
  forall (r : Tuple.input),
    fst r = true ->
    snd (snd r) = true ->
    andb (Tuple.op r) = true.
\end{lstlisting}
The proof engineer had no need for the decompiler here because the proof engineer modified
the proof over records directly. % TODO it is being silly; can we get it working?

\paragraph{Takeaways}

\textbf{Refactoring \& Repair}:
The proof engineer used \toolname for both refactoring and repair.
Refactorings like the changes from tuples to records were the most common use cases.
The proof engineer has also used \toolname to repair some non-dependently-typed
functions and proofs to instead use dependent types.

\textbf{Configuration \& Flexibility}:
The proof engineer used at least three automatic configurations and no manual configurations.
One of these configurations was backed by a search procedured that we implemented
specifically to support this user: the search procedure to
configure the equivalence between nested tuples and records.
This search procedure took about two weeks for us to implement using techniques from
the repair and reuse tools \textsc{Pumpkin Patch} and \textsc{Devoid}.
Flexibility could be further improved by exposing an interface to users to allow them to
write these search procedures themselve.

\textbf{Workflow Integration}:
The industrial proof engineer was able to use \toolname to integrate Coq with existing proof engineering
workflows using solver-aided tools at \company.
The workflow for using \toolname itself, however, was a bit nonstandard,
and there was little need for tactic proofs about the compiler-generated functions and specifications.
In the initial days, we worked closely with the proof engineer;
later, the proof engineer worked independently and reached out occasional by email.
\toolname was usable enough for this to work.
However, we found two challenges with workflow integration:
the proof engineer sometimes could not distinguish between user errors and bugs in our code,
and the proof engineer typically waited only about ten seconds at most for \toolname
to port a function or proof.
Both of these observations informed significant changes to \toolname, like better error messages
and caching of transformed subterms.

\subsection{Vectors from Lists and their Lengths}
\label{sec:dep}

TODO note reddit here

can view as repair bc started from list proofs

We were able to integrate the program transformation from DEVOID into our framework, plus extend
the main example from DEVOID with new automation missing in the original paper.
The DEVOID paper introduced a program transformation for transporting proofs across a particular class
of equivalences corresponding to \textit{algebraic ornaments}. Algebraic ornaments describe relations
between two inductive types, where one inductive type is exactly the old inductive type indexed by a fold
over the original type.
A simple of example of this is the relation between a list:

\begin{lstlisting}
Inductive list (T : Type) : Type :=
| nil : list T
| cons : T -> list T -> list T.
\end{lstlisting}
and a length-indexed vector:

\begin{lstlisting}
Inductive vector (T : Type) : nat -> Type :=
| nil : vector T O
| cons : T -> forall (n : nat), vector T n -> vector T (S n).
\end{lstlisting}

The running example in the DEVOID paper ported a list \lstinline{zip} function,
a \lstinline{zip_with} function, and a proof \lstinline{zip_with_is_zip} relating the two
functions from \lstinline{list T} to $\Sigma$\lstinline{(n : nat).vector T n} (vectors of
\textit{some} length).
However, while they noted that for any \textit{particular} length \lstinline{n}, \lstinline{vector T n} is equivalent to
\lstinline{l : list T & length l = n}, they relied on manual steps from the user to derive proofs about \lstinline{vector T n}
from proofs about lists and their lengths.

Using \toolname with two composed configurations, we were able to fit the original example into our framework,
and then extend it to automatically derive proofs about \lstinline{vector T n} from proofs about lists and their lengths.
The key was to combine proofs about lists and their lengths using a special eliminator \toolname generated.
Using this, we could prove this lemma:

\begin{lstlisting}
Lemma zip_with_is_zip :
  forall A B n (v1 : { l1 : list A & length l1 = n }) (v2 : { l2 : list B & length l2 = n }),
    zip_with pair v1 v2 = zip v1 v2.
\end{lstlisting}
by just applying the proof over \lstinline{list} from DEVOID, plus a very simple length invariant:

\begin{lstlisting}
Proof.
  intros A B n v1. 
  apply packed_list_rect with (P := fun (v1 : {l1 : list A & length l1 = n}) => forall v2 : {l2 : list B & length l2 = n}, zip_with pair v1 v2 = zip v1 v2).
  intros l H v2.
  unfold zip_with, zip, packed_list_rect, hs_to_coqV_p.list_to_t_rect, packed_rect. simpl.
  apply eq_existT_uncurried.
  (* list proof: *)
  exists (hs_to_coq.zip_with_is_zip l (projT1 v2)).
  (* length invariant: *)
  apply (Eqdep_dec.UIP_dec Nat.eq_dec).
Defined.
\end{lstlisting}
We then ran \lstinline{Repair Module} to transport those functions and proofs from \lstinline{list T}
to $\Sigma$\lstinline{(n : nat).vector T n}, which for our lemma gave us a term of this type:

\begin{lstlisting}
Lemma zip_with_is_zip :
  forall A B n (v1 : { l1 : sigT (vector T) & projT1 l1 = n }) (v2 : { l2 : sigT (vector T) & projT1 l2 = n }),
    zip_with pair v1 v2 = zip v1 v2.
\end{lstlisting}
We then composed this with \lstinline{Repair Module} on another configuration that ports proofs of
packed types with particular first projections to unpacked types indexed at the first projection.
This gave us a proof of this theorem:

\begin{lstlisting}
Lemma zip_with_is_zip :
  forall A B n (v1 : vector A n) (v2 : vector B n),
    zip_with pair v1 v2 = zip v1 v2.
\end{lstlisting}

Explanation of these configurations and example.

While this produced proof terms with the correct type, the decompiler struggled
to generate useful tactic scripts for dependently typed functions.
We discuss ideas for handling this in Section~\ref{sec:decompiler}.
% TODO actually do this

\subsection{REPLICA Benchmarks}
\label{sec:replica}

Section~\ref{sec:overview} showed an example of swapping constructors.
The configuration this used handles arbitrary swapping and renaming of constructors of inductive types.
In addition to using this for list proofs, we also used it to replay part of a change found in
a REPLICA benchmark.

In that change, the proof engineer had a simple language:

\begin{lstlisting}
Inductive Term : Set :=
  | Var : Identifier -> Term
  | Int : Z -> Term
  | Eq : Term -> Term -> Term
  | Plus : Term -> Term -> Term
  | Times : Term -> Term -> Term
  | Minus : Term -> Term -> Term
  | Choose : Identifier -> Term -> Term.
\end{lstlisting}
as well as some definitions and proofs about the language.
As part of the change, the user moved the \lstinline{Int} constructor down:

\begin{lstlisting}
Inductive Term : Set :=
  | Var : Identifier -> Term
  | Eq : Term -> Term -> Term
  | Int : Z -> Term
  | Plus : Term -> Term -> Term
  | Times : Term -> Term -> Term
  | Minus : Term -> Term -> Term
  | Choose : Identifier -> Term -> Term.
\end{lstlisting}

Using the search procedure for the swap configuration, we were able to use \toolname
to automatically configure the program transformation to move this constructor,
then transform all of the functions and proofs about the language.
We also went beyond the proof engineer's change and tried swapping two constructors with the same type,
or renaming all of the constructors.
In all cases, with a bit of human guidance, \toolname was able to repair the functions and proofs.

Note that, in the original benchmark, for just the first swap change,
the original tactics would have also worked for the proof. 
As we saw in Section~\ref{sec:overview}, this is not always true.
REPLICA also saw that users move cases in match statements to match moved constructors
in inductive types, and the transported terms \toolname generated did this automatically
(with induction principles though).

\paragraph{Configuration}
The configuration this used handles swapping and renaming constructors of inductive types.
When a user swaps or renames constructors, the new type is equivalent to the old type exactly.
The configuration just maps the old constructors to the swapped new constructors, and maps appropriate
cases of the old eliminator to appropriate cases of the new eliminator.
Equality does not change.

More formally, let $A$ and $B$ be inductive types:

\begin{lstlisting}
$A$ := $\mathrm{Ind} (\mathit{Ty}_A : \Pi (\vec{i_A} : \vec{\mathrm{X}_A}) . \mathrm{s}_A)\{\mathrm{C}_{A_1}, \ldots, \mathrm{C}_{A_n}\}$
$B$ := $\mathrm{Ind} (\mathit{Ty}_B : \Pi (\vec{i_B} : \vec{\mathrm{X}_B}) . \mathrm{s}_B)\{\mathrm{C}_{B_1}, \ldots, \mathrm{C}_{B_n}\}$
\end{lstlisting}		
Assume there is some invertible swap map $m$ such that for any index $j$,
\lstinline{C}$_{B_{m(j)}}$ is exactly \lstinline{C}$_{A_j}[B / A]$.
Then:

\begin{lstlisting}
DepConstr(j, A) : C$_{A_{j}}$ := Constr(j, A) 
DepConstr(j, B) : C$_{A_{j}}$[B / A] := Constr(m(j), B)

DepElim(a, p){f$_{1}$, $\ldots$, f$_{n}$} : p a := Elim(a, p){f$_{1}$, $\ldots$, f$_{n}$}
DepElim(b, p){f$_{1}$, $\ldots$, f$_{n}$} : p b := Elim(b, p){f$_{m(1)}$, $\ldots$, f$_{m(n)}$}

IdEta(A) := $\lambda$ ($\vec{t}$ : $\vec{T}$) (a : A $\vec{t}$).a
IdEta(B) := $\lambda$ ($\vec{t}$ : $\vec{T}$) (b : B $\vec{t}$).b

RewEta(A) := eq_refl A
RewEta(B) := eq_refl B
\end{lstlisting}

Example (honestly let's just show with particular type and explain more generally, and leave out formalities until later if we need them).

\subsection{Unary and Binary Numbers}
\label{sec:bin}

All of the configurations we have seen so far have not included \lstinline{Rew-Eta}.
This is because they are all ornaments---all changes preserve the same inductive structure.
We can also repair proofs to use a type with a different inductive structure.
We do not yet have any search procedures for this, but since \toolname lets you supply
custom configurations, you can use a custom configuration for this.

As an example, we ported functions and proofs between Coq's natural numbers:

\begin{lstlisting}
Inductive nat :=
| O : nat
| S : nat -> nat.
\end{lstlisting}
and a simple version of binary numbers, which comes from another source:

\begin{lstlisting}
Inductive binnat :=
| zero : binnat
| consOdd : binnat -> binnat
| consEven : binnat -> binnat.
\end{lstlisting}
This is one of the oldest known transport problems.
We are able to supply the equivalence and configuration manually,
then port functions and proofs.
For example, we port the addition function automatically,
then we can automatically port a proof of the lemma \lstinline{plus_n_Sm}:

\begin{lstlisting}
Lemma plus_n_Sm:
  forall (n m : nat), Nat.S (Nat.add n m) = Nat.add n (Nat.S m).
\end{lstlisting}
to a proof over binary numbers:

\begin{lstlisting}
Lemma plus_n_Sm:
  forall (n m : binnat), Bin.S (Bin.add n m) = Bin.add n (Bin.S m).
Proof.
  intros n m.
  induction (bin_natty n) as [|n0 n1 IHn0].
  - reflexivity.
  - apply (binnat_plus_n_Sm_expanded_rewrites2_inductive m n0 IHn0).
Defined.
\end{lstlisting}
where the new proof and new addition function do not at any point convert
the binary number back to natural numbers.

Notably, to port \lstinline{plus_n_Sm}, we had to deal with the equality problem.
\lstinline{RewEta} is what let us do this.
There is still one caveat, though: we had to explicitly write out applications of \lstinline{RewEta}
in our proof over unary natural numbers, since we do not yet have a way to configure \toolname with custom
matching procedures, and unification was not sufficient for matching with \lstinline{RewEta}.
We hope to make it possible for the user to also add custom matching rules,
and we think some ideas from Section~\ref{sec:discussion} could help with this.
Another thing worth noting is that we do not automatically get the efficient version of \lstinline{add} this way,
though it's worth determining whether there is a way to configure \toolname to do so.

\paragraph{Configuration}
We supplied this configuration manually.
Our definitions were as follows (eventually should just show the interesting parts):

\begin{lstlisting}
DepConstr(1, nat) := O 
DepConstr(2, nat) := S
DepConstr(1, binnat) := zero
DepConstr(2, binnat) :=
  binnat_rec
    (fun _ : binnat => binnat)
    (consOdd zero)
    (fun b0 _ : binnat => consEven b0)
    (fun _ IHb : binnat => consOdd IHb)

DepElim(n, p){f$_{1}$, f$_{2}$} : p n := nat_rect p f$_{1}$ f$_{2}$ n
DepElim(b, p){f$_{1}$, f$_{2}$} : p b := natty_rect p f$_{1}$ f$_{2}$ b (bin_natty n)

IdEta(nat) := fun (n : nat) => n
IdEta(binnat) := fun (b : binnat) => b

RewEta(2, nat) := fun (P : nat -> Type) (PO : P O) (PS : forall n : nat, P n -> P (nat_S n))
  (n : nat) (Q : P (nat_S n) -> Type) (H : Q (PS n (nat_rect P PO PS n))) =>
eq_rect (PS n (nat_rect P PO PS n)) (fun H0 : P (nat_S n) => Q H0) H
  (nat_rect P PO PS (nat_S n)) eq_refl
RewEta(2, binnat) := fun (P : binnat -> Type) (PO : P z) (PS : forall b : binnat, P b -> P (S b))
  (n : binnat) (Q : P (e.suc_binnat n) -> Type)
  (H : Q (PS n (binnat_nat_rect P PO PS n))) =>
eq_rect (PS n (binnat_nat_rect P PO PS n)) (fun H0 : P (e.suc_binnat n) => Q H0)
  H (binnat_nat_rect P PO PS (e.suc_binnat n)) (eq_sym (refold_elim_S P PO PS n)).
\end{lstlisting}
We need to introduce \lstinline{natty} and credit it to the Conor McBride paper,
and note why we go through it.
Interestingly, we used transport across algebraic ornaments in order to determine
\lstinline{DepElim}.
We need to note how \lstinline{bin_natty} is generated that way.
Also we need multiple \lstinline{RewEta} one for each constructor ugh.

% TODO what does the tactic decompiler do for this? It's broken. Why?

%\subsubsection{Algebraic}

%It is straightforward to fit the search algorithm from DEVOID into this framework, and in fact
%we can loosen the restriction that the language has primitive projections.
%Let $A$ be $A$ from DEVOID, let $B_{ind}$ be $B$ from DEVOID, let $I_B$ be $I_B$ from DEVOID,
%and let \lstinline{index} be \lstinline{index} from DEVOID.
%Let $B$ wrap $B_{ind}$ packed into a sigma type:

%\begin{lstlisting}
%B := $\lambda$ ($\vec{t}$ : $\vec{T}$) . ($\Sigma$ (i : I$_B$ $\vec{t}$) . B$_{ind}$ (index i $\vec{t}$))
%\end{lstlisting}
%Let $\vec{T_{B_j}}$ be the arguments of constructor type $C_{B_j}$ (type of constructor of $B_{\mathrm{ind}}$).
%Define \lstinline{DepConstr(j, B)} recursively using the following derivation (based on and same fall-through convention as the DEVOID paper %for now,
%and I'd prefer to move this away from a derivation but not sure how to do so and maintain formality): % TODO check

%\begin{mathpar}
%\mprset{flushleft}
%\small
%\hfill\fbox{$\Gamma$ $\vdash$ $(T_A, T_B)$ $\Downarrow_{C}$ $t$}\\%

%\inferrule[Dep-Constr-Conclusion]
%  { \Gamma \vdash \vec{t_{B_j}} : \vec{T_{B_j}} \\ \Gamma \vdash Constr(j, B)\ \vec{t_{B_j}} : B_{\mathrm{ind}} \vec{i_B}  }
%  { \Gamma \vdash (A\ \vec{i_A},\ B_{\mathrm{ind}}\ \vec{i_B}) \Downarrow_{p_{c}} \exists\ (\vec{i_B}[\mathrm{off}\ A\ B]) (Constr(j, B)\ \vec{t_{B_j}}) }

%\inferrule[Dep-Constr-Index] % new hypothesis for index
%  { \mathrm{new}\ n_B\ b_B \\ \Gamma,\ n_B : t_B \vdash (\Pi (n_A : t_A) . b_A,\ b_B) \Downarrow_{i_{c}} t }
%  {  \Gamma \vdash (\Pi (n_A : t_A) . b_A,\ \Pi (n_B : t_B) . b_B) \Downarrow_{C} t}

%\inferrule[Dep-Constr-IH] % inductive hypothesis
%  { \Gamma,\ n_B : B\ \vec{i_B} \vdash (b_A [n_B / n_A], b_B [\pi_l\ n_B / \vec{i_B}[\mathrm{off}\ A\ B]]) \Downarrow_{C} t }
%  { \Gamma \vdash (\Pi (n_A : A\ \vec{i_A}) . b_A, \Pi (n_B : B\ \vec{i_B}) \Downarrow_{C} \lambda (n_B : B\ \vec{i_B}) . t }

%\inferrule[Dep-Constr-Prod] % otherwise, unchanged (when we get rid of the gross fall-through thing, needs not new, and needs to check t_A and t_B not IHs)
%  { \Gamma,\ n_B : t_B \vdash (b_A [n_B / n_A], b_B) \Downarrow_{C} t }
%  { \Gamma \vdash (\Pi (n_A : t_A) . b_A, \Pi (n_B, t_B) . b_B) \Downarrow_{C} \lambda (n_B : t_B) . t }\\

%\inferrule[Dep-Constr]
%{ \Gamma \vdash Constr(j, A) : C_{A_j} \\ \Gamma \vdash (C_{A_j}, C_{B_j}) \Downarrow_{C} t }
%{ \Gamma \vdash (Constr(j, A), Constr(j, B_{\mathrm{ind}}) \Downarrow_{C} t }
%\end{mathpar}
%and \lstinline{DepElim(b, p)} similarly:

%\begin{mathpar}
%TODO
%\end{mathpar}

%Then:

%\begin{lstlisting}
%DepConstr(j, A) : C$_{A_{j}}$ := Constr(j, A)
%DepConstr(j, B) : C$_{A_{j}}$[B / A] := DepConstr(j, B)

%DepElim(a, p){f$_{1}$, $\ldots$, f$_{n}$} : p a := Elim(a, p){f$_{1}$, $\ldots$, f$_{n}$}
%DepElim(b, p){f$_{1}$, $\ldots$, f$_{n}$} : p b := DepElim(b, p)

%IdEta(A) := $\lambda$(a : A).a
%IdEta(B) := $\lambda$(b : B).$\exists$ ($\pi_l$ b) ($\pi_r$ b)
%\end{lstlisting}

% TODO investigate below projection thing, and write in when you finish
%For now assume we have some \lstinline{pack} function to pack into an existential;
%this is just for convenience.
%The indexer is just the first projection of this lifted across the eliminator rule, AFAIK---note this isn't exactly $\Pi_{l}$ like we use
%in the tool, but is really an eliminated $\Pi_{l}$? I will need to check on this, it's the only weird part.
%Also assume some \lstinline{index_args} function to add the new index to the appropriate arguments---I'll
%elaborate on this later but it's also something search needs to find and it's determined in terms of the \lstinline{indexer} that search finds.
%Also now, we no longer assume primitive projections.

%\subsubsection{Unpack sigma}

%This one is kind of weird but it gets us user-friendly types. I'll explain later.

%\begin{lstlisting}
%DepConstr(j, A) := (* TODO pack into existential, deal with equality *)
%DepConstr(j, B) : C$_{B_{j}}$ := Constr(j, B)

%DepElim(a, p){f$_{1}$, $\ldots$, f$_{n}$} : p a := (* TODO *)
%DepElim(b, p){f$_{1}$, $\ldots$, f$_{n}$} : p b := Elim(b, p){f$_{1}$, $\ldots$, f$_{n}$}

%IdEta(A) := $\lambda$(a : A).$\exists$ ($\exists$ ($\pi_l$ ($\pi_l$ a)) ($\pi_r$ ($\pi_l$ a))) ($\pi_r$ a)
%IdEta(B) := $\lambda$(b : B).b
%\end{lstlisting}

%\subsubsection{Records and tuples}

%This one should be easier. We'll play a similar trick with $B$ and $B_{ind}$ like we do for algebraic,
%and give things similar names.
%Then:

%\begin{lstlisting}
%DepConstr(j, A) : C$_{A_{j}}$ := Constr(j, A)
%DepConstr(j, B) : C$_{A_{j}}$[B / A] := $\lambda$ ($\vec{t_{A_j}}$ : $\vec{T_{A_j}}$) . (* TODO recursively pack into pair *)

%DepElim(a, p){f$_{1}$, $\ldots$, f$_{n}$} : p a := Elim(a, p){f$_{1}$, $\ldots$, f$_{n}$}
%DepElim(b, p){f$_{1}$, $\ldots$, f$_{n}$} : p b := (* TODO recursively eliminate product *)

%IdEta(A) := $\lambda$(a : A).a
%IdEta(B) := (* TODO recursive eta *)
%\end{lstlisting}
