\section{Conclusions \& Future Work}
\label{sec:discussion}

We showed how to combine a configurable proof term transformation with a tactic decompiler to build \toolname,
a proof refactoring and repair tool that is flexible and useful for real proof engineering scenarios.
\toolname has already helped an industrial proof engineer integrate Coq with a company proof engineering workflow,
and has supported refactoring and repair benchmarks common in the proof engineering community.

Moving forward, our goal is to make proofs easier to reuse and maintain regardless of proof engineering expertise.
We want to reach more users, and we want \toolname to integrate seemlessly with Coq.
We believe that the most significant progress here will come from adapting data structures and ideas
from the rewrite system and constraint solver communities to use the notion of equality that encodes the correctness of \textsc{Transform}: equality up 
to transport. After all, three of the biggest challenges we encountered in scaling up the \toolname proof term transformation---multiple equivalences, nontermination, and the need for custom rewrite rules---are problems that those communities have spent decades researching already.% TODO cite

We conclude with a discussion of those three challenges (Section~\ref{sec:problems}), why and how we believe ideas from the rewrite system and constraint
solver communities can help (Section~\ref{sec:egraph}), and how we believe these ideas can help the proof engineering community
even beyond building better refactoring and repair tools (Section~\ref{sec:beyond}).
Our hope is to inspire these communities to work together to cover an exciting new domain.

\subsection{Three Challenges}
\label{sec:problems}

\paragraph{Multiple Equivalences}

Deciding when to run the transformation rules from the proof term transformation in Section~\ref{sec:key2} is left to the implementation.
\toolname automates the most basic case of this: changing \textit{every} occurrence of the old type \A to the equivalent new type \B.
This can lead to confusing or undesired behavior.
The ideal would be a type-directed search procedure that decides when to apply a given rule in the proof term transformation.

\begin{figure}
\begin{minipage}{0.54\textwidth}
\begin{lstlisting}
Repair (@\codediff{Tuple.output}@) (@\codediff{Record.output}@) 
  in Tuple.op as op_1.
Repair (@\codediff{Tuple.input}@) (@\codediff{Record.input}@)
  in op_1 as op.

(@\codeauto{op}@) : ((@\codediff{bool * Record.output}@)) $\rightarrow$ (@\texttt{Record.output}@).
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{0.44\textwidth}
\begin{lstlisting}
Repair (@\codediff{Tuple.input}@) (@\codediff{Record.input}@)
  in Tuple.op as op_1.
Repair (@\codediff{Tuple.output}@) (@\codediff{Record.output}@)
  in op_1 as op.

(@\codeauto{op}@) : (@\codediff{Record.input}@) $\rightarrow$ (@\texttt{Record.output}@).
\end{lstlisting}
\end{minipage}
\caption{The initial attempt at refactoring \lstinline{op} (left) and the correct attempt (right).}
\label{fig:op}
\end{figure}

This challenge came up with our industrial proof engineer from Section~\ref{sec:industry}. % TODO once we fix that section up, move the old example here
When the proof engineer used \toolname to port a function:

\begin{lstlisting}
op : (r : bool * (nat * bool)) $\rightarrow$ nat * bool.
\end{lstlisting}
from the tuples on the left to the records on the right of Figure~\ref{fig:records},
whether \toolname behaved as expected depended on how the proof engineer interacted with the \lstinline{Repair} command.
The proof engineer first tried to refactor the output type before the input type (Figure~\ref{fig:op}, left),
but later had to switch the order (Figure~\ref{fig:op}, right) to get the function with the expected type.

This is because the argument \lstinline{r : (bool * (nat * bool))} to \lstinline{Tuple.op} can be viewed as 
having either type \lstinline{bool * Tuple.output} or \lstinline{Tuple.input}.
Both functions in Figure~\ref{fig:op} are correct and equivalent refactorings of \lstinline{Tuple.op},
but only one is what the proof engineer expected.

The proof engineer wished for type-directed search.
That way, the proof engineer could write:

\begin{lstlisting}
Repair Tuple.op as op : (@\texttt{Record.input}@) $\rightarrow$ (@\texttt{Record.input}@).
\end{lstlisting}
and have \toolname apply the proof term transformation along the correct equivalences
to produce \lstinline{op} with the desired type.
This would make for a much better workflow.

\paragraph{Nontermination}

Naively applying the proof term transformation from Section~\ref{sec:key2} can result in nontermination,
especially when the output type refers to the input type.
Consider the second equivalence that we used in Section~\ref{sec:dep}: % TODO make sure we mention this equivalence in that section

\begin{lstlisting}
$\forall$ n, $\Sigma$(s : $\Sigma$(n : nat).vector T n).$\pi_l$ s = n $\simeq$ vector T n
\end{lstlisting}
\toolname lifts the term:

\begin{lstlisting}
vector T 0
\end{lstlisting}
backwards along this equivalence, to the term:

\begin{lstlisting}
$\Sigma$(s : $\Sigma$(n : nat).vector T n).$\pi_l$ s = 0
\end{lstlisting}
and then stops.
However, it could just as well keep going:

\begin{lstlisting}
$\Sigma$(s : $\Sigma$(n : nat).$\Sigma$(r : $\Sigma$(m : nat).vector T m).$\pi_l$ r = n).$\pi_l$ s = 0
\end{lstlisting}
and so on, forever.

To cope with this, \toolname includes termination checks, and does not attempt to run a transformation rule
if it determines that doing so would result in divergent behavior.
However, these termination checks are ad-hoc and do not capture every potentially nonterminating use case.
This can both clutter the code and result in unreliable behavior.

\paragraph{Custom Rewrite Rules}

\toolname with manual configuration is not always smart enough to match the appropriate rule in the proof term
transformation without the proof engineer manually expanding the input term.
Proof engineers would benefit from the ability to add custom rewrite rules to \toolname
and expand the input terms automatically.

For example, to support the change from unary \lstinline{nat} to binary \lstinline{N}
from Section~\ref{sec:bin}, we had to manually expand \lstinline{Nat.add_n_Sm}
to explicitly apply \lstinline{Iota} over \lstinline{Nat}.
\toolname was unable to expand the term automatically with Coq's unification alone.
It would help the proof engineer to be able to extend \toolname with custom rewrite
rules that expand applications of \lstinline{Iota} automatically, but are faster and less brittle than unification.
The search procedures in \textbf{Configure} all include custom rewrite rules, but
without modifying \toolname, there is currently no way for the proof engineer to do the same.

\subsection{Three Proposed Solutions with Univalent E-Graphs}
\label{sec:egraph}

% TODO cite:
%https://dl.acm.org/doi/book/10.5555/909447
%https://dl.acm.org/doi/pdf/10.1145/1186632.1186633
%https://dl.acm.org/doi/10.1145/1594834.1480915

% TODO cite cubical agda congruence and related HoTT stuff

The key to implementing efficient type-directed automatic transport both for \toolname
and for other systems may be to adapt special data structures from the constraint solver world to use a
notion of equivalence that corresponds to equality up to transport.
This may help not only build better proof refactoring and repair tools, but also build better proof reuse tools,
implement better congruence tactics in univalent type theories, and better automate transport in HoTT.


We are aware of exactly one implementation of type-directed automatic transport:
the univalent parametricity framework uses type classes coupled with user-defined hints to implement type-based search.
However, the authors note that this approach scales poorly, and that without the right user-defined
hints, it can be slow or diverge.
This is a step in the right direction, but it is not enough.

TODO draft text, not done yet.

Automatic transport is really nothing but a rewrite system.
Equality up to transport is just an equivalence relation, and automatic transport just searches for proofs of that relation and rewrites along those proofs, either by directly applying transport (in homotopy type theory and univalent parametricity) or by transforming the term in a metalanguage (in \toolname).

When thinking about how to build clean and efficient type-directed transport, then, we find it natural to look
at how other programming languages communities have already implemented clean and efficient rewrite systems.
Modern rewrite systems often use special data structures called e-graphs that are built specifically
for clean and efficient rewriting across equivalences.
They are used, for example, inside of SMT solvers like Z3.
They are basically designed to help deal with the problem like the one we saw with op earlier, when there were multiple equivalences to 
choose from.

Recent work extended these data structures to handle dependent types. The result was wonderful automation of equality proofs in the Lean theorem prover. The cost was introducing an axiom called UIP to Lean that is not in Coq and is incompatible with homotopy type theory (without any axioms, the notion of equality between dependent types is just not very useful for automation like this).

What if we were to implement this data structure for equality up to transport rather than heterogenous equality as in Lean?
I think this also hides the key to better proof repair. Proof repair is really nothing but proof reuse over time. And I bet that, in a dependently-typed language, you can think of any change as an equivalence between sigma types. The problem just becomes exposing a good interface around searching for and transporting along this equivalence (maybe using example-based search like the original PUMPKIN PATCH, maybe doing something different), getting rid of all references to the old type (which \toolname can do), and then hopefully at some point converting everything back to a reasonable, maintainable set of tactics (which our decompiler does). This is the future, guys.

\paragraph{Multiple Equivalences}

\paragraph{Termination}

\paragraph{Custom Rewrite Rules}

TODO matching Iota is just letting the user add additional rewrite rules (or call out to custom code).
see chandra conversation.
cite szalinski to note how to not be just syntactic/how to call out to something else

\subsection{Beyond Refactoring \& Repair}
\label{sec:beyond}

TODO not done yet: In univalent type theories like homotopy type theory, I bet we could implement automatic transport this way, using what would essentially be an extremely powerful congruence tactic coupled with the transport function. In the univalent parametricity framework, I imagine we could get much more predictable and efficient type-directed transport when managing a lot of equivalences. And in \toolname, I bet we could also get efficient type-directed search. Text about the other challenges.

TODO cite congruence closure in cubical agda. our thing is just a rewrite system for transport. already used for congruence. so can use for transport too.
and right now you have to transport manually which sucks.
The only solution we are aware of so far for this is that of the univalent parametricity framework,
which as the authors note can sometimes be slow or unpredictable without user-supplied hints.

