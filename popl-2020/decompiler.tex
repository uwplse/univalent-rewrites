\section{Decompiling Proof Terms to Tactics}
\label{sec:decompiler}

\textbf{Transform} produces a proof term,
while the proof engineer typically writes and maintains proof scripts made up of tactics.
We improve usability thanks to the realization that, since Coq's proof term language Gallina is very structured,
we can decompile these Gallina terms to suggested Ltac proof scripts for the proof engineer to maintain.

\begin{quote}
\textbf{Insight 3}: The transformed proof terms can then be translated back to tactic scripts.
\end{quote}

\textbf{Decompile} implements a prototype of this translation:
it translates a proof term to a candidate proof script that attempts to prove the same theorem the same way.
Note that this problem is not well defined: while there is always a proof script that 
works (applying the proof term with the \lstinline{apply} tactic), the result is often qualitatively unreadable.
This is the baseline behavior to which the decompiler defaults.
The goal of the decompiler is to improve on that baseline as much as possible,
or else suggest a candidate proof script that is close enough to correct that the proof engineer can step through it
and manually massage it into something that works and is maintainable.

The output language for the implementation of \textbf{Decompile} is Ltac, the proof script language for Coq.
Ltac can be confusing to reason about, since Ltac tactics can refer to Gallina terms, and the semantics of Ltac depends both on the
semantics of Gallina and on the implementation of proof search procedures written in OCaml.
To give a sense of how the decompiler works without the clutter of these proof search details, we start by defining a mini
decompiler from CIC$_{\omega}$ to a simple subset of Ltac containing just a few predefined tactics (Section~\ref{sec:first}).
We then explain how we scale that up to the actual implementation (Section~\ref{sec:second}), and where we plan to go from there.

\subsection{A Mini Decompiler}
\label{sec:first}

\begin{figure}
\small
\begin{grammar}
<v> $\in$ Vars, <t> $\in$ CIC$_{\omega}$

<p> ::= intro <v> |  rewrite <t> <t> | symmetry | apply <t> | \\
induction <t> <t> \{ <p>, \ldots, <p> \} | split \{ <p>, <p> \} | left | right | <p> . <p>
\end{grammar}
\vspace{-0.3cm}
\caption{Qtac syntax.}
\label{fig:ltacsyntax1}
\end{figure}

The mini decompiler takes CIC$_{\omega}$ terms and produces tactics in a mini version of Ltac which we call Qtac.
The syntax for Qtac is in Figure~\ref{fig:ltacsyntax1}.
Qtac includes hypothesis introduction (\lstinline{intro}),
rewriting by equalities (\lstinline{rewrite}), symmetry of equality (\lstinline{symmetry}),
application of a term to prove the goal (\lstinline{apply}), induction (\lstinline{induction}),
case splitting of conjunctions (\lstinline{split}),
constructors of disjunctions (\lstinline{left} and \lstinline{right}), and
composition (\lstinline{.}).
Unlike in Ltac, in Qtac, \lstinline{induction} and \lstinline{rewrite} always take a motive explicitly, rather than relying on a unification engine.
Similarly, \lstinline{apply} applies only the function without inferring any arguments, and leaves those arguments to proof obligations.
The implementation reasons about Ltac and so does not make these assumptions.

\begin{figure}
\begin{mathpar}
\mprset{flushleft}
\small
\hfill\fbox{$\Gamma$ $\vdash$ $t$ $\Rightarrow$ $p$}\vspace{-0.4cm}\\

\inferrule[Intro]
  { \Gamma,\ n : T \vdash b \Rightarrow p }
  { \Gamma \vdash \lambda (n : T) . b \Rightarrow \mathrm{intro}\ n.\ p }

\inferrule[Symmetry]
  { \Gamma \vdash H \Rightarrow p }
  { \Gamma \vdash \mathtt{eq\_sym}\ H \Rightarrow \mathrm{symmetry}.\ p } \\

\inferrule[Split]
  { \Gamma \vdash l \Rightarrow p \\ \Gamma \vdash r \Rightarrow q }
  { \Gamma \vdash \mathrm{Constr}(0,\ \wedge)\ l r \Rightarrow \mathrm{split} \{ p, q \}.\ }

\inferrule[Left]
  { \Gamma \vdash H \Rightarrow p }
  { \Gamma \vdash \mathrm{Constr}(0,\ \vee)\ H \Rightarrow \mathrm{left}.\ p }

\inferrule[Right]
  { \Gamma \vdash H \Rightarrow p }
  { \Gamma \vdash \mathrm{Constr}(1,\ \vee)\ H \Rightarrow \mathrm{right}.\ p } \\

\inferrule[Rewrite]
  { \Gamma \vdash H_1 : x = y \\ \Gamma \vdash H_2 \Rightarrow p }
  { \Gamma \vdash \mathrm{Elim}(H_1,\ P) \{ x,\ H_2,\ y \} \Rightarrow \mathrm{symmetry}.\ \mathrm{rewrite}\ P\ H_1.\ p }

\inferrule[Induction]
  { \Gamma \vdash \vec{f} \Rightarrow \vec{p} }
  { \Gamma \vdash \mathrm{Elim}(t,\ P)\ \vec{f} \Rightarrow \mathrm{induction}\ P\ t\ \vec{p} }

\inferrule[Apply]
  { \Gamma \vdash t \Rightarrow p }
  { \Gamma \vdash f t \Rightarrow \mathrm{apply}\ f.\ p }

\inferrule[Base]
  { \\ }
  { \Gamma \vdash t \Rightarrow \mathrm{apply}\ t }
\end{mathpar}
\vspace{-0.3cm}
\caption{Qtac decompiler semantics.}
\label{fig:someantics}
\end{figure}

The semantics for the mini decompiler $\Gamma \vdash t \Rightarrow p$ are in Figure~\ref{fig:someantics} (assuming $=$, \lstinline{eq_sym}, $\wedge$, and $\vee$ are defined as in Coq).
As with the real decompiler, the mini decompiler defaults to the proof script
that applies the entire proof term with \lstinline{apply} (\textsc{Base}).
Otherwise, it improves on that behavior by recursing over the proof term and constructing a proof script using a predefined set of tactics.

For the mini decompiler, this is straightforward: Lambda terms become introduction of hypotheses (\textsc{Intro}), since they introduce new bindings
in the environment of the body. Applications of \lstinline{eq_sym} become symmetry of equality (\textsc{Symmetry}).
Constructors of conjunction and disjunction map to the respective tactics (\textsc{Split}, \textsc{Left}, and \textsc{Right}).
Applications of equality eliminators compose symmetry (to orient the rewrite direction with the goal) with rewrites (\textsc{Rewrite}),
and all other applications of eliminators become induction (\textsc{Induction}).
The remaining applications become apply tactics (\textsc{Apply}).
In all cases, the decompiler recurses on the remaining body, breaking into cases when relevant, until no other preconditions match.
At that point the \textsc{Base} case holds, and we are done.

\begin{figure}
\begin{minipage}{0.44\textwidth}
\begin{lstlisting}
fun (@\codesimb{(y0 : list A)}@) =>(@\vspace{-0.04cm}@)
  (@\codesima{list_rect}@) _ _(@\vspace{-0.04cm}@)
    (fun (@\codesima{a l IHl}@) =>(@\vspace{-0.04cm}@)
      (@\codesimc{eq_ind_r}@) _ (@\vspace{-0.04cm}@)
        (@\codesimd{eq_refl}@)(@\vspace{-0.04cm}@)
        (@\codesimc{(app_nil_r (rev l) (a::[]))}@))(@\vspace{-0.04cm}@)
    (@\codesime{eq_refl}@)(@\vspace{-0.04cm}@)
    (@\codesima{y0}@)
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{0.54\textwidth}
\begin{lstlisting}
- (@\codesimb{intro y0.}@)
  (@\codesima{induction y0 as [a l IHl|]}.@)(@\vspace{-0.04cm}@)
  + (@\codesimc{simpl. rewrite (app_nil_r (rev l) (a::[])).}@)(@\vspace{-0.04cm}@)
    (@\codesimd{reflexivity.}@)(@\vspace{-0.04cm}@)
  + (@\codesime{reflexivity.}@)
\end{lstlisting}
\end{minipage}
\vspace{-0.3cm}
\caption{Proof term (left) and corresponding decompiled proof script (right) for the base case of 
\lstinline{rev_app_distr} from Section~\ref{sec:overview}, with corresponding terms and tactics 
highlighted with the same color, and nothing else highlighted for clarity.}
\label{fig:rainbow}
\end{figure}

While the mini decompiler is very simple, only a few small changes are needed
to move this from CIC$_{\omega}$ to Coq.
Furthermore, the result can already handle some of the example proofs \toolname has produced.
The generated proof term of \lstinline{rev_app_distr} with swapped list constructors from Section~\ref{sec:overview},
for example, consists only of induction, rewriting, simplification, and reflexivity.
Figure~\ref{fig:rainbow} shows the proof term for the base case of \lstinline{rev_app_distr} 
alongside the decompiled tactic script that \toolname produces.
This script is fairly low-level and close to the proof term, but it is already something that the proof engineer
can step through piece by piece to understand, modify, and maintain.
There are very few differences from the mini decompiler needed to produce this,
for example handling of rewrites in both directions (\lstinline{eq_ind_r} as opposed to \lstinline{eq_ind}),
simplifying to handle motive inference for rewrites,
and turning applications of \lstinline{eq_refl} into reflexivity.

In fact, since \toolname uses an existing command to translate pattern matching and fixpoints to eliminators,
\textit{all} of the proof terms that \toolname produces will use induction and rewriting instead.
Because we have control over output terms, even a mini decompiler gets us pretty far.

% TODO add any new things RanDair implements, like exists

\subsection{Scaling Up to Ltac}
\label{sec:second}

The mini decompiler abstracts a lot of the details that make Ltac so useful to proof engineers---and so painful to 
reason about automatically.
This section discusses how we scale up from this mini decompiler to a prototype Gallina to Ltac decompiler (\href{https://github.com/uwplse/coq-plugin-lib/blob/master/src/coq/decompiler/decompiler.ml}{decompiler.ml}),
and how we imagine the decompiler continuing to evolve from there.

\paragraph{Second Pass}
The mini decompiler reasons about tactics one subterm at a time, and produces simple tactic scripts.
To produce a more natural set of tactics, \textbf{Decompile} operates in two passes: first it runs something similar to the mini decompiler, and then it modifies those tactics to produce a more natural proof script.
For example, the second pass cancels out sequences of \lstinline{intros} and \lstinline{revert} tactics.
The second pass can also take suggested tactics or parts of the old proof script from the proof engineer as input---and there
is ongoing work using this functionality to iteratively replace tactics with custom tactics and decision procedures,
checking the result as the second pass recurses.
When done, this will further improve usability,
as custom tactics and decision procedures are common in some proof engineering styles.
Further improvements could come from preserving comments and indentation.

\paragraph{Induction and Rewriting}
The mini decompiler includes more predictable versions of \lstinline{rewrite} and \lstinline{induction}
than those found in Coq. \textbf{Decompile} includes additional logic to reason about these tactics.
For example, Qtac assumes that there is only one \lstinline{rewrite} direction. Ltac has two rewrite directions,
and so the decompiler infers the right direction based on the motive used.

Qtac also assumes that both tactics take the inductive motive explicitly.
In Coq, however, both tactics infer the motive automatically.
Consequentially, Coq will sometimes infer the wrong motive without manipulation of goals and hypotheses,
or will fail to infer a motive at all.
This is especially common for the \lstinline{rewrite} tactic, which is purely syntactic.
To handle induction, the decompiler strategically uses the \lstinline{revert} tactic to manipulate the goal
so that Coq can better infer the motive.
To handle rewrites, it uses the \lstinline{simpl} tactic to refold the goal before rewriting.
Neither of these approaches are guaranteed to work, so the proof engineer may sometimes need to tweak the output proof script appropriately.
We have found that even if we pass Coq's induction principle an explicit motive, Coq still sometimes fails due
to unrepresented assumptions.
Long term, using another tactic like \lstinline{change} to manipulate hypotheses and goals before applying these tactics
may help with cases for which Coq cannot infer the correct motive.

\paragraph{Manipulating Hypotheses}
Changing from Qtac to Ltac is not the only challenge in writing the decompiler---we also scale from CIC$_{\omega}$ to Coq.
This introduces let bindings, which are generated by tactics like \lstinline{rewrite in}, \lstinline{apply in}, and \lstinline{pose}.
\textbf{Decompile} implements support for \lstinline{rewrite in} and \lstinline{apply in} similarly to how it implements support for
\lstinline{rewrite} and \lstinline{apply}, but with three differences:

\begin{enumerate}
\item it ensures that the unmanipulated hypothesis does not occur in the body of the let expression,
\item it swaps the direction of the rewrite, and
\item it checks for generated subgoals and recurses into those subgoals.
\end{enumerate}
In all other cases, the implementation uses the \lstinline{pose} tactic, a catch-all for let bindings.

\paragraph{Pretty Printing}
After decompiling proof terms, \textbf{Decompile} pretty prints the result to the proof engineer.
Like the mini decompiler, \textbf{Decompile} represents its output language using a predefined grammar of Ltac tactics,
albeit one that is larger than Qtac.
It maintains the recursive proof structure as it goes, then uses that structure to print proofs of subgoals using bullet points.
It displays the resulting proof script to the proof engineer, who can modify it as needed.
For convenience, it includes scripts that automate the process of printing all of these tactic proofs to a Coq file,
in case the proof engineer does not want such an interactive workflow.
\toolname keeps all output proof terms from the proof term transformation in the Coq environment in case the decompiler does not succeed.
Once the proof engineer has this new proof, she can remove the old specifications, functions, and proofs, using the repaired
versions from then on.


