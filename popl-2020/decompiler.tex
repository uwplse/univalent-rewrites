\section{Decompiling Proof Terms to Tactics}
\label{sec:decompiler}

\textbf{Transform} produces a proof term,
while the proof engineer typically writes and maintains proof scripts made up of tactics.
We improve user experience thanks the realization that, since Coq's proof term language Gallina is very structured,
we can decompile these Gallina terms to Ltac proof scripts for the proof engineer to maintain.

\begin{quote}
\textbf{Insight 3}: The transformed proof terms can then be translated back to tactics.
\end{quote}

The \textbf{Decompile} component implements a prototype of this translation:
it accepts a proof term and generates a candidate proof script that attempts to prove the same theorem.
Of course, this problem is not very well defined: there is a proof script that always 
works (applying the entire proof term with the \lstinline{apply} tactic), but is often unreadable.
This is the baseline for success of the decompiler, and the decompiler defaults to this behavior.
From there, the goal of the decompiler is to improve on that baseline as much as possible,
or else produce a candidate proof script that is close enough that the proof engineer can step through it
and manually massage it into something that both works and is maintainable.

The output language for the implementation of \textbf{Decompile} is Ltac, the proof script language for Coq.
Ltac can be confusing to reason about, since Ltac tactics can refer to Gallina terms, and the semantics of Ltac depends both on the
semantics of Gallina and on the implementation of proof search procedures written in OCaml.
To give a sense of how the decompiler works without the clutter of these proof search details, we start by defining a toy
decompiler from CIC$_{\omega}$ to a simple subset of Ltac containing just a few predefined tactics (Section~\ref{sec:first}).
We then explain how we scale that up to the actual implementation (Section~\ref{sec:second}).

\subsection{A Toy Decompiler}
\label{sec:first}

\begin{figure}
\small
\begin{grammar}
<v> $\in$ Vars, <t> $\in$ CIC$_{\omega}$

<p> ::= intro <v> |  rewrite <t> <t> | symmetry | apply <t> | induction <t> <t> \{ <p>, \ldots, <p> \} | split \{ <p>, <p> \} | left | right | <p> . <p>
\end{grammar}
\caption{Qtac syntax.}
\label{fig:ltacsyntax1}
\end{figure}

The toy decompiler takes CIC$_{\omega}$ terms and produces tactics in a toy version of Ltac which we call Qtac.\footnote{Pronounced \textit{cute-tac}.}
The syntax for Qtac is in Figure~\ref{fig:ltacsyntax1}.
Qtac includes hypothesis introduction (\lstinline{intro},
rewriting by equalities (\lstinline{rewrite}), symmetry (\lstinline{symmetry}) of equality,
application of a term to prove the goal (\lstinline{apply}), induction over terms (\lstinline{induction}),
case splitting of conjunctions (\lstinline{split}),
constructors of disjunctions (\lstinline{left} and \lstinline{right}), and
composition (\lstinline{.}).

Unlike in Ltac, in Qtac, \lstinline{induction} and \lstinline{rewrite} always take a motive explicitly, rather than relying on a unification engine.
Smilarly, \lstinline{apply} applies only the function without inferring any arguments, and leaves those arguments to proof obligations.
The implementation reasons about Ltac and so does not make these assumptions.

\begin{figure}
\begin{mathpar}
\mprset{flushleft}
\small
\hfill\fbox{$\Gamma$ $\vdash$ $t$ $\Rightarrow$ $p$}\\

\inferrule[Intro]
  { \Gamma,\ n : T \vdash b \Rightarrow p }
  { \Gamma \vdash \lambda (n : T) . b \Rightarrow \mathrm{intro}\ n.\ p }

\inferrule[Symmetry]
  { \Gamma \vdash H \Rightarrow p }
  { \Gamma \vdash \mathtt{eq\_sym}\ H \Rightarrow \mathrm{symmetry}.\ p } \\

\inferrule[Split]
  { \Gamma \vdash l \Rightarrow p \\ \Gamma \vdash r \Rightarrow q }
  { \Gamma \vdash \mathrm{Constr}(0,\ \wedge)\ l r \Rightarrow \mathrm{split} \{ p, q \}.\ }

\inferrule[Left]
  { \Gamma \vdash H \Rightarrow p }
  { \Gamma \vdash \mathrm{Constr}(0,\ \vee)\ H \Rightarrow \mathrm{left}.\ p }

\inferrule[Right]
  { \Gamma \vdash H \Rightarrow p }
  { \Gamma \vdash \mathrm{Constr}(1,\ \vee)\ H \Rightarrow \mathrm{right}.\ p } \\

\inferrule[Rewrite]
  { \Gamma \vdash H_1 : x = y \\ \Gamma \vdash H_2 \Rightarrow p }
  { \Gamma \vdash \mathrm{Elim}(H_1,\ P) \{ x,\ H_2,\ y \} \Rightarrow \mathrm{symmetry}.\ \mathrm{rewrite}\ P\ H_1.\ p }

\inferrule[Induction]
  { \Gamma \vdash \vec{f} \Rightarrow \vec{p} }
  { \Gamma \vdash \mathrm{Elim}(t,\ P)\ \vec{f} \Rightarrow \mathrm{induction}\ P\ t\ \vec{p} }

\inferrule[Apply]
  { \Gamma \vdash t \Rightarrow p }
  { \Gamma \vdash f t \Rightarrow \mathrm{apply}\ f.\ p }

\inferrule[Base]
  { \\ }
  { \Gamma \vdash t \Rightarrow \mathrm{apply}\ t }
\end{mathpar}
\caption{Qtac decompiler semantics.}
\label{fig:someantics}
\end{figure}

The semantics for the toy decompiler are in Figure~\ref{fig:someantics} (assuming $=$, \lstinline{eq_sym}, $\wedge$, and $\vee$ are defined as in Coq).
As with the real decompiler, the toy decompiler defaults to the naive proof script
that applies the entire proof term with the \lstinline{apply} tactic (\textsc{Base}).
Otherwise, it improves on that behavior by recursing over the proof term and constructing a proof script using a predefined set of tactics.

For the toy decompiler, this is fairly straightforward: Lambda terms become introduction of hypotheses (\textsc{Intro}), since they introduce new bindings
in the environment of the body. Applications of \lstinline{eq_sym} become symmetry of equality (\textsc{Symmetry}).
Constructors of conjunction and disjunction become map to the respective tactics (\textsc{Split}, \textsc{Left}, and \textsc{Right}).
Applications of equality eliminators compose symmetry (to orient the rewrite direction with the goal) with rewrites (\textsc{Rewrite}),
and all other applications of eliminators become induction (\textsc{Induction}).
The remaining applications become apply tactics (\textsc{Apply}).
In all cases, the decompiler recurses on the remaining body, breaking into cases when relevant, until no other preconditions match.
At that point the \textsc{Base} case holds, and we are done.

While the toy compiler is very simple, only a few simple changes are needed
to move this from CIC$_{\omega}$ to Coq.
Furthermore, the result can already handle some of the example proofs \toolname has produced.
The generated proof term of \lstinline{rev_app_distr} with swapped list constructors from Section~\ref{sec:overview},
for example, consists only of induction, rewriting, simplification, and reflexivity.
The proof term for the base case:

\begin{lstlisting}
fun (@\codesimb{(y0 : list A)}@) =>
  (@\codesima{list_rect}@) _ _
    (fun (@\codesima{a l IHl}@) =>
      (@\codesimc{eq_ind_r}@) _ (@\codesimd{eq_refl}@) (@\codesimc{(app_nil_r (rev l) (a::[]))}@))
    (@\codesime{eq_refl}@)
    (@\codesima{y0}@)
\end{lstlisting}
decompiles to this proof script:

\begin{lstlisting}
- (@\codesimb{intro y0.}@) (@\codesima{induction y0 as [a l IHl|]}.@)
  + (@\codesimc{simpl. rewrite (app_nil_r (rev l) (a::[])).}@) (@\codesimd{reflexivity.}@)
  + (@\codesime{reflexivity.}@)
\end{lstlisting}
where corresponding terms and tactics are highlighted with the same color, and nothing else is highlighted for clarity.
Of course, this script is fairly low-level and close to the proof term, but it is already something that the proof engineer
can step through piece by piece to understand and maintain the proof.
There are very few differences from the toy decompiler needed to produce this,
for example handling of rewrites in both directions (\lstinline{eq_ind_r} as opposed to \lstinline{eq_ind}),
simplifying to handle motive inference for rewrites,
and turning applications of \lstinline{eq_refl} into reflexivity.

In fact, since \toolname uses the \lstinline{Preprocess} command from \textsc{Devoid}, \textit{all} of the proof terms that \toolname
produces will use induction and rewriting rather than fixpoints and pattern matching.
Because we have control over output terms, even a toy decompiler gets us pretty far.

% TODO add any new things RanDair implements, like exists

\subsection{Scaling Up}
\label{sec:second}

The toy decompiler abstracts a lot of the details that make Ltac so useful to proof engineers---and so painful to 
reason about automatically.
This section discusses how we scale up from this toy decompiler to a prototype Gallina to Ltac decompiler,
and how we imagine the decompiler continuing to evolve from there.

\paragraph{Second Pass}
The toy decompiler reasons about tactics one subterm at a time, and produces tactics only from a predefined set of tactics.
This does not always match the thought processes of proof engineers.
To produce a more natural set of tactics, \textbf{Decompile} operates in two passes: first it runs something that looks a lot
like the toy decompiler, and then it modifies those tactics to produce a more natural proof script.
For example, the first pass, like the toy decompiler, produces a single \lstinline{intro} per hypothesis in a lambda abstraction;
the second pass combines each sequence of \lstinline{intro} tactics into an \lstinline{intros} tactic.

The prototype decompiler still has no special reasoning for decision procedures and custom tactics, which are common
in some proof engineering styles.
This second pass will be the natural integration point for these.
Our current plan is to take additional input in this pass, either directly from the proof engineer
or by feeding in the original proof script from before \textbf{Transform}.
We can then iteratively replace tactics with custom tactics and decision procedures, and check the result see if it works.
We also plan to support tacticals like \lstinline{;} and \lstinline{try}.
For now, massaging the output to use these is left to the proof engineer.

\paragraph{Induction and Rewriting}
The toy decopmiler includes simpler and more predictable versions of \lstinline{rewrite} and \lstinline{induction}
than those found in Coq. The implementation of \textbf{Decompile} includes additional logic to reason about these tactics.
For example, it assumes that there is only one \lstinline{rewrite} direction. Coq has two rewrite directions,
and so the decompiler infers the right direction based on the motive used.

It also assumes that both tactics take the inductive motive explicitly.
In Coq, however, both tactics infer the motive automatically.
Consequentially, Coq will sometimes infer the wrong motive without manipulation of goals and hypotheses,
or will fail to infer a motive at all.
This is especially common for the \lstinline{rewrite} tactic, which is purely syntactic.
To handle induction, the decompiler strategically use the \lstinline{revert} tactic to manipulate the goal
so that Coq can better infer the motive.
To handle rewrites, it uses the \lstinline{simpl} tactic to refold the goal before rewriting.
Neither of these approaches are guaranteed to work, so the proof engineer may sometimes need to tweak the output proof script appropriately.
We have found that even if we pass Coq's induction principle an explicit motive, Coq still sometimes fails due
to unrepresented assumptions.
Long term, using another tactic like \lstinline{change} to manipulate hypotheses and goals before applying these tactics
may help with cases for which Coq cannot infer the correct motive.

\paragraph{Manipulating Hypotheses}
Changing from Qtac to Ltac is not the only challenge in writing the decompiler---we also scale from CIC$_{\omega}$ to Coq.
This introduces let bindings, which are generated by tactics like \lstinline{rewrite in}, \lstinline{apply in}, and \lstinline{pose}.
\textbf{Decompile} implements support for \lstinline{rewrite in} and \lstinline{apply in} similarly to how it implements support for
\lstinline{rewrite} and \lstinline{apply}, but with two differences:

\begin{enumerate}
\item it ensures that the unmanipulated hypothesis does not occur in body of the let expression,
\item it swaps the direction of the rewrite, and
\item it checks for generated subgoals and recurses into those subgoals.
\end{enumerate}
In all other cases, the implementation uses the \lstinline{pose} tactic, a catch-all for let bindings.

\paragraph{Pretty Printing}
After decompiling proof terms, there is one final step to present the information to the proof engineer: pretty printing.
Like the toy decompiler, the implementation of \textbf{Decompile} represents its output language using a predefined grammar of Ltac tactics,
albeit one that is larger than Qtac.
It maintains the recursive proof structure as it goes, then uses that proof structure to print proofs of subgoals using bullet points.
It displays the resulting proof script to the proof engineer, who can then modify it as needed to ensure that it works correctly
and is maintainable.
For convenience, it includes scripts that automate the process of printing all of these tactic proofs to a Coq file,
in case the proof engineer does not want such an interactive workflow.
\toolname keeps all output proof terms from the proof term transformation in the Coq environment as a fallback in case the decompiler does not succeed.
Once the proof engineer has this new proof, she can remove the old specifications, functions, and proofs, using the repaired
versions from then on.


