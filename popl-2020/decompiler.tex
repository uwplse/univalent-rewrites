\section{Decompiling Proof Terms to Tactics}
\label{sec:decompiler}

\textbf{Transform} produces a proof term,
while the proof engineer typically writes and maintains proof scripts made up of tactics.
We improve user experience thanks the realization that, since Coq's proof term language Gallina is very structured,
we can decompile these Gallina terms to Ltac proof scripts for the proof engineer to maintain. In other words:

\begin{quote}
\textbf{Insight 3}: The transformed proof terms can then be translated back to tactics.
\end{quote}

The \textbf{Decompile} component implements a prototype of this translation.
This decompiler prototype has shown early promising results.
For example, it produced the automatically generated tactic proof for \codeauto{\lstinline{rev_app_distr}} 
in Section~\ref{sec:overview}, as well as the tactic proofs of \lstinline{section}
and \lstinline{retraction} in Section~\ref{fig:equivalence}.

The output language for the implementation of \textbf{Decompile} is Ltac, the proof script language for Coq.
Ltac can be confusing to reason about, since Ltac tactics can refer to Gallina terms, and the semantics of Ltac depends both on the
semantics of Gallina and on the implementation of proof search procedures written in OCaml.
To give a sense of how the decompiler works without the clutter of these proof search details, we start by defining a toy
decompiler from CIC$_{\omega}$ to a simple subset of Ltac containing just a few predefined tactics (Section~\ref{sec:first}).
We then explain how we scale that up to the actual implementation (Section~\ref{sec:second}).

\subsection{A Toy Decompiler}
\label{sec:first}

\begin{figure}
\small
\begin{grammar}
<v> $\in$ Vars, <t> $\in$ CIC$_{\omega}$

<p> ::= intro <v> |  rewrite <t> <t> | symmetry | apply <t> | induction <t> <t> \{ <p>, \ldots, <p> \} | split \{ <p>, <p> \} | left | right | <p> . <p>
\end{grammar}
\caption{Qtac syntax.}
\label{fig:ltacsyntax1}
\end{figure}

The toy decompiler takes CIC$_{\omega}$ terms and produces tactics in a toy version of Ltac which we call Qtac.\footnote{Pronounced \textit{cute-tac}}.
The syntax for Qtac is in Figure~\ref{fig:ltacsyntax1}.
Qtac includes hypothesis introduction (\lstinline{intro},
rewriting by equalities (\lstinline{rewrite}), symmetry (\lstinline{symmetry}) of equality,
application of a term to prove the goal (\lstinline{apply}), induction over terms (\lstinline{induction}),
case splitting of conjunctions (\lstinline{split}),
constructors of disjunctions (\lstinline{left} and \lstinline{right}), and
composition (\lstinline{.}).

Unlike in Ltac, in Qtac, \lstinline{induction} and \lstinline{rewrite} always take a motive explicitly, rather than relying on a unification engine.
Smilarly, \lstinline{apply} applies only the function without inferring any arguments, and leaves those arguments to proof obligations.
The implementation reasons about Ltac and so does not make these assumptions.

\begin{figure}
\begin{mathpar}
\mprset{flushleft}
\small
\hfill\fbox{$\Gamma$ $\vdash$ $t$ $\Rightarrow$ $p$}\\

\inferrule[Intro]
  { \Gamma,\ n : T \vdash b \Rightarrow p }
  { \Gamma \vdash \lambda (n : T) . b \Rightarrow \mathrm{intro}\ n.\ p }

\inferrule[Symmetry]
  { \Gamma \vdash H \Rightarrow p }
  { \Gamma \vdash \mathtt{eq\_sym}\ H \Rightarrow \mathrm{symmetry}.\ p } \\

\inferrule[Split]
  { \Gamma \vdash l \Rightarrow p \\ \Gamma \vdash r \Rightarrow q }
  { \Gamma \vdash \mathrm{Constr}(0,\ \wedge)\ l r \Rightarrow \mathrm{split} \{ p, q \}.\ }

\inferrule[Left]
  { \Gamma \vdash H \Rightarrow p }
  { \Gamma \vdash \mathrm{Constr}(0,\ \vee)\ H \Rightarrow \mathrm{left}.\ p }

\inferrule[Right]
  { \Gamma \vdash H \Rightarrow p }
  { \Gamma \vdash \mathrm{Constr}(1,\ \vee)\ H \Rightarrow \mathrm{right}.\ p } \\

\inferrule[Rewrite]
  { \Gamma \vdash H_1 : x = y \\ \Gamma \vdash H_2 \Rightarrow p }
  { \Gamma \vdash \mathrm{Elim}(H_1,\ P) \{ x,\ H_2,\ y \} \Rightarrow \mathrm{symmetry}.\ \mathrm{rewrite}\ P\ H_1.\ p }

\inferrule[Induction]
  { \Gamma \vdash \vec{f} \Rightarrow \vec{p} }
  { \Gamma \vdash \mathrm{Elim}(t,\ P)\ \vec{f} \Rightarrow \mathrm{induction}\ P\ t\ \vec{p} }

\inferrule[Apply]
  { \Gamma \vdash t \Rightarrow p }
  { \Gamma \vdash f t \Rightarrow \mathrm{apply}\ f.\ p }

\inferrule[Base]
  { \\ }
  { \Gamma \vdash t \Rightarrow \mathrm{apply}\ t }
\end{mathpar}
\caption{Qtac decompiler semantics.}
\label{fig:someantics}
\end{figure}

The semantics for the toy decompiler are in Figure~\ref{fig:someantics} (assuming $=$, \lstinline{eq_sym}, $\wedge$, and $\vee$ are defined as in Coq).
This decompiler works like the real decompiler: it accepts a proof term and generates a candidate proof script that attempts to prove the same theorem.
As with the real decompiler, the baseline for success of the toy decompiler is the naive proof script
that applies the entire proof term with the \lstinline{apply} tactic.
Such a proof script will always work, but will often be unreadable.
The decompiler defaults to this baseline behavior (\textsc{Base}).

Otherwise, the goal of the decompiler is to improve on that baseline as much as possible,
or else produce a candidate proof script that is close enough that the proof engineer can manually massage it into something that
both works and is maintainable.
It does this by recursing over the proof term and constructing a proof script using a predefined set of tactics.

For the toy decompiler, this is fairly straightforward: Lambda terms become introduction of hypotheses (\textsc{Intro}), since they introduce new bindings
in the environment of the body. Applications of \lstinline{eq_sym} become symmetry of equality (\textsc{Symmetry}).
Constructors of conjunction and disjunction become map to the respective tactics (\textsc{Split}, \textsc{Left}, and \textsc{Right}).
Applications of equality eliminators compose symmetry (to orient the rewrite direction with the goal) with rewrites (\textsc{Rewrite}),
and all other applications of eliminators become induction (\textsc{Induction}).
The remaining applications become apply tactics (\textsc{Apply}).
In all cases, the decompiler recurses on the remaining body, breaking into cases when relevant, until no other preconditions match.
At that point the \textsc{Base} case holds, and we are done.

While the toy compiler is very simple, if we move from CIC$_{\omega}$ to Coq,
this can already handle some of the example proofs \toolname has produced.
The generated proof term of \codeauto{\lstinline{rev_app_distr}} from Section~\ref{sec:overview},
for example, consists only of induction, rewriting, simplification, and reflexivity.
Since \toolname uses the \lstinline{Preprocess} command from \textsc{Devoid}, \textit{all} of the proof terms that \toolname
produces will use induction and rewriting rather than fixpoints and pattern matching.
In short, because we have control over output terms, even a toy decompiler gets us pretty far.

\subsection{Scaling Up}
\label{sec:second}

The toy decompiler abstracts a lot of the details that make Ltac so useful to proof engineers---and so painful to 
reason about automatically.
This section discusses how we scale up from this toy decompiler to a prototype Gallina to Ltac decompiler,
and how we imagine the decompiler continuing to evolve from there.

\paragraph{Second Pass}
The toy decompiler reasons about tactics one subterm at a time, and produces tactics only from a predefined set of tactics.
This does not always match the thought processes of proof engineers.
To produce a more natural set of tactics, \textsc{Decompile} operates in two passes: first it runs something that looks a lot
like the toy decompiler, and then it modifies those tactics to produce a more natural proof script.
For example, the first pass, like the toy decompiler, produces a single \lstinline{intro} per hypothesis in a lambda abstraction;
the second pass combines each sequence of \lstinline{intro} tactics into an \lstinline{intros} tactic.

The prototype decompiler still can do very little with decision procedures and custom tactics, which are very common
in some proof engineering styles.
This second pass will be the natural integration point for these.
Our current plan is to take additional input in this pass, either directly from the proof engineer
or by feeding in the original proof script from before \textsc{Transform}.
We can then iteratively replace tactics with custom tactics and decision procedures, and check the result see if it works.
We also plan to support tacticals like \lstinline{;} and \lstinline{try}.
For now, massaging the output to use these is left to the proof engineer.

\paragraph{Induction and Rewriting}
unification: simplifying rewrites, inductive motives, revert.
\lstinline{simpl} before \lstinline{rewrite} (for now just always when we do induction, but eventually can be smarter based on motive) (technically not
in second pass yet, but easier conceptually).
rewrite direction (swaps for rewrite in).
Applications of induction eliminators are deconstructed into a proof of each case, arguments to the motive, and the value we perform induction on.
The position of the inducted value (hard time explaining this): Pos = \# of arguments … – pms + 1 (???)
Following the inducted value are additional arguments to the motive. A “revert” tactic is inserted for each such hypothesis before induction. This forgets the hypotheses by adding them as arguments to the goal, producing a motive requiring these arguments exactly, and so they must be re-introduced in each case.
why it's a candidate and not guaranteed.

\paragraph{Manipulating Hypotheses}
TODO left off here

Coq instead of CIC$_{\omega}$.
Corresponds to let bindings.
extend with let expressions. pose. rewrite-in. apply-in. subgoals for those.
checking if let binding is actually in the conclusion.
skipping if it is not.
Rewrite-in transforms an existing hypothesis by binding a new hypothesis to replace all occurrences of the original in the remainder of the proof.
\lstinline{let _ : _ := eq_ind _ _ _ H2 _ H1 in _} where
		H2 is a hypothesis in context,
		H2 doesn’t occur in the body.
		Generate \lstinline{rewrite H1 in H2}. Recurse on body.
Apply-in transforms a hypothesis by applying a function to it, rebinding its type to the result of the function.
		In general,
		\lstinline{let _ : _ := f H in _} where
		H is a hypothesis in context,
		H doesn’t occur in the body.
		Generate  \lstinline{apply f in H}. Recurse on body.
		Unless we have,
		\lstinline{let H2 : _ := f H1 in H2 _} where
function \lstinline{f} expects additional arguments after \lstinline{H1}.
Performing \lstinline{apply f in H1}, Coq transforms the goal to be the result type of \lstinline{f}, and creates additional goals for arguments required after H1. Thus, we recurse on H2 and all following arguments it’s applied to.
Pose is a catch-all for let-expressions. It explicitly binds a new name to a value.
\lstinline{let A : B := C in D} generates \lstinline{pose C as A}, recurse on the body.

\paragraph{Pretty Printing}
When all this is done, it pretty-prints and helps clean up that proof script for the proof engineer.
The proof engineer can massage the final result as desired into a proof script that both works and is maintainable.





