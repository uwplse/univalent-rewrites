\section{Proof Reuse Over Time}
\label{sec:key1}

We can view any tool that refactors or repairs proofs after a change in specification
as \textit{reusing} the proofs about our old specification to derive proofs about our new specification.
In other words:

\begin{quote}
\textbf{Insight 1}:
Proof refactoring and repair are both just 
proof reuse %~\cite{Ringer2019, felty1994generalization, caplan1995logical, pons2000generalization, johnsen2004theorem}
\textit{over time} (Section~\ref{sec:reuse}). The key to supporting both is to build a generic proof reuse
tool that can handle the additional challenges imposed by the reuse occuring over time (Section~\ref{sec:time}). 
\end{quote}

\subsection{Proof Reuse}
\label{sec:reuse}

Proof refactoring and proof repair are both a form of proof reuse.
The only difference from standard proof reuse is that these reuse \textit{old} proofs and
specifications rather than proofs and specifications that exist elsewhere, like in a library.

Consider the example from Section~\ref{sec:overview}. This was a refactoring, or a semantics-preserving change.
Thus, \toolname used the old proof of \lstinline{rev_app_distr} defined over the old version of \lstinline{list}
to generate the new proof of \lstinline{rev_app_distr} defined over the new version of \lstinline{list}.
Since this was a refactoring, this required no additional information from the user, and the resulting functions
and proofs behaved exactly the same way.

Repair, in contrast, refers to changes that are \textit{not} semantics-preserving.
One example might be changing a specification to use positive rather than natural numbers.
Coq defines an inductive type \lstinline{positive}:

\begin{lstlisting}
Inductive positive : Set :=
| xI : positive -> positive
| xO : positive -> positive
| xH : positive.
\end{lstlisting}
This behaves the same way as the set of natural numbers that are nonzero:

\begin{lstlisting}
Definition pos_nat := { n : nat | Nat.eqb n 0 = false }.
\end{lstlisting}
Thus, to repair our proofs about \lstinline{nat} functions to proofs about \lstinline{positive} functions with the desired behavior,
we need the additional restriction that those numbers are nonzero. 
This restriction manifests as a proof obligation for the user.
Once we have a proof of that obligation, we can reuse proofs about \lstinline{nat} to derive
proofs about \lstinline{positive}.

In general, the problems of proof refactoring and repair reduce to proof reuse across \textit{type equivalences}~\cite{univalent2013homotopy},
or pairs of functions that map back and forth between two types and are mutual inverses.
For example, for our \lstinline{list} refactoring, we have two functions:

\begin{lstlisting}
f : Coq.Init.Datatypes.list -> list
g : list -> Coq.Init.Datatypes.list
\end{lstlisting}
that are mutual inverses:

\begin{lstlisting}
section: forall l, g (f l) = l.
retraction: forall l, f (g l) = l.
\end{lstlisting}
and so this type equivalence holds:

\begin{lstlisting}
Coq.Init.Datatypes.list $\simeq$ list
\end{lstlisting}
\toolname can discover these functions and prove these facts automatically, then use them to refactor and repair proofs.
We will see more of this in Section~\ref{sec:search}.

Note that there can be infinitely many equivalences corresponding to any change in specification,
so the choice in equivalence is to some degree an art that depends on what the user wants.
In the repair example, we use this equivalence:

\begin{lstlisting}
pos_nat $\simeq$ positive
\end{lstlisting}
Note, however, that we could just as well define a transformation that shifts every natural number by one,
and unshifts back in the opposite direction.
This would induce a different equivalence: % TODO could we though?

\begin{lstlisting}
nat $\simeq$ positive
\end{lstlisting}
This equivalence corresponds to a simple refactoring.
However, it produces functions and proofs with different behaviors from the 
equivalence that uses \lstinline{pos_nat}.

The problem of proof reuse across equivalences is known as \textit{transport}. % TODO cite
Thus, any tool that can refactor or repair proofs implements transport across the classes
of equivalences that it supports.
But we cannot use just any transport method: we must transport functions and proofs \textit{over time}.

\subsection{Over Time}
\label{sec:time}

That proof refactoring and repair are reuse \textit{over time} dictates that proofs
must no longer refer in any way to the old specification, since the old specification no longer exists.
This means that most transport methods % TODO cite
are not immediately useful for refactoring and repair.

The goal of a proof refactoring and repair tool is, in essence, to
define a transport method over a broad set of changes that
removes references to the old specification, rather than converting back and forth
like standard transport methods.
\toolname accomplishes this using a configurable proof term transformation.

\section{A Configurable Proof Term Transformation}
\label{sec:key2}

At the heart of \toolname is a configurable proof term transformation for transporting
proofs across equivalences. This is based on the proof term transformation from 
\textsc{Devoid}~\cite{Ringer2019}, which solved this problem for particular class of equivalences.
The goal of \toolname is to implement something like \textsc{Devoid}, but over
a much broader set of changes, and with better workflow integration.
We were able to generalize the \textsc{Devoid} algorithm to do this:

\begin{quote}
\textbf{Insight 2}:
The proof term transformation from the \textsc{Devoid} proof reuse tool can be generalized
to build such a generic proof reuse tool (Section~\ref{sec:generic}),
and the result is configurable both by the developer and by the user (Section~\ref{sec:configurable}).
\end{quote}

\subsection{A Proof Term Transformation}
\label{sec:generic}

\begin{figure}
\begin{mathpar}
\mprset{flushleft}
\small
\hfill\fbox{$\Gamma$ $\vdash$ $t$ $\Uparrow$ $t'$}\\

\inferrule[Dep-Elim]
  { \Gamma \vdash p_{a} \Uparrow p_b \\ \Gamma \vdash \vec{f_{a}}\phantom{l} \Uparrow \vec{f_{b}} }
  { \Gamma \vdash \mathrm{DepElim}(a,\ p_{a}) \vec{f_{a}} \Uparrow \mathrm{DepElim}(b,\ p_b) \vec{f_{b}} }

\inferrule[Dep-Constr]
{ \Gamma \vdash \vec{t}_{a} \Uparrow \vec{t}_{b} } %\\ TODO must we explicitly lift A to B if we want to handle parameters/indices?
{ \Gamma \vdash \mathrm{DepConstr}(j,\ A)\ \vec{t}_{a} \Uparrow \mathrm{DepConstr}(j,\ B)\ \vec{t}_{b}  }

\inferrule[Id-Eta]
  { \\ }
  { \Gamma \vdash \mathrm{IdEta}(A) \Uparrow \mathrm{IdEta}(B) }

\inferrule[Rew-Eta]
  { \Gamma \vdash c_A \Uparrow c_B \\ \Gamma \vdash q_A \Uparrow q_B \\ \Gamma \vdash e_A \Uparrow e_B }
  { \Gamma \vdash \mathrm{RewEta}(j,\ A,\ q_A,\ t_A) \Uparrow \mathrm{RewEta}(j,\ B,\ q_B,\ t_B) }

\inferrule[Equivalence]
  { \\ }
  { \Gamma \vdash A\ \Uparrow B }

\inferrule[Constr]
{ \Gamma \vdash T \Uparrow T' \\ \Gamma \vdash \vec{t} \Uparrow \vec{t'} }
{ \Gamma \vdash \mathrm{Constr}(j,\ T)\ \vec{t} \Uparrow \mathrm{Constr}(j,\ T')\ \vec{t'} }

\inferrule[Ind]
  { \Gamma \vdash T \Uparrow T' \\ \Gamma \vdash \vec{C} \Uparrow \vec{C'}  }
  { \Gamma \vdash \mathrm{Ind} (\mathit{Ty} : T) \vec{C} \Uparrow \mathrm{Ind} (\mathit{Ty} : T') \vec{C'} }

\inferrule[Elim] % TODO wait why do we have c here when it clearly refers to the term we eliminate over? um
  { \Gamma \vdash c \Uparrow c' \\ \Gamma \vdash Q \Uparrow Q' \\ \Gamma \vdash \vec{f} \Uparrow \vec{f'}}
  { \Gamma \vdash \mathrm{Elim}(c, Q) \vec{f} \Uparrow \mathrm{Elim}(c', Q') \vec{f'}  }

%% Application
\inferrule[App]
 { \Gamma \vdash f \Uparrow f' \\ \Gamma \vdash t \Uparrow t'}
 { \Gamma \vdash f t \Uparrow f' t' }

% Lamda
\inferrule[Lam]
  { \Gamma \vdash T \Uparrow T' \\ \Gamma,\ t : T \vdash b \Uparrow b' }
  {\Gamma \vdash \lambda (t : T).b \Uparrow \lambda (t : T').b'}

% Product
\inferrule[Prod]
  { \Gamma \vdash T \Uparrow T' \\ \Gamma,\ t : T \vdash b \Uparrow b' }
  {\Gamma \vdash \Pi (t : T).b \Uparrow \Pi (t : T').b'}
\end{mathpar}
\caption{Proof term transformation.}
\label{fig:final}
\end{figure}

This generalized proof term transformation forms the core of \toolname, configured in the \textbf{Configure}
step and implemented in the \textbf{Transform} step.
Figure~\ref{fig:final} shows this proof term transformation.
The transformation operates over terms in CIC$_{\omega}$.
It assumes the same grammar and semantics for CIC$_{\omega}$
as the transformation from \textsc{Devoid} that it is based off of
with one exception: it removes the restriction that projections must be primitive.

Like the transformation from \textsc{Devoid}, this transformation is parameterized over
two equivalent types \A and \B (rule \lstinline{Equivalence}).
The goal of the proof term transformation is to preserve that equivalence in some way, while no longer referring to the old specification.
That is, for equivalent types \A and \B, the transformation takes as input functions and proofs
that refer to \A and returns functions and proofs that refer to \B.
Furthermore, the transformed functions behave the same way as the input functions,
and the transformed proofs talk about the same things as the input proofs.

More formally, the output of the proof term transformation ought to be equal to the input of the program transformation
up to transport along the equivalence between \A and \B.
This is the same as the correctness criteria for the program transformation from \textsc{Devoid} that this is based on.
The difference is that this program transformation is configurable to different \A and \B with different relationships between them,
while \textsc{Devoid} is specific to a particular kind of relation between \A and \B.

The proof term transformation is configurable to a particular equivalence between \A and \B.
The only interesting rules are the two sets of two that correspond to that equivalence:

\begin{enumerate}
\item \lstinline{Dep-Constr} and \lstinline{Dep-Elim} to transform constructors and eliminators, and
\item \lstinline{Id-Eta} and \lstinline{Rew-Eta} to transform identity and equalities.
\end{enumerate}
These four rules rely on a configuration (Section~\ref{sec:configurable}) % TODO lists
provided either by a search procedure or by the user.
From there, the rest of the transformation is straightforward.

\subsection{Configurable}
\label{sec:configurable}

At a high level, the configuration ensures that the transformation accomplishes two goals:

\begin{enumerate}
\item preserves the equivalence between \A and \B, and
\item produces well-typed terms.
\end{enumerate}
These two goals each correspond to a pair of configuration parts, respectively:

\begin{enumerate}
\item \lstinline{DepConstr} and \lstinline{DepElim} define how to transform constructors and eliminators, thereby preserving the equivalence (Section~\ref{sec:equivalence}), and 
\item \lstinline{IdEta} and \lstinline{RewEta} define how to transform identity and equalities, thereby producing well-typed terms (Section~\ref{sec:equality}).
\end{enumerate}

The four parts of this configuration must be in relation to one another in a certain way in order for the proof
term transformation to work correctly (Section~\ref{sec:art}).
Section~\ref{sec:search} shows many example configurations.

\subsubsection{Equivalence}
\label{sec:equivalence}

The proof term transformation is correct when it maps an input to an output that is equal
to the input up to transport along the supplied equivalence between \A and \B.
The two rules that correspond to this in the program transformation are \lstinline{Dep-Constr} and \lstinline{Dep-Elim},
which use configuration parts \lstinline{DepConstr} and \lstinline{DepElim}, respectively.

At a high level, these describe how to construct and eliminate \A and \B, wrapping the two types with a common inductive structure.
Since Coq is a constructive logic, the only way to construct an \A (respectively \B) is to use its
constructors; since we assume primitive eliminators, the only way to match over an \A (respectively \B)
is to apply its eliminator. 
The \lstinline{Dep-Constr} rule thus transports constructions of \A to constructions of \B,
while the \lstinline{Dep-Elim} rule rule transports eliminators of \A to eliminators of \B.
The result will construct \B in place of \A, and it will eliminate \B in place of \A.
Thanks to this, \toolname can remove all references to the old type \A,
and produce a refactored or repaired term that refers only to the new type \B.

Notably, though, \A and \B may not be inductive types, so \lstinline{DepConstr} and \lstinline{DepElim}
correspond to \textit{dependent} construction and elimination.
In addition, even when \A and \B are both inductive types,
\lstinline{DepConstr} and \lstinline{DepElim} over \B do not necessarily correspond
to the constructors and eliminators of \B itself.
Instead, they describe how to construct and eliminate \B \textit{as if it is an \A},
wrapping the constructors and eliminators of \B to contructors and eliminators with the
structure of \A.
This way, we do not need to change the number and order of arguments to constructors or eliminators when applying the program transformation.

It is easiest to see this by example.
If we consider our simple \lstinline{list} refactoring, our dependent
constructors for \lstinline{Coq.Init.Datatypes.list} are just the normal constructors
\lstinline{nil} and \lstinline{cons}. Our dependent constructors for our updated
\lstinline{list}, however, swaps the order back to the original order of constructors:

\begin{lstlisting}
DepConstr(0, list T) : list T := nil.
DepConstr(1, list T) : T -> list T -> list T := cons.
\end{lstlisting}
The dependent eliminator swaps the cases similarly, so that its type signature is identical
to \lstinline{list_rect} but with the updated \lstinline{list} type and constructors.

Things are a bit more complicated with our \lstinline{nat} to \lstinline{positive} repair.
We can make many different choices here. One is to choose these dependent constructors:

\begin{lstlisting}
DepConstr(0, pos_nat) : pos_nat :=
  $\exists$ 1 eq_refl.

DepConstr(1, pos_nat) : pos_nat -> pos_nat :=
  fun pn => $\exists$ (S ($pi_l$ pn)) eq_refl.
\end{lstlisting} % TODO test this example w/ lifting, define equivalence, etc. revisit h-set problem! can we define depelim when we don't have an hset???
and a dependent eliminator with this type:

\begin{lstlisting}
forall (P : pos_nat -> Type),
  P DepConstr(0, pos_nat) ->
  (forall (pn : pos_nat), P pn -> P (DepConstr(1, pos_nat) pn)) ->
  forall (pn : pos_nat), P pn.
\end{lstlisting}
It is then straightforward to adapt functions and proofs over \lstinline{nat} to \lstinline{pos_nat}.
The intuition is that \lstinline{DepElim} just removes the zero case 
For example, the addition function just becomes this:

\begin{lstlisting}
fun n m : pos_nat =>
  $\exists$ ($\pi_l$ n + $\pi_l$ m) (DepElim _ eq_refl (fun _ _ => eq_refl) n)
\end{lstlisting} % TODO syntax
The program tranformation can then transport that to inductive \lstinline{positive}.

Section~\ref{sec:search} includes many more examples, including cases when \A and \B
themselves have different numbers of constructors (but the configuration requires that
is exactly one \lstinline{DepConstr(j, A)} for each \lstinline{DepConstr(j, B)}).

In all cases, together these induce a type equivalence between \A and \B:

\begin{lstlisting}
Definition f : A -> B := DepElim _ [DepConstr(0, B), ..., DepConstr (n, B)]
Definition g : B -> A := DepElim _ [DepConstr(0, A), ..., DepConstr (n, A)]
\end{lstlisting}
The search procedures described in Section~\ref{sec:search} discover these functions and prove this automatically
for \A and \B with certain properties.
Otherwise, as in \textsc{Devoid}, this must be true in order for the proof term transformation
to work correctly, but the user does not need to prove that it hold.
Section~\ref{sec:art} discusses this in more detail.

\subsubsection{Equality}
\label{sec:equality}

The other two rules corresponding to the configuration deal with transporting equalities.
Since our language does not assume univalence, we do not have that equivalence is equivalent to equality.
The result of this, as noted in \citet{tabareau2019marriage},
is that we must do additional work to transform definitional and propositional equalities.
Otherwise, if we tranform a term \lstinline{t : T} to some \lstinline{t' : T'}, we do not necessarily
have that our transformation maps \lstinline{T} to \lstinline{T'}.
For this to work, we may sometimes need to transform definitional equalities to propositional equalities.

We see this with natural numbers \lstinline{nat} and binary numbers \lstinline{binnat}
in Section~\ref{sec:search}.
There, we at some point try to repair this theorem over \lstinline{nat} from the Coq standard library:

\begin{lstlisting}
(* TODO update this text and example now that we've changed 6, or move 6 up *)
Lemma plus_Sn_m :
  forall (n m : nat), S n + m = S (n + m).
Proof.
  intros n m; induction n; simpl; auto.
Qed.
\end{lstlisting}
to instead refer to binary numbers.
The term underneath this is just reflexivity:

\begin{lstlisting}
(fun n m : nat => eq_refl) : forall n m : nat, S n + m = S (n + m)
\end{lstlisting}
Naively transforming reflexivity (definitional equality) to reflexivity results in a type error,
since the transported depenedent constructor corresponding to \lstinline{S}---
a function that takes the successor of any binary natural number---
does not preserve definitional equality.

The trick to transforming this correctly is to view the definitional equality above as a contracted propositional equality:

\begin{lstlisting}
(fun n m : nat =>
  eq_rect
    (fun m => S n + m)
    (fun (p : nat -> nat) => S n + m = p m)
    eq_refl
    (fun m => S (n + m))
    eq_refl)
: forall n m : nat, S n + m = S (n + m)
\end{lstlisting}
We can then transform that propositional equality to a propositional equality over binary numbers (after transforming \lstinline{+} to operate
over binary numbers):

\begin{lstlisting}
(fun n m : Bin.nat =>
  eq_rect
    (fun m => Bin.S n + m)
    (fun (p : Bin.nat -> Bin.nat) => S n + m = p m)
    eq_refl
    (fun m => Bin.S (n + m))
    (refold_elim_S (fun _ => Bin.nat -> Bin.nat) (fun b3 : Bin.nat => b3) (fun _ add (b3 : Bin.nat) => Bin.S (add b3)) n)
: forall n m : Bin.nat, S n + m = S (n + m)
\end{lstlisting}
which gives us a proof of the theorem we want.

The key to this working is \lstinline{refold_elim_S}, which essentially states that
we can always refold the successor case of inductive proofs about \lstinline{Bin.nat}
that go through the dependent eliminator over \lstinline{Bin.nat}:

\begin{lstlisting}
refold_elim_S :
  forall (P : Bin.nat -> Type) (PO : P Bin.O) (PS : forall n, P n -> P (Bin.S n)) (n : Bin.nat),
    DepElim P PO PS (Bin.S n) = PS n (DepElim P PO PS n).
\end{lstlisting}
% TODO everywhere here need to be consistent about DepConstr, DepElim, syntax, etc.
The equivalent over \lstinline{nat} would follow by reflexivity, so its application is implicit.
We can derive a rewrite rule from this:

\begin{lstlisting}
(fun P PO PS n (Q : P (Bin.S n) -> Type) (H : Q (DepElim P PO PS (Bin.S n))) : Q (PS n (DepElim P PO PS n)) :=
  eq_rect
    (DepElim P PO PS (Bin.S n))
    (fun (H : P (Bin.S n)) => Q H)
    H
    (PS n (DepElim P PO PS n))
    (refold_elim_S P PO PS n)
: forall P PO PS n (Q : P (Bin.S n) -> Type), Q (DepElim P PO PS (Bin.S n)) -> Q (PS n (DepElim P PO PS n)).
\end{lstlisting}
This rewrite rule turns out to be \lstinline{RewEta} for \lstinline{Bin.nat}, one of the two configuration parts that 
handles contracted equalities:
\lstinline{Rew-Eta} uses \lstinline{RewEta} to transform expanded propositional equalities, while
\lstinline{Id-Eta} uses \lstinline{IdEta} to transform the expanded identity function.
In this example, \lstinline{IdEta} is just the identity function.
In our repair, \lstinline{IdEta} expands the existential:

\begin{lstlisting}
fun (n : pos_nat) => $\exists$ ($\pi_l$ n) ($\pi_r$ n)
\end{lstlisting}
so that, unlike \textsc{Devoid}, we do not need our algorithm to assume primitive projections.

For any type \A or \B,
There is just one \lstinline{IdEta}, but there are as many \lstinline{RewEta} as there are
inductive hypotheses of \lstinline{DepElim}.
This is because we can minimize the equality problem to preserving definitional equalities
in the eliminator---we'll see this more in Section~\ref{sec:search}.

\lstinline{RewEta} in the above example is not reflexivity precisely becuse our two inductive types
\lstinline{nat} and \lstinline{Bin.nat} do not have the same inductive structure---they are not \textit{ornaments}. % TODO cite
When our types are ornaments, like with our simple refactor above, every \lstinline{RewEta} is just reflexivity.

\subsubsection{Equivalence \& Equality}
\label{sec:art}

The configuration (\lstinline{DepConstr}, \lstinline{DepElim}, \lstinline{IdEta}, \lstinline{RewEta}) instantiates
the proof term transformation to a particular equivalence between \A and \B.
We noted in Section~\ref{sec:key1} that choosing an equivalence is a bit of an art:
there can be infinitely many equivalences that correpond to a 
given change in specification, only some of which are useful.
Beyond that, even once we have chosen an equivalence, we could define many possible configurations that correspond
to the equivalence, some of which will produce functions and proofs that are more useful or efficient than others.

Thankfully, once the art is done, we can at least ensure that it is \textit{correct art}.
The correctness criteria for the configuration relate \lstinline{DepConstr}, \lstinline{DepElim}, \lstinline{IdEta}, and \lstinline{RewEta}
in a way that preserves both equivalence (Section~\ref{sec:equivalence}) and equality (Section~\ref{sec:equality}).
To preserve equivalence, we need that each \lstinline{DepConstr(j, A)} is equal to each \lstinline{DepConstr(j, B)} up to transport,
and similarly that \lstinline{DepElim} over \A is equal to \lstinline{DepElim} over \B up to transport:

\begin{lstlisting}
TODO
\end{lstlisting}
The intuition for this is based on insights from \textsc{Devoid},
and is proven on an example in the univalent parametricity framework.\footnote{\url{https://github.com/CoqHott/univalent_parametricity/commit/7dc14e69942e6b3302fadaf5356f9a7e724b0f3c}}
That these induce a type equivalence follows from that.

To preserve equality, we need to relate the dependent constructors and eliminators to
\lstinline{IdEta} and \lstinline{RewEta}:

\begin{lstlisting}
TODO
\end{lstlisting}
The intuition is that these rules is that we need proofs about equality to lift to proofs about equality,
and again we can only ever construct or eliminate at some point, so it is enough
to use these facts just in the dependent constructors and eliminators.

These can be difficult to prove---the former requires either a special framework % TODO cite UP
or a univalent type theory.
Thankfully, the user does not need to prove these in order to use \toolname.
Rather, these simply need to hold in order for the proof term transformation to work correctly.

All of this comes together to transport functions and proofs across the equivalence between \A and \B
using a program transformation.
Section~\ref{sec:search} shows some particular instantions of this configuration and their applicability to real programs and proofs.

\subsection{Practical Concerns}
\label{sec:practical}

This is draft text.

\paragraph{Matching}

Note that \citet{tabareau2019marriage} says that transporting these equalities directly
is intractable in general.
This is partly correct: it is easy to \textit{describe} how to transport these equalities,
but it is difficult to implement.
Notably, applications of \lstinline{IdEta} and \lstinline{RewEta} show up contracted in typical code examples,
as we saw in the theorem at the top of this section.
Thus, the implementation must be able to identify these contracted \lstinline{IdEta} and \lstinline{RewEta}
applications and expand them before applying transport.
This same problem shows up for dependent constructors and eliminators to some degree,
but it is much more pronounced for equalities,
since \lstinline{RewEta} can be too complex for unification to match against.

The implementation of each search procedure from Section~\ref{sec:search} includes custom rewrite rules that correspond to expanding
contracted applications of each of these.
There is not yet any way for the user to provide these custom rewrite rules as well,
so when configuration is manual, the user must first expand input terms to apply \lstinline{RewEta} over \A.
This can be painful, so we have plans for allowing users to also write custom rewrite rules,
which we discuss in Section~\ref{sec:discussion}.

undecidability in general, unification and evars, optimizations for 

\paragraph{Preprocess}

and other stuff from \textsc{Devoid}

\paragraph{Reduction}

delta, eta, opaque

\paragraph{From Rules to an Algorithm}

When these correctness crtieria hold, this means that you \textit{can} always run one of the transformation rules.
This does not necessarily mean that you \textit{should}.
Whether you should depends what the user wants.
In some cases, repeatedly transforming the input program or proof would never terminate
(consider, for example, when \B refers to \A---a case disallowed by \textsc{Devoid} but allowed by \toolname).
Something about how we hadle this.
Section~\ref{sec:discussion} describes some ideas
for handling this using type-directed search in the future.
% TODO maybe move the above plus the other place that talks about this to its own subsection

\paragraph{Efficiency}

optimizations for faster transport, optimizations for faster or smaller terms

%First we need that \lstinline{DepElim} over $A$ into \lstinline{DepConstr} over $B$ and \lstinline{DepElim} over $B$ into
%\lstinline{DepConstr} over $A$ form an equivalence between $A$ and $B$. When that's true, I think it should hold that \lstinline{DepElim} over $A$
%and \lstinline{DepElim} over $B$ are in univalent relation with one another. If not, then that's an extra condition.
%Finally, we need the transformation to preserve definitional equalities. Not sure about the general case, but for vectors and lists,
%we need:

%\begin{lstlisting}
%  $\forall$ A l (f : $\forall$ (l : sigT (Vector.t A)), l = l),
%    vect_dep_elim A (fun l => l = l) (f nil) (fun t s _ => f (cons t s)) l = f (id_eta l).
%\end{lstlisting}
%and:

%\begin{lstlisting}
%Definition elim_id (A : Type) (s : {H : nat & t A H}) :=
%  vect_dep_elim
%    A
%    (fun _ => {H : nat & t A H})
%    nil
%    (fun (h : A) _ IH =>
%      cons h IH)
%    s.

% $\forall$ A h s,
%    exists (H : cons h (elim_id A s) = elim_id A (cons h s)),
%      H = eq_refl.
%\end{lstlisting}
%More generally, for each constructor index $j$, define:

%\begin{lstlisting}
%  eqc (j, B) (f : $\forall$ b : B, b = b) :=
%    fun ... (* TODO get the hypos from the type of the eliminator *) =>
%      f (DepConstr (j, B)) (* TODO args *)%%

  %elim_id := (* TODO *)
%\end{lstlisting}
%Then we need:

%\begin{enumerate}
%\item $\forall b f, \mathrm{DepElim}(b,\ p_{b}) \{\mathrm{eqc} (1, B) f, \ldots, \mathrm{eqc} (n, B) f\} = f (\mathrm{IdEta}(A) a) $
%\item Something relating the constructors and \lstinline{elim_id} to reflexivity
%\end{enumerate}
%and similarly for $A$.

%Really the point of these conditions is that from them, with some restrictions on input terms, we can get
%that lifting terms gives us the same type that we'd get from lifting the type. But there are still
%some restrictions (see the few that fail).

%It's probably not always possible to define these three things for every equivalence.
%Could generalize by rewriting. But this lets us avoid the rewriting problem from Nicolas' paper.

% TODO how does this get us something like primitive projections? Just makes IdEta definitionally equal to regular Id?

% TODO so we can probably just frame search in terms of DepConstr and DepElim and then generate proofs about this on an ad-hoc basis
% and get away with not including the specific details of our instantiations. We can give examples instead, give intuition, and say we generate
% the proofs in Coq

%For the second one we need not just an eliminator rule but also an identity rule.
%DEVOID assumed primitive projections which let them get away without thinking of this,
%but then had this weirdly ad-hoc ``repacking'' thing in their implementation.
%It turns out this is just a more general identity rule, which basically says what
%the identity function should lift to so that the transformation preserves definitional equalities.
%Actually deciding when to run this rule is one of the biggest challenges in practice,
%so we'll talk about that more in the implementation section.

