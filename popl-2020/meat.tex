\section{Key Insights}
\label{sec:key}

\toolname is a flexible tool that supports both proof refactoring and proof repair and integrates
with a typical proof engineering workflow in Coq.
There were three key insights that helped us achieve this:

\begin{enumerate}
\item Proof refactoring and repair are both just 
proof reuse %~\cite{Ringer2019, felty1994generalization, caplan1995logical, pons2000generalization, johnsen2004theorem}
\textit{over time}. The key to supporting both is to build a generic proof reuse
tool that can handle the additional challenges imposed by the reuse occuring over time. 
\item The proof term transformation from the \textsc{DEVOID}~\cite{Ringer2019} proof reuse tool can be generalized
to build such a generic proof reuse tool, and the result is configurable both by the developer and by the user.
\item The transformed proof terms can then be translated back to tactics.
\end{enumerate}

\paragraph{Refactoring \& repair as reuse over time}

Proof refactoring and proof repair are similar problems: both refer to adapting proofs in response to changes
in specifications. In Section~\ref{sec:overview}, for example, our specification was the theorem statement
for \lstinline{rev_app_distr}, which changed when our \lstinline{list} type changed.
Our corresponding change in the proof was a refactoring because the change was \textit{semantics-preserving}:
the proof about the new version of \lstinline{list} behaved exactly the same way as the proof about the old
version of \lstinline{list}.

Repair, in contrast, refers to changes that are \textit{not} semantics-preserving.
One example might be changing a specification to use positive rather than natural numbers, %.
following the lead of the Coq standard library:

\begin{lstlisting}
Nat2Pos.id : forall (n : nat), n <> 0 -> Pos.to_nat (Pos.of_nat n) = n.
\end{lstlisting}
In other words, to repair our proofs about \lstinline{nat} functions to proofs about \lstinline{positive} functions with the desired behavior,
we need the additional restriction that those numbers are nonzero. 
This restriction manifests as a proof obligation for the user.

In both of these cases, we can think of any tool that refactors or repairs our proofs as
\textit{reusing} the proofs about our old specification to derive proofs about our new specification,
either with no new information (in the case of refactoring) or with some additional proof obligations (in the case of repair).
Restating our first key insight:

\begin{quote}
\textbf{Insight 1}:
Proof refactoring and repair are both just 
proof reuse %~\cite{Ringer2019, felty1994generalization, caplan1995logical, pons2000generalization, johnsen2004theorem}
\textit{over time}. The key to supporting both is to build a generic proof reuse
tool that can handle the additional challenges imposed by the reuse occuring over time. 
\end{quote}
In particular, we can reduce the problems of proof refactoring and repair to proof reuse across \textit{type equivalences}~\cite{univalent2013homotopy},
or pairs of functions that map back and forth between two types and are mutual inverses.
The problem of proof reuse across equivalences is known as \textit{transport}. % TODO cite
Refactoring corresponds to transport across an equivalence between the old specification and the new specification,
while repair corresponds to transport across an equivalence between \textit{a refinement} of the old specification
and the new specification (or similarly in the opposite direction).
Note that there can be infinitely many equivalences corresponding to any change in specification,
so the choice in equivalence is to some degree an art that depends on what the user wants.
In the refactoring example, we use this equivalence:

\begin{lstlisting}
Coq.Init.Datatypes.list $\simeq$ list
\end{lstlisting}
while in the repair example, we use this equivalence:

\begin{lstlisting}
{ n : nat & n <> O } $\simeq$ positive
\end{lstlisting}

\paragraph{A configurable proof term transformation for transport across equivalences}

That proof refactoring and repair are reuse \textit{over time} dictates that proofs
must no longer refer in any way to the old specification, since the old specification no longer exists.
This means that most transport methods % TODO cite
are not immediately useful for refactoring and repair.
There is one exception that we are aware of: \textsc{DEVOID}~\cite{Ringer2019} defines a proof term transformation
that transports programs and proofs across equivalences for a particular class of changes.
Restating our second key insight:

\begin{quote}
\textbf{Insight 2}:
The proof term transformation from the \textsc{DEVOID} proof reuse tool can be generalized
to build such a generic proof reuse tool, and the result is configurable both by the developer and by the user.
\end{quote}
This generalized proof term transformation forms the core of \toolname.

\paragraph{Decompiling proof terms to tactics}

Of course, as in \textsc{DEVOID}, such a proof term transformation produces a proof term,
while the proof engineer typically writes proof scripts made up of tactics.
Feeding the user proof terms does not make for a very good user experience.
Restating our final insight:

\begin{quote}
\textbf{Insight 3}: The transformed proof terms can then be translated back to tactics.
\end{quote} 

These insights led us to design \toolname as a configurable proof term transformation
combined with a Gallina to Ltac decompiler.
This gives us the \textbf{Configure}, \textbf{Transform}, \textbf{Decompile} workflow we saw in Section~\ref{sec:overview}.

\section{Transform}
\label{sec:meat}


\begin{figure}
\begin{mathpar}
\mprset{flushleft}
\small
\hfill\fbox{$\Gamma$ $\vdash$ $t$ $\Uparrow$ $t'$}\\

\inferrule[Dep-Elim]
  { \Gamma \vdash p_{a} \Uparrow p_b \\ \Gamma \vdash \vec{f_{a}}\phantom{l} \Uparrow \vec{f_{b}} }
  { \Gamma \vdash \mathrm{DepElim}(a,\ p_{a}) \vec{f_{a}} \Uparrow \mathrm{DepElim}(b,\ p_b) \vec{f_{b}} }

\inferrule[Dep-Constr]
{ \Gamma \vdash \vec{t}_{a} \Uparrow \vec{t}_{b} } %\\ TODO must we explicitly lift A to B if we want to handle parameters/indices?
{ \Gamma \vdash \mathrm{DepConstr}(j,\ A)\ \vec{t}_{a} \Uparrow \mathrm{DepConstr}(j,\ B)\ \vec{t}_{b}  }

\inferrule[Id-Eta]
  { \\ }
  { \Gamma \vdash \mathrm{IdEta}(A) \Uparrow \mathrm{IdEta}(B) }

\inferrule[Rew-Eta]
  { \Gamma \vdash c_A \Uparrow c_B \\ \Gamma \vdash q_A \Uparrow q_B \\ \Gamma \vdash e_A \Uparrow e_B }
  { \Gamma \vdash \mathrm{RewEta}(j,\ A,\ q_A,\ t_A) \Uparrow \mathrm{RewEta}(j,\ B,\ q_B,\ t_B) }

\inferrule[Equivalence]
  { \\ }
  { \Gamma \vdash A\ \Uparrow B }

\inferrule[Constr]
{ \Gamma \vdash T \Uparrow T' \\ \Gamma \vdash \vec{t} \Uparrow \vec{t'} }
{ \Gamma \vdash \mathrm{Constr}(j,\ T)\ \vec{t} \Uparrow \mathrm{Constr}(j,\ T')\ \vec{t'} }

\inferrule[Ind]
  { \Gamma \vdash T \Uparrow T' \\ \Gamma \vdash \vec{C} \Uparrow \vec{C'}  }
  { \Gamma \vdash \mathrm{Ind} (\mathit{Ty} : T) \vec{C} \Uparrow \mathrm{Ind} (\mathit{Ty} : T') \vec{C'} }

\inferrule[Elim] % TODO wait why do we have c here when it clearly refers to the term we eliminate over? um
  { \Gamma \vdash c \Uparrow c' \\ \Gamma \vdash Q \Uparrow Q' \\ \Gamma \vdash \vec{f} \Uparrow \vec{f'}}
  { \Gamma \vdash \mathrm{Elim}(c, Q) \vec{f} \Uparrow \mathrm{Elim}(c', Q') \vec{f'}  }

%% Application
\inferrule[App]
 { \Gamma \vdash f \Uparrow f' \\ \Gamma \vdash t \Uparrow t'}
 { \Gamma \vdash f t \Uparrow f' t' }

% Lamda
\inferrule[Lam]
  { \Gamma \vdash T \Uparrow T' \\ \Gamma,\ t : T \vdash b \Uparrow b' }
  {\Gamma \vdash \lambda (t : T).b \Uparrow \lambda (t : T').b'}

% Product
\inferrule[Prod]
  { \Gamma \vdash T \Uparrow T' \\ \Gamma,\ t : T \vdash b \Uparrow b' }
  {\Gamma \vdash \Pi (t : T).b \Uparrow \Pi (t : T').b'}
\end{mathpar}
\caption{Proof term transformation.}
\label{fig:final}
\end{figure}

While \textbf{Configure} (Section~\ref{sec:search}) is the first step for the user, \textbf{Transform} is at the heart of \toolname.
Figure~\ref{fig:final} shows the configurable proof term transformation that \textbf{Transform} implements.
The transformation operates over terms in CIC$_{\omega}$.
It assumes the same grammar and semantics for CIC$_{\omega}$
as the transformation from \textsc{Devoid} that it is based off of
with one exception: it removes the restriction that projections must be primitive.

Like the transformation from \textsc{Devoid}, this transformation is parameterized over
two equivalent types \A and \B (rule \lstinline{Equivalence}).
At a high level, the configuration ensures that the transformation accomplishes two goals:

\begin{enumerate}
\item preserves the equivalence between \A and \B, and
\item produces well-typed terms.
\end{enumerate}
These two goals each correspond to a pair of rules, respectively:

\begin{enumerate}
\item \lstinline{Dep-Constr} and \lstinline{Dep-Elim} transform constructors and eliminators, thereby preserving the equivalence (Section~\ref{sec:equivalence}), and 
\item \lstinline{Id-Eta} and \lstinline{Rew-Eta} transform identity and equalities, thereby producing well-typed terms (Section~\ref{sec:equality}).
\end{enumerate}
These rely on a configuration (\lstinline{DepConstr}, \lstinline{DepElim}, \lstinline{IdEta}, \lstinline{RewEta}) % TODO lists
provided either by a search procedure or by the user.
The four parts of this configuration must be in relation to one another in a certain way in order for the proof
term transformation to work correctly (Section~\ref{sec:art}).
From there, the rest of the transformation is straightforward.

\subsection{Equivalence}
\label{sec:equivalence}

(draft text) want functions \& proofs that behave the same along the type equivalence between \A and \B
functions behave same, proofs talk about the same things
so equivalent inputs to equivalent outputs, and proofs about functions talk about equivalent things
like DEVOID, means we lift to something equal up to transport

two rules give us this: \lstinline{Dep-Constr} and \lstinline{Dep-Elim},
which lift configuration parts \lstinline{DepConstr} and \lstinline{DepElim}.
idea is that the only way you can make an \A or a \B is by constructors, and the only way you can match over one
(because we assume primitive eliminators) is by the eliminators.
so first transports constructors, second eliminators.
the result will no longer refer to \A because it will explicitly construct \B wherever it constructed \A,
and it will eliminate \B wherever it eliminated \A.

but note that these are not necessarily standard inductive constructors and eliminators.
since we can have a refinement, or non-inductive types, these are dependent constructors or eliminators.
and since our types can have different numbers of constructors, we really think about this like
wrapping the constructors and eliminators of \B with the structure of \A.

this is a bit weird and easiest to show by example.
for our simplest example with our two lists, we have this:

\begin{lstlisting}
TODO
\end{lstlisting}
for \lstinline{nat} and \lstinline{positive}, we have this:

\begin{lstlisting}
TODO
\end{lstlisting}
and later in Section~\ref{sec:search}, we'll see an example with different numbers of constructors,
\lstinline{nat} and \lstinline{binnat}. In that example, we have this:

\begin{lstlisting}
TODO
\end{lstlisting}

in all cases, together these induce a type equivalence between \A and \B:

\begin{lstlisting}
TODO
\end{lstlisting}
the search procedures from Section~\ref{sec:search} discover these functions and prove this automatically
for certain \A and \B with certain properties. otherwise, like in DEVOID, this must be true
in order for everything to work, but the user does not need to prove it.
more on that in Section~\ref{sec:art}.

\subsection{Equality}
\label{sec:equality}

(draft text) it's not enough to just have things be equivalent.
we don't have univalence (which would state that equivalence is equivalent to equality).
the result as noted in the marriage of univalence and parametricity framework is that unless we handle equalities,
lifting terms might have different types than the lifted types.
this is because definitional equalities might need to lift to propositional equalities.
a simple example might be this one from \lstinline{nat} and \lstinline{binnat}:

\begin{lstlisting}
TODO
\end{lstlisting}
over \lstinline{nat} this is reflexivity, but over \lstinline{binnat} this is not.
so we must somehow lift the thing on the left to the thing on the right.

two rules for this: \lstinline{Id-Eta} and \lstinline{Rew-Eta}, which lift
configuration parts \lstinline{IdEta} and \lstinline{RewEta}.
\lstinline{IdEta} lifts the expanded identity function, and \lstinline{RewEta} lifts equalities.
There is just one \lstinline{IdEta}. there are as many \lstinline{RewEta} as there are inductive hypotheses
of \lstinline{DepElim}.
for example, for our refactor, we have the simplest case:

\begin{lstlisting}
TODO
\end{lstlisting}
for our repair, identity is expanded for compatibility with eliminators in correctness criteria, but nothing else changes:

\begin{lstlisting}
TODO
\end{lstlisting}
and for \lstinline{nat} and \lstinline{binnat}, \lstinline{IdEta} is straightforward, but actually one of our \lstinline{RewEta}
is interesting because the successor case of our eliminator doesn't definitionally preserve equality:

\begin{lstlisting}
TODO
\end{lstlisting}
thus we can lift the example up top to this:

\begin{lstlisting}
TODO
\end{lstlisting}

note that the marriage of univalence and parametricity paper said this approach would be intractable in general.
They are partly right!
It is easily \textit{describable}, but there are many challenges to implementation which we will note in Section~\ref{sec:implementation}.
notably, \lstinline{IdEta} and \lstinline{RewEta} are typically implied implicitly in code, so the implementation needs to match
against them.
for example, the example up top didn't apply \lstinline{RewEta} explicitly, so if we want to fully automate this then
we need a step that understands that casts are implicit rewrites, and expands them in the right place.
this problem shows up a bit for constructors and eliminators but is much more pronounced here.

for now for these all of the configuration rules our search procedure include custom matching definitions,
but there unfortunately isn't a way for users to supply custom matching things,
and unification isn't good enough for \lstinline{RewEta}.
so when configuration is manual, input terms must be expanded to apply \lstinline{RewEta} over \A.
custom matching WIP; will talk more about this in future work.

\subsection{Equivalence \& Equality}
\label{sec:art}

The transformation is configurable by equivalence, but choosing the configuration (either by providing direct input or writing a 
search procedure for an entire class of equivalences) is a bit of an art: There can be infinitely many equivalences that correpond to a 
given change in specification, only some of which are useful. Furthermore, different configurations based on those equivalences
can be more or less useful than others. Finally, certain proof obligations for configuring the transformation can be tricky.
Thankfully, once that art is done, we know what it means for it to be correct.

The correctness criteria for the configuration relate \lstinline{DepConstr}, \lstinline{DepElim}, \lstinline{IdEta}, and \lstinline{RewEta}
in a particular way.
This goes back to the equivalence and equality thing from the previous section.
Formal thing here, intuition for what it means for the whole thing, explanation, and example.
Section~\ref{sec:search} shows some particular instantions of this and their applicability to real programs and proofs.

%Nicolas proved the first of these a while ago
%for the equivalence in the DEVOID ITP paper.\footnote{\url{https://github.com/CoqHott/univalent_parametricity/commit/7dc14e69942e6b3302fadaf5356f9a7e724b0f3c}}

Note about decidability of matching: when all of this is correct, what this means is that you \textit{can} always
run one of the transformation rules. But that doesn't mean you \textit{should}. Depends what the user wants,
and in some cases, would not terminate (refinement types, unpacking indexed types). Implementation section will
discuss how we actually decide which ones to run so user doesn't need to apply transport by hand over and over again,
and discussion section describes some cool ideas for doing this nicely with type-based search in the future.

%First we need that \lstinline{DepElim} over $A$ into \lstinline{DepConstr} over $B$ and \lstinline{DepElim} over $B$ into
%\lstinline{DepConstr} over $A$ form an equivalence between $A$ and $B$. When that's true, I think it should hold that \lstinline{DepElim} over $A$
%and \lstinline{DepElim} over $B$ are in univalent relation with one another. If not, then that's an extra condition.
%Finally, we need the transformation to preserve definitional equalities. Not sure about the general case, but for vectors and lists,
%we need:

%\begin{lstlisting}
%  $\forall$ A l (f : $\forall$ (l : sigT (Vector.t A)), l = l),
%    vect_dep_elim A (fun l => l = l) (f nil) (fun t s _ => f (cons t s)) l = f (id_eta l).
%\end{lstlisting}
%and:

%\begin{lstlisting}
%Definition elim_id (A : Type) (s : {H : nat & t A H}) :=
%  vect_dep_elim
%    A
%    (fun _ => {H : nat & t A H})
%    nil
%    (fun (h : A) _ IH =>
%      cons h IH)
%    s.

% $\forall$ A h s,
%    exists (H : cons h (elim_id A s) = elim_id A (cons h s)),
%      H = eq_refl.
%\end{lstlisting}
%More generally, for each constructor index $j$, define:

%\begin{lstlisting}
%  eqc (j, B) (f : $\forall$ b : B, b = b) :=
%    fun ... (* TODO get the hypos from the type of the eliminator *) =>
%      f (DepConstr (j, B)) (* TODO args *)%%

  %elim_id := (* TODO *)
%\end{lstlisting}
%Then we need:

%\begin{enumerate}
%\item $\forall b f, \mathrm{DepElim}(b,\ p_{b}) \{\mathrm{eqc} (1, B) f, \ldots, \mathrm{eqc} (n, B) f\} = f (\mathrm{IdEta}(A) a) $
%\item Something relating the constructors and \lstinline{elim_id} to reflexivity
%\end{enumerate}
%and similarly for $A$.

%Really the point of these conditions is that from them, with some restrictions on input terms, we can get
%that lifting terms gives us the same type that we'd get from lifting the type. But there are still
%some restrictions (see the few that fail).

%It's probably not always possible to define these three things for every equivalence.
%Could generalize by rewriting. But this lets us avoid the rewriting problem from Nicolas' paper.

% TODO how does this get us something like primitive projections? Just makes IdEta definitionally equal to regular Id?

% TODO so we can probably just frame search in terms of DepConstr and DepElim and then generate proofs about this on an ad-hoc basis
% and get away with not including the specific details of our instantiations. We can give examples instead, give intuition, and say we generate
% the proofs in Coq

%For the second one we need not just an eliminator rule but also an identity rule.
%DEVOID assumed primitive projections which let them get away without thinking of this,
%but then had this weirdly ad-hoc ``repacking'' thing in their implementation.
%It turns out this is just a more general identity rule, which basically says what
%the identity function should lift to so that the transformation preserves definitional equalities.
%Actually deciding when to run this rule is one of the biggest challenges in practice,
%so we'll talk about that more in the implementation section.

